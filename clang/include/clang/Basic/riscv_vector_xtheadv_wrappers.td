//==--- riscv_vector_xtheadv.td - RISC-V V-ext Builtin function list ------===//
//
//  Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
//  See https://llvm.org/LICENSE.txt for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the builtins for RISC-V V-extension. See:
//
//     https://github.com/riscv-non-isa/rvv-intrinsic-doc/tree/v0.7.1
//
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// 7. Vector Loads and Stores
//===----------------------------------------------------------------------===//

let HeaderCode =
[{
// Vector Unit-stride loads
#define __riscv_vlb_v_i8m1(src_ptr, vl) __riscv_th_vlb_v_i8m1(src_ptr, vl)
#define __riscv_vlb_v_i8m2(src_ptr, vl) __riscv_th_vlb_v_i8m2(src_ptr, vl)
#define __riscv_vlb_v_i8m4(src_ptr, vl) __riscv_th_vlb_v_i8m4(src_ptr, vl)
#define __riscv_vlb_v_i8m8(src_ptr, vl) __riscv_th_vlb_v_i8m8(src_ptr, vl)
#define __riscv_vlb_v_i16m1(src_ptr, vl) __riscv_th_vlb_v_i16m1(src_ptr, vl)
#define __riscv_vlb_v_i16m2(src_ptr, vl) __riscv_th_vlb_v_i16m2(src_ptr, vl)
#define __riscv_vlb_v_i16m4(src_ptr, vl) __riscv_th_vlb_v_i16m4(src_ptr, vl)
#define __riscv_vlb_v_i16m8(src_ptr, vl) __riscv_th_vlb_v_i16m8(src_ptr, vl)
#define __riscv_vlb_v_i32m1(src_ptr, vl) __riscv_th_vlb_v_i32m1(src_ptr, vl)
#define __riscv_vlb_v_i32m2(src_ptr, vl) __riscv_th_vlb_v_i32m2(src_ptr, vl)
#define __riscv_vlb_v_i32m4(src_ptr, vl) __riscv_th_vlb_v_i32m4(src_ptr, vl)
#define __riscv_vlb_v_i32m8(src_ptr, vl) __riscv_th_vlb_v_i32m8(src_ptr, vl)
#define __riscv_vlb_v_i64m1(src_ptr, vl) __riscv_th_vlb_v_i64m1(src_ptr, vl)
#define __riscv_vlb_v_i64m2(src_ptr, vl) __riscv_th_vlb_v_i64m2(src_ptr, vl)
#define __riscv_vlb_v_i64m4(src_ptr, vl) __riscv_th_vlb_v_i64m4(src_ptr, vl)
#define __riscv_vlb_v_i64m8(src_ptr, vl) __riscv_th_vlb_v_i64m8(src_ptr, vl)
#define __riscv_vlh_v_i8m1(src_ptr, vl) __riscv_th_vlh_v_i8m1(src_ptr, vl)
#define __riscv_vlh_v_i8m2(src_ptr, vl) __riscv_th_vlh_v_i8m2(src_ptr, vl)
#define __riscv_vlh_v_i8m4(src_ptr, vl) __riscv_th_vlh_v_i8m4(src_ptr, vl)
#define __riscv_vlh_v_i8m8(src_ptr, vl) __riscv_th_vlh_v_i8m8(src_ptr, vl)
#define __riscv_vlh_v_i16m1(src_ptr, vl) __riscv_th_vlh_v_i16m1(src_ptr, vl)
#define __riscv_vlh_v_i16m2(src_ptr, vl) __riscv_th_vlh_v_i16m2(src_ptr, vl)
#define __riscv_vlh_v_i16m4(src_ptr, vl) __riscv_th_vlh_v_i16m4(src_ptr, vl)
#define __riscv_vlh_v_i16m8(src_ptr, vl) __riscv_th_vlh_v_i16m8(src_ptr, vl)
#define __riscv_vlh_v_i32m1(src_ptr, vl) __riscv_th_vlh_v_i32m1(src_ptr, vl)
#define __riscv_vlh_v_i32m2(src_ptr, vl) __riscv_th_vlh_v_i32m2(src_ptr, vl)
#define __riscv_vlh_v_i32m4(src_ptr, vl) __riscv_th_vlh_v_i32m4(src_ptr, vl)
#define __riscv_vlh_v_i32m8(src_ptr, vl) __riscv_th_vlh_v_i32m8(src_ptr, vl)
#define __riscv_vlh_v_i64m1(src_ptr, vl) __riscv_th_vlh_v_i64m1(src_ptr, vl)
#define __riscv_vlh_v_i64m2(src_ptr, vl) __riscv_th_vlh_v_i64m2(src_ptr, vl)
#define __riscv_vlh_v_i64m4(src_ptr, vl) __riscv_th_vlh_v_i64m4(src_ptr, vl)
#define __riscv_vlh_v_i64m8(src_ptr, vl) __riscv_th_vlh_v_i64m8(src_ptr, vl)
#define __riscv_vlw_v_i8m1(src_ptr, vl) __riscv_th_vlw_v_i8m1(src_ptr, vl)
#define __riscv_vlw_v_i8m2(src_ptr, vl) __riscv_th_vlw_v_i8m2(src_ptr, vl)
#define __riscv_vlw_v_i8m4(src_ptr, vl) __riscv_th_vlw_v_i8m4(src_ptr, vl)
#define __riscv_vlw_v_i8m8(src_ptr, vl) __riscv_th_vlw_v_i8m8(src_ptr, vl)
#define __riscv_vlw_v_i16m1(src_ptr, vl) __riscv_th_vlw_v_i16m1(src_ptr, vl)
#define __riscv_vlw_v_i16m2(src_ptr, vl) __riscv_th_vlw_v_i16m2(src_ptr, vl)
#define __riscv_vlw_v_i16m4(src_ptr, vl) __riscv_th_vlw_v_i16m4(src_ptr, vl)
#define __riscv_vlw_v_i16m8(src_ptr, vl) __riscv_th_vlw_v_i16m8(src_ptr, vl)
#define __riscv_vlw_v_i32m1(src_ptr, vl) __riscv_th_vlw_v_i32m1(src_ptr, vl)
#define __riscv_vlw_v_i32m2(src_ptr, vl) __riscv_th_vlw_v_i32m2(src_ptr, vl)
#define __riscv_vlw_v_i32m4(src_ptr, vl) __riscv_th_vlw_v_i32m4(src_ptr, vl)
#define __riscv_vlw_v_i32m8(src_ptr, vl) __riscv_th_vlw_v_i32m8(src_ptr, vl)
#define __riscv_vlw_v_i64m1(src_ptr, vl) __riscv_th_vlw_v_i64m1(src_ptr, vl)
#define __riscv_vlw_v_i64m2(src_ptr, vl) __riscv_th_vlw_v_i64m2(src_ptr, vl)
#define __riscv_vlw_v_i64m4(src_ptr, vl) __riscv_th_vlw_v_i64m4(src_ptr, vl)
#define __riscv_vlw_v_i64m8(src_ptr, vl) __riscv_th_vlw_v_i64m8(src_ptr, vl)
#define __riscv_vlbu_v_u8m1(src_ptr, vl) __riscv_th_vlbu_v_u8m1(src_ptr, vl)
#define __riscv_vlbu_v_u8m2(src_ptr, vl) __riscv_th_vlbu_v_u8m2(src_ptr, vl)
#define __riscv_vlbu_v_u8m4(src_ptr, vl) __riscv_th_vlbu_v_u8m4(src_ptr, vl)
#define __riscv_vlbu_v_u8m8(src_ptr, vl) __riscv_th_vlbu_v_u8m8(src_ptr, vl)
#define __riscv_vlbu_v_u16m1(src_ptr, vl) __riscv_th_vlbu_v_u16m1(src_ptr, vl)
#define __riscv_vlbu_v_u16m2(src_ptr, vl) __riscv_th_vlbu_v_u16m2(src_ptr, vl)
#define __riscv_vlbu_v_u16m4(src_ptr, vl) __riscv_th_vlbu_v_u16m4(src_ptr, vl)
#define __riscv_vlbu_v_u16m8(src_ptr, vl) __riscv_th_vlbu_v_u16m8(src_ptr, vl)
#define __riscv_vlbu_v_u32m1(src_ptr, vl) __riscv_th_vlbu_v_u32m1(src_ptr, vl)
#define __riscv_vlbu_v_u32m2(src_ptr, vl) __riscv_th_vlbu_v_u32m2(src_ptr, vl)
#define __riscv_vlbu_v_u32m4(src_ptr, vl) __riscv_th_vlbu_v_u32m4(src_ptr, vl)
#define __riscv_vlbu_v_u32m8(src_ptr, vl) __riscv_th_vlbu_v_u32m8(src_ptr, vl)
#define __riscv_vlbu_v_u64m1(src_ptr, vl) __riscv_th_vlbu_v_u64m1(src_ptr, vl)
#define __riscv_vlbu_v_u64m2(src_ptr, vl) __riscv_th_vlbu_v_u64m2(src_ptr, vl)
#define __riscv_vlbu_v_u64m4(src_ptr, vl) __riscv_th_vlbu_v_u64m4(src_ptr, vl)
#define __riscv_vlbu_v_u64m8(src_ptr, vl) __riscv_th_vlbu_v_u64m8(src_ptr, vl)
#define __riscv_vlhu_v_u8m1(src_ptr, vl) __riscv_th_vlhu_v_u8m1(src_ptr, vl)
#define __riscv_vlhu_v_u8m2(src_ptr, vl) __riscv_th_vlhu_v_u8m2(src_ptr, vl)
#define __riscv_vlhu_v_u8m4(src_ptr, vl) __riscv_th_vlhu_v_u8m4(src_ptr, vl)
#define __riscv_vlhu_v_u8m8(src_ptr, vl) __riscv_th_vlhu_v_u8m8(src_ptr, vl)
#define __riscv_vlhu_v_u16m1(src_ptr, vl) __riscv_th_vlhu_v_u16m1(src_ptr, vl)
#define __riscv_vlhu_v_u16m2(src_ptr, vl) __riscv_th_vlhu_v_u16m2(src_ptr, vl)
#define __riscv_vlhu_v_u16m4(src_ptr, vl) __riscv_th_vlhu_v_u16m4(src_ptr, vl)
#define __riscv_vlhu_v_u16m8(src_ptr, vl) __riscv_th_vlhu_v_u16m8(src_ptr, vl)
#define __riscv_vlhu_v_u32m1(src_ptr, vl) __riscv_th_vlhu_v_u32m1(src_ptr, vl)
#define __riscv_vlhu_v_u32m2(src_ptr, vl) __riscv_th_vlhu_v_u32m2(src_ptr, vl)
#define __riscv_vlhu_v_u32m4(src_ptr, vl) __riscv_th_vlhu_v_u32m4(src_ptr, vl)
#define __riscv_vlhu_v_u32m8(src_ptr, vl) __riscv_th_vlhu_v_u32m8(src_ptr, vl)
#define __riscv_vlhu_v_u64m1(src_ptr, vl) __riscv_th_vlhu_v_u64m1(src_ptr, vl)
#define __riscv_vlhu_v_u64m2(src_ptr, vl) __riscv_th_vlhu_v_u64m2(src_ptr, vl)
#define __riscv_vlhu_v_u64m4(src_ptr, vl) __riscv_th_vlhu_v_u64m4(src_ptr, vl)
#define __riscv_vlhu_v_u64m8(src_ptr, vl) __riscv_th_vlhu_v_u64m8(src_ptr, vl)
#define __riscv_vlwu_v_u8m1(src_ptr, vl) __riscv_th_vlwu_v_u8m1(src_ptr, vl)
#define __riscv_vlwu_v_u8m2(src_ptr, vl) __riscv_th_vlwu_v_u8m2(src_ptr, vl)
#define __riscv_vlwu_v_u8m4(src_ptr, vl) __riscv_th_vlwu_v_u8m4(src_ptr, vl)
#define __riscv_vlwu_v_u8m8(src_ptr, vl) __riscv_th_vlwu_v_u8m8(src_ptr, vl)
#define __riscv_vlwu_v_u16m1(src_ptr, vl) __riscv_th_vlwu_v_u16m1(src_ptr, vl)
#define __riscv_vlwu_v_u16m2(src_ptr, vl) __riscv_th_vlwu_v_u16m2(src_ptr, vl)
#define __riscv_vlwu_v_u16m4(src_ptr, vl) __riscv_th_vlwu_v_u16m4(src_ptr, vl)
#define __riscv_vlwu_v_u16m8(src_ptr, vl) __riscv_th_vlwu_v_u16m8(src_ptr, vl)
#define __riscv_vlwu_v_u32m1(src_ptr, vl) __riscv_th_vlwu_v_u32m1(src_ptr, vl)
#define __riscv_vlwu_v_u32m2(src_ptr, vl) __riscv_th_vlwu_v_u32m2(src_ptr, vl)
#define __riscv_vlwu_v_u32m4(src_ptr, vl) __riscv_th_vlwu_v_u32m4(src_ptr, vl)
#define __riscv_vlwu_v_u32m8(src_ptr, vl) __riscv_th_vlwu_v_u32m8(src_ptr, vl)
#define __riscv_vlwu_v_u64m1(src_ptr, vl) __riscv_th_vlwu_v_u64m1(src_ptr, vl)
#define __riscv_vlwu_v_u64m2(src_ptr, vl) __riscv_th_vlwu_v_u64m2(src_ptr, vl)
#define __riscv_vlwu_v_u64m4(src_ptr, vl) __riscv_th_vlwu_v_u64m4(src_ptr, vl)
#define __riscv_vlwu_v_u64m8(src_ptr, vl) __riscv_th_vlwu_v_u64m8(src_ptr, vl)
#define __riscv_vle8_v_i8m1(src_ptr, vl) __riscv_th_vle8_v_i8m1(src_ptr, vl)
#define __riscv_vle8_v_i8m2(src_ptr, vl) __riscv_th_vle8_v_i8m2(src_ptr, vl)
#define __riscv_vle8_v_i8m4(src_ptr, vl) __riscv_th_vle8_v_i8m4(src_ptr, vl)
#define __riscv_vle8_v_i8m8(src_ptr, vl) __riscv_th_vle8_v_i8m8(src_ptr, vl)
#define __riscv_vle16_v_i16m1(src_ptr, vl) __riscv_th_vle16_v_i16m1(src_ptr, vl)
#define __riscv_vle16_v_i16m2(src_ptr, vl) __riscv_th_vle16_v_i16m2(src_ptr, vl)
#define __riscv_vle16_v_i16m4(src_ptr, vl) __riscv_th_vle16_v_i16m4(src_ptr, vl)
#define __riscv_vle16_v_i16m8(src_ptr, vl) __riscv_th_vle16_v_i16m8(src_ptr, vl)
#define __riscv_vle32_v_i32m1(src_ptr, vl) __riscv_th_vle32_v_i32m1(src_ptr, vl)
#define __riscv_vle32_v_i32m2(src_ptr, vl) __riscv_th_vle32_v_i32m2(src_ptr, vl)
#define __riscv_vle32_v_i32m4(src_ptr, vl) __riscv_th_vle32_v_i32m4(src_ptr, vl)
#define __riscv_vle32_v_i32m8(src_ptr, vl) __riscv_th_vle32_v_i32m8(src_ptr, vl)
#define __riscv_vle64_v_i64m1(src_ptr, vl) __riscv_th_vle64_v_i64m1(src_ptr, vl)
#define __riscv_vle64_v_i64m2(src_ptr, vl) __riscv_th_vle64_v_i64m2(src_ptr, vl)
#define __riscv_vle64_v_i64m4(src_ptr, vl) __riscv_th_vle64_v_i64m4(src_ptr, vl)
#define __riscv_vle64_v_i64m8(src_ptr, vl) __riscv_th_vle64_v_i64m8(src_ptr, vl)
#define __riscv_vle8_v_u8m1(src_ptr, vl) __riscv_th_vle8_v_u8m1(src_ptr, vl)
#define __riscv_vle8_v_u8m2(src_ptr, vl) __riscv_th_vle8_v_u8m2(src_ptr, vl)
#define __riscv_vle8_v_u8m4(src_ptr, vl) __riscv_th_vle8_v_u8m4(src_ptr, vl)
#define __riscv_vle8_v_u8m8(src_ptr, vl) __riscv_th_vle8_v_u8m8(src_ptr, vl)
#define __riscv_vle16_v_u16m1(src_ptr, vl) __riscv_th_vle16_v_u16m1(src_ptr, vl)
#define __riscv_vle16_v_u16m2(src_ptr, vl) __riscv_th_vle16_v_u16m2(src_ptr, vl)
#define __riscv_vle16_v_u16m4(src_ptr, vl) __riscv_th_vle16_v_u16m4(src_ptr, vl)
#define __riscv_vle16_v_u16m8(src_ptr, vl) __riscv_th_vle16_v_u16m8(src_ptr, vl)
#define __riscv_vle32_v_u32m1(src_ptr, vl) __riscv_th_vle32_v_u32m1(src_ptr, vl)
#define __riscv_vle32_v_u32m2(src_ptr, vl) __riscv_th_vle32_v_u32m2(src_ptr, vl)
#define __riscv_vle32_v_u32m4(src_ptr, vl) __riscv_th_vle32_v_u32m4(src_ptr, vl)
#define __riscv_vle32_v_u32m8(src_ptr, vl) __riscv_th_vle32_v_u32m8(src_ptr, vl)
#define __riscv_vle64_v_u64m1(src_ptr, vl) __riscv_th_vle64_v_u64m1(src_ptr, vl)
#define __riscv_vle64_v_u64m2(src_ptr, vl) __riscv_th_vle64_v_u64m2(src_ptr, vl)
#define __riscv_vle64_v_u64m4(src_ptr, vl) __riscv_th_vle64_v_u64m4(src_ptr, vl)
#define __riscv_vle64_v_u64m8(src_ptr, vl) __riscv_th_vle64_v_u64m8(src_ptr, vl)
#define __riscv_vle16_v_f16m1(src_ptr, vl) __riscv_th_vle16_v_f16m1(src_ptr, vl)
#define __riscv_vle16_v_f16m2(src_ptr, vl) __riscv_th_vle16_v_f16m2(src_ptr, vl)
#define __riscv_vle16_v_f16m4(src_ptr, vl) __riscv_th_vle16_v_f16m4(src_ptr, vl)
#define __riscv_vle16_v_f16m8(src_ptr, vl) __riscv_th_vle16_v_f16m8(src_ptr, vl)
#define __riscv_vle32_v_f32m1(src_ptr, vl) __riscv_th_vle32_v_f32m1(src_ptr, vl)
#define __riscv_vle32_v_f32m2(src_ptr, vl) __riscv_th_vle32_v_f32m2(src_ptr, vl)
#define __riscv_vle32_v_f32m4(src_ptr, vl) __riscv_th_vle32_v_f32m4(src_ptr, vl)
#define __riscv_vle32_v_f32m8(src_ptr, vl) __riscv_th_vle32_v_f32m8(src_ptr, vl)
#define __riscv_vle64_v_f64m1(src_ptr, vl) __riscv_th_vle64_v_f64m1(src_ptr, vl)
#define __riscv_vle64_v_f64m2(src_ptr, vl) __riscv_th_vle64_v_f64m2(src_ptr, vl)
#define __riscv_vle64_v_f64m4(src_ptr, vl) __riscv_th_vle64_v_f64m4(src_ptr, vl)
#define __riscv_vle64_v_f64m8(src_ptr, vl) __riscv_th_vle64_v_f64m8(src_ptr, vl)

// Vector Unit-stride stores
#define __riscv_vsb_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_i8m1(base, value, vl) __riscv_th_vse8_v_i8m1(base, value, vl)
#define __riscv_vse8_v_i8m2(base, value, vl) __riscv_th_vse8_v_i8m2(base, value, vl)
#define __riscv_vse8_v_i8m4(base, value, vl) __riscv_th_vse8_v_i8m4(base, value, vl)
#define __riscv_vse8_v_i8m8(base, value, vl) __riscv_th_vse8_v_i8m8(base, value, vl)
#define __riscv_vse8_v_u8m1(base, value, vl) __riscv_th_vse8_v_u8m1(base, value, vl)
#define __riscv_vse8_v_u8m2(base, value, vl) __riscv_th_vse8_v_u8m2(base, value, vl)
#define __riscv_vse8_v_u8m4(base, value, vl) __riscv_th_vse8_v_u8m4(base, value, vl)
#define __riscv_vse8_v_u8m8(base, value, vl) __riscv_th_vse8_v_u8m8(base, value, vl)
#define __riscv_vse8_v_i8m1_m(mask, base, value, vl) __riscv_th_vse8_v_i8m1_m(mask, base, value, vl)
#define __riscv_vse8_v_i8m2_m(mask, base, value, vl) __riscv_th_vse8_v_i8m2_m(mask, base, value, vl)
#define __riscv_vse8_v_i8m4_m(mask, base, value, vl) __riscv_th_vse8_v_i8m4_m(mask, base, value, vl)
#define __riscv_vse8_v_i8m8_m(mask, base, value, vl) __riscv_th_vse8_v_i8m8_m(mask, base, value, vl)
#define __riscv_vse8_v_u8m1_m(mask, base, value, vl) __riscv_th_vse8_v_u8m1_m(mask, base, value, vl)
#define __riscv_vse8_v_u8m2_m(mask, base, value, vl) __riscv_th_vse8_v_u8m2_m(mask, base, value, vl)
#define __riscv_vse8_v_u8m4_m(mask, base, value, vl) __riscv_th_vse8_v_u8m4_m(mask, base, value, vl)
#define __riscv_vse8_v_u8m8_m(mask, base, value, vl) __riscv_th_vse8_v_u8m8_m(mask, base, value, vl)
#define __riscv_vse16_v_f16m1(base, value, vl) __riscv_th_vse16_v_f16m1(base, value, vl)
#define __riscv_vse16_v_f16m2(base, value, vl) __riscv_th_vse16_v_f16m2(base, value, vl)
#define __riscv_vse16_v_f16m4(base, value, vl) __riscv_th_vse16_v_f16m4(base, value, vl)
#define __riscv_vse16_v_f16m8(base, value, vl) __riscv_th_vse16_v_f16m8(base, value, vl)
#define __riscv_vse16_v_i16m1(base, value, vl) __riscv_th_vse16_v_i16m1(base, value, vl)
#define __riscv_vse16_v_i16m2(base, value, vl) __riscv_th_vse16_v_i16m2(base, value, vl)
#define __riscv_vse16_v_i16m4(base, value, vl) __riscv_th_vse16_v_i16m4(base, value, vl)
#define __riscv_vse16_v_i16m8(base, value, vl) __riscv_th_vse16_v_i16m8(base, value, vl)
#define __riscv_vse16_v_u16m1(base, value, vl) __riscv_th_vse16_v_u16m1(base, value, vl)
#define __riscv_vse16_v_u16m2(base, value, vl) __riscv_th_vse16_v_u16m2(base, value, vl)
#define __riscv_vse16_v_u16m4(base, value, vl) __riscv_th_vse16_v_u16m4(base, value, vl)
#define __riscv_vse16_v_u16m8(base, value, vl) __riscv_th_vse16_v_u16m8(base, value, vl)
#define __riscv_vse16_v_f16m1_m(mask, base, value, vl) __riscv_th_vse16_v_f16m1_m(mask, base, value, vl)
#define __riscv_vse16_v_f16m2_m(mask, base, value, vl) __riscv_th_vse16_v_f16m2_m(mask, base, value, vl)
#define __riscv_vse16_v_f16m4_m(mask, base, value, vl) __riscv_th_vse16_v_f16m4_m(mask, base, value, vl)
#define __riscv_vse16_v_f16m8_m(mask, base, value, vl) __riscv_th_vse16_v_f16m8_m(mask, base, value, vl)
#define __riscv_vse16_v_i16m1_m(mask, base, value, vl) __riscv_th_vse16_v_i16m1_m(mask, base, value, vl)
#define __riscv_vse16_v_i16m2_m(mask, base, value, vl) __riscv_th_vse16_v_i16m2_m(mask, base, value, vl)
#define __riscv_vse16_v_i16m4_m(mask, base, value, vl) __riscv_th_vse16_v_i16m4_m(mask, base, value, vl)
#define __riscv_vse16_v_i16m8_m(mask, base, value, vl) __riscv_th_vse16_v_i16m8_m(mask, base, value, vl)
#define __riscv_vse16_v_u16m1_m(mask, base, value, vl) __riscv_th_vse16_v_u16m1_m(mask, base, value, vl)
#define __riscv_vse16_v_u16m2_m(mask, base, value, vl) __riscv_th_vse16_v_u16m2_m(mask, base, value, vl)
#define __riscv_vse16_v_u16m4_m(mask, base, value, vl) __riscv_th_vse16_v_u16m4_m(mask, base, value, vl)
#define __riscv_vse16_v_u16m8_m(mask, base, value, vl) __riscv_th_vse16_v_u16m8_m(mask, base, value, vl)
#define __riscv_vse32_v_f32m1(base, value, vl) __riscv_th_vse32_v_f32m1(base, value, vl)
#define __riscv_vse32_v_f32m2(base, value, vl) __riscv_th_vse32_v_f32m2(base, value, vl)
#define __riscv_vse32_v_f32m4(base, value, vl) __riscv_th_vse32_v_f32m4(base, value, vl)
#define __riscv_vse32_v_f32m8(base, value, vl) __riscv_th_vse32_v_f32m8(base, value, vl)
#define __riscv_vse32_v_i32m1(base, value, vl) __riscv_th_vse32_v_i32m1(base, value, vl)
#define __riscv_vse32_v_i32m2(base, value, vl) __riscv_th_vse32_v_i32m2(base, value, vl)
#define __riscv_vse32_v_i32m4(base, value, vl) __riscv_th_vse32_v_i32m4(base, value, vl)
#define __riscv_vse32_v_i32m8(base, value, vl) __riscv_th_vse32_v_i32m8(base, value, vl)
#define __riscv_vse32_v_u32m1(base, value, vl) __riscv_th_vse32_v_u32m1(base, value, vl)
#define __riscv_vse32_v_u32m2(base, value, vl) __riscv_th_vse32_v_u32m2(base, value, vl)
#define __riscv_vse32_v_u32m4(base, value, vl) __riscv_th_vse32_v_u32m4(base, value, vl)
#define __riscv_vse32_v_u32m8(base, value, vl) __riscv_th_vse32_v_u32m8(base, value, vl)
#define __riscv_vse32_v_f32m1_m(mask, base, value, vl) __riscv_th_vse32_v_f32m1_m(mask, base, value, vl)
#define __riscv_vse32_v_f32m2_m(mask, base, value, vl) __riscv_th_vse32_v_f32m2_m(mask, base, value, vl)
#define __riscv_vse32_v_f32m4_m(mask, base, value, vl) __riscv_th_vse32_v_f32m4_m(mask, base, value, vl)
#define __riscv_vse32_v_f32m8_m(mask, base, value, vl) __riscv_th_vse32_v_f32m8_m(mask, base, value, vl)
#define __riscv_vse32_v_i32m1_m(mask, base, value, vl) __riscv_th_vse32_v_i32m1_m(mask, base, value, vl)
#define __riscv_vse32_v_i32m2_m(mask, base, value, vl) __riscv_th_vse32_v_i32m2_m(mask, base, value, vl)
#define __riscv_vse32_v_i32m4_m(mask, base, value, vl) __riscv_th_vse32_v_i32m4_m(mask, base, value, vl)
#define __riscv_vse32_v_i32m8_m(mask, base, value, vl) __riscv_th_vse32_v_i32m8_m(mask, base, value, vl)
#define __riscv_vse32_v_u32m1_m(mask, base, value, vl) __riscv_th_vse32_v_u32m1_m(mask, base, value, vl)
#define __riscv_vse32_v_u32m2_m(mask, base, value, vl) __riscv_th_vse32_v_u32m2_m(mask, base, value, vl)
#define __riscv_vse32_v_u32m4_m(mask, base, value, vl) __riscv_th_vse32_v_u32m4_m(mask, base, value, vl)
#define __riscv_vse32_v_u32m8_m(mask, base, value, vl) __riscv_th_vse32_v_u32m8_m(mask, base, value, vl)
#define __riscv_vse64_v_f64m1(base, value, vl) __riscv_th_vse64_v_f64m1(base, value, vl)
#define __riscv_vse64_v_f64m2(base, value, vl) __riscv_th_vse64_v_f64m2(base, value, vl)
#define __riscv_vse64_v_f64m4(base, value, vl) __riscv_th_vse64_v_f64m4(base, value, vl)
#define __riscv_vse64_v_f64m8(base, value, vl) __riscv_th_vse64_v_f64m8(base, value, vl)
#define __riscv_vse64_v_i64m1(base, value, vl) __riscv_th_vse64_v_i64m1(base, value, vl)
#define __riscv_vse64_v_i64m2(base, value, vl) __riscv_th_vse64_v_i64m2(base, value, vl)
#define __riscv_vse64_v_i64m4(base, value, vl) __riscv_th_vse64_v_i64m4(base, value, vl)
#define __riscv_vse64_v_i64m8(base, value, vl) __riscv_th_vse64_v_i64m8(base, value, vl)
#define __riscv_vse64_v_u64m1(base, value, vl) __riscv_th_vse64_v_u64m1(base, value, vl)
#define __riscv_vse64_v_u64m2(base, value, vl) __riscv_th_vse64_v_u64m2(base, value, vl)
#define __riscv_vse64_v_u64m4(base, value, vl) __riscv_th_vse64_v_u64m4(base, value, vl)
#define __riscv_vse64_v_u64m8(base, value, vl) __riscv_th_vse64_v_u64m8(base, value, vl)
#define __riscv_vse64_v_f64m1_m(mask, base, value, vl) __riscv_th_vse64_v_f64m1_m(mask, base, value, vl)
#define __riscv_vse64_v_f64m2_m(mask, base, value, vl) __riscv_th_vse64_v_f64m2_m(mask, base, value, vl)
#define __riscv_vse64_v_f64m4_m(mask, base, value, vl) __riscv_th_vse64_v_f64m4_m(mask, base, value, vl)
#define __riscv_vse64_v_f64m8_m(mask, base, value, vl) __riscv_th_vse64_v_f64m8_m(mask, base, value, vl)
#define __riscv_vse64_v_i64m1_m(mask, base, value, vl) __riscv_th_vse64_v_i64m1_m(mask, base, value, vl)
#define __riscv_vse64_v_i64m2_m(mask, base, value, vl) __riscv_th_vse64_v_i64m2_m(mask, base, value, vl)
#define __riscv_vse64_v_i64m4_m(mask, base, value, vl) __riscv_th_vse64_v_i64m4_m(mask, base, value, vl)
#define __riscv_vse64_v_i64m8_m(mask, base, value, vl) __riscv_th_vse64_v_i64m8_m(mask, base, value, vl)
#define __riscv_vse64_v_u64m1_m(mask, base, value, vl) __riscv_th_vse64_v_u64m1_m(mask, base, value, vl)
#define __riscv_vse64_v_u64m2_m(mask, base, value, vl) __riscv_th_vse64_v_u64m2_m(mask, base, value, vl)
#define __riscv_vse64_v_u64m4_m(mask, base, value, vl) __riscv_th_vse64_v_u64m4_m(mask, base, value, vl)
#define __riscv_vse64_v_u64m8_m(mask, base, value, vl) __riscv_th_vse64_v_u64m8_m(mask, base, value, vl)

}] in
def th_unit_stride_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Strided loads
#define __riscv_vlsb_v_i8m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i8m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i8m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i8m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m1(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m2(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m4(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m8(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m1(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m2(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m4(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m8(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m1(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m2(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m4(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m8(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m1(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m2(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m4(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m8(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m1(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m2(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m4(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m8(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m1(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m2(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m4(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m8(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m1(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m2(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m4(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m8(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m1(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m2(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m4(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m8(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m1(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m1(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m2(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m2(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m4(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m4(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m8(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m8(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m1(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m1(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m2(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m2(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m4(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m4(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m8(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m8(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m1(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m1(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m2(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m2(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m4(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m4(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m8(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m8(src_ptr, stride, vl)

// Vector Strided stores
#define __riscv_vssb_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_i8m1(base, stride, value, vl) __riscv_th_vsse8_v_i8m1(base, stride, value, vl)
#define __riscv_vsse8_v_i8m2(base, stride, value, vl) __riscv_th_vsse8_v_i8m2(base, stride, value, vl)
#define __riscv_vsse8_v_i8m4(base, stride, value, vl) __riscv_th_vsse8_v_i8m4(base, stride, value, vl)
#define __riscv_vsse8_v_i8m8(base, stride, value, vl) __riscv_th_vsse8_v_i8m8(base, stride, value, vl)
#define __riscv_vsse8_v_u8m1(base, stride, value, vl) __riscv_th_vsse8_v_u8m1(base, stride, value, vl)
#define __riscv_vsse8_v_u8m2(base, stride, value, vl) __riscv_th_vsse8_v_u8m2(base, stride, value, vl)
#define __riscv_vsse8_v_u8m4(base, stride, value, vl) __riscv_th_vsse8_v_u8m4(base, stride, value, vl)
#define __riscv_vsse8_v_u8m8(base, stride, value, vl) __riscv_th_vsse8_v_u8m8(base, stride, value, vl)
#define __riscv_vsse8_v_i8m1_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_i8m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_i8m2_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_i8m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_i8m4_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_i8m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_i8m8_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_i8m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_u8m1_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_u8m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_u8m2_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_u8m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_u8m4_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_u8m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse8_v_u8m8_m(mask, base, bstride, value, vl) __riscv_th_vsse8_v_u8m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_f16m1(base, stride, value, vl) __riscv_th_vsse16_v_f16m1(base, stride, value, vl)
#define __riscv_vsse16_v_f16m2(base, stride, value, vl) __riscv_th_vsse16_v_f16m2(base, stride, value, vl)
#define __riscv_vsse16_v_f16m4(base, stride, value, vl) __riscv_th_vsse16_v_f16m4(base, stride, value, vl)
#define __riscv_vsse16_v_f16m8(base, stride, value, vl) __riscv_th_vsse16_v_f16m8(base, stride, value, vl)
#define __riscv_vsse16_v_i16m1(base, stride, value, vl) __riscv_th_vsse16_v_i16m1(base, stride, value, vl)
#define __riscv_vsse16_v_i16m2(base, stride, value, vl) __riscv_th_vsse16_v_i16m2(base, stride, value, vl)
#define __riscv_vsse16_v_i16m4(base, stride, value, vl) __riscv_th_vsse16_v_i16m4(base, stride, value, vl)
#define __riscv_vsse16_v_i16m8(base, stride, value, vl) __riscv_th_vsse16_v_i16m8(base, stride, value, vl)
#define __riscv_vsse16_v_u16m1(base, stride, value, vl) __riscv_th_vsse16_v_u16m1(base, stride, value, vl)
#define __riscv_vsse16_v_u16m2(base, stride, value, vl) __riscv_th_vsse16_v_u16m2(base, stride, value, vl)
#define __riscv_vsse16_v_u16m4(base, stride, value, vl) __riscv_th_vsse16_v_u16m4(base, stride, value, vl)
#define __riscv_vsse16_v_u16m8(base, stride, value, vl) __riscv_th_vsse16_v_u16m8(base, stride, value, vl)
#define __riscv_vsse16_v_f16m1_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_f16m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_f16m2_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_f16m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_f16m4_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_f16m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_f16m8_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_f16m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_i16m1_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_i16m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_i16m2_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_i16m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_i16m4_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_i16m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_i16m8_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_i16m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_u16m1_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_u16m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_u16m2_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_u16m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_u16m4_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_u16m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse16_v_u16m8_m(mask, base, bstride, value, vl) __riscv_th_vsse16_v_u16m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_f32m1(base, stride, value, vl) __riscv_th_vsse32_v_f32m1(base, stride, value, vl)
#define __riscv_vsse32_v_f32m2(base, stride, value, vl) __riscv_th_vsse32_v_f32m2(base, stride, value, vl)
#define __riscv_vsse32_v_f32m4(base, stride, value, vl) __riscv_th_vsse32_v_f32m4(base, stride, value, vl)
#define __riscv_vsse32_v_f32m8(base, stride, value, vl) __riscv_th_vsse32_v_f32m8(base, stride, value, vl)
#define __riscv_vsse32_v_i32m1(base, stride, value, vl) __riscv_th_vsse32_v_i32m1(base, stride, value, vl)
#define __riscv_vsse32_v_i32m2(base, stride, value, vl) __riscv_th_vsse32_v_i32m2(base, stride, value, vl)
#define __riscv_vsse32_v_i32m4(base, stride, value, vl) __riscv_th_vsse32_v_i32m4(base, stride, value, vl)
#define __riscv_vsse32_v_i32m8(base, stride, value, vl) __riscv_th_vsse32_v_i32m8(base, stride, value, vl)
#define __riscv_vsse32_v_u32m1(base, stride, value, vl) __riscv_th_vsse32_v_u32m1(base, stride, value, vl)
#define __riscv_vsse32_v_u32m2(base, stride, value, vl) __riscv_th_vsse32_v_u32m2(base, stride, value, vl)
#define __riscv_vsse32_v_u32m4(base, stride, value, vl) __riscv_th_vsse32_v_u32m4(base, stride, value, vl)
#define __riscv_vsse32_v_u32m8(base, stride, value, vl) __riscv_th_vsse32_v_u32m8(base, stride, value, vl)
#define __riscv_vsse32_v_f32m1_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_f32m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_f32m2_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_f32m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_f32m4_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_f32m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_f32m8_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_f32m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_i32m1_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_i32m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_i32m2_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_i32m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_i32m4_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_i32m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_i32m8_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_i32m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_u32m1_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_u32m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_u32m2_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_u32m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_u32m4_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_u32m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse32_v_u32m8_m(mask, base, bstride, value, vl) __riscv_th_vsse32_v_u32m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_f64m1(base, stride, value, vl) __riscv_th_vsse64_v_f64m1(base, stride, value, vl)
#define __riscv_vsse64_v_f64m2(base, stride, value, vl) __riscv_th_vsse64_v_f64m2(base, stride, value, vl)
#define __riscv_vsse64_v_f64m4(base, stride, value, vl) __riscv_th_vsse64_v_f64m4(base, stride, value, vl)
#define __riscv_vsse64_v_f64m8(base, stride, value, vl) __riscv_th_vsse64_v_f64m8(base, stride, value, vl)
#define __riscv_vsse64_v_i64m1(base, stride, value, vl) __riscv_th_vsse64_v_i64m1(base, stride, value, vl)
#define __riscv_vsse64_v_i64m2(base, stride, value, vl) __riscv_th_vsse64_v_i64m2(base, stride, value, vl)
#define __riscv_vsse64_v_i64m4(base, stride, value, vl) __riscv_th_vsse64_v_i64m4(base, stride, value, vl)
#define __riscv_vsse64_v_i64m8(base, stride, value, vl) __riscv_th_vsse64_v_i64m8(base, stride, value, vl)
#define __riscv_vsse64_v_u64m1(base, stride, value, vl) __riscv_th_vsse64_v_u64m1(base, stride, value, vl)
#define __riscv_vsse64_v_u64m2(base, stride, value, vl) __riscv_th_vsse64_v_u64m2(base, stride, value, vl)
#define __riscv_vsse64_v_u64m4(base, stride, value, vl) __riscv_th_vsse64_v_u64m4(base, stride, value, vl)
#define __riscv_vsse64_v_u64m8(base, stride, value, vl) __riscv_th_vsse64_v_u64m8(base, stride, value, vl)
#define __riscv_vsse64_v_f64m1_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_f64m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_f64m2_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_f64m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_f64m4_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_f64m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_f64m8_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_f64m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_i64m1_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_i64m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_i64m2_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_i64m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_i64m4_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_i64m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_i64m8_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_i64m8_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_u64m1_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_u64m1_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_u64m2_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_u64m2_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_u64m4_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_u64m4_m(mask, base, bstride, value, vl)
#define __riscv_vsse64_v_u64m8_m(mask, base, bstride, value, vl) __riscv_th_vsse64_v_u64m8_m(mask, base, bstride, value, vl)

}] in
def th_strided_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Unit-stride Fault-Only-First loads
#define __riscv_vle8ff_v_i8m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_i8m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_i8m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_i8m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m8(src_ptr, new_vl_ptr, vl)

#define __riscv_vle8ff_v_u8m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_u8m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_u8m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_u8m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m8(src_ptr, new_vl_ptr, vl)

#define __riscv_vle16ff_v_f16m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_f16m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_f16m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_f16m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m8(src_ptr, new_vl_ptr, vl)
}] in
def th_unit_stride_ff_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Indexed Load/Store Operations
#define __riscv_vlxb_v_i8m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i8m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i8m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i8m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m8(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m8(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m8(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m8(src_ptr, indexed, vl)
#define __riscv_vsxb_v_i8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m8(dst_ptr, indexed, value, vl)
}] in
def th_indexed_wrapper_macros: RVVHeader;

let HeaderCode =
[{
#define __riscv_vlseg2b_v_i8m1x2(base, vl) __riscv_th_vlseg2b_v_i8m1x2(base, vl)
#define __riscv_vlseg2e16_v_f16m1x2(base, vl) __riscv_th_vlseg2e16_v_f16m1x2(base, vl)
#define __riscv_vlseg2e16_v_f16m2x2(base, vl) __riscv_th_vlseg2e16_v_f16m2x2(base, vl)
#define __riscv_vlseg2e16_v_f16m4x2(base, vl) __riscv_th_vlseg2e16_v_f16m4x2(base, vl)
#define __riscv_vlseg2e16_v_i16m1x2(base, vl) __riscv_th_vlseg2e16_v_i16m1x2(base, vl)
#define __riscv_vlseg2e16_v_i16m2x2(base, vl) __riscv_th_vlseg2e16_v_i16m2x2(base, vl)
#define __riscv_vlseg2e16_v_i16m4x2(base, vl) __riscv_th_vlseg2e16_v_i16m4x2(base, vl)
#define __riscv_vlseg2e16_v_u16m1x2(base, vl) __riscv_th_vlseg2e16_v_u16m1x2(base, vl)
#define __riscv_vlseg2e16_v_u16m2x2(base, vl) __riscv_th_vlseg2e16_v_u16m2x2(base, vl)
#define __riscv_vlseg2e16_v_u16m4x2(base, vl) __riscv_th_vlseg2e16_v_u16m4x2(base, vl)
#define __riscv_vlseg2e16ff_v_f16m1x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_f16m1x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_f16m2x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_f16m2x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_f16m4x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_f16m4x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_i16m1x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_i16m1x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_i16m2x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_i16m2x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_i16m4x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_i16m4x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_u16m1x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_u16m1x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_u16m2x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_u16m2x2(base, new_vl, vl)
#define __riscv_vlseg2e16ff_v_u16m4x2(base, new_vl, vl) __riscv_th_vlseg2e16ff_v_u16m4x2(base, new_vl, vl)
#define __riscv_vlseg2e32_v_f32m1x2(base, vl) __riscv_th_vlseg2e32_v_f32m1x2(base, vl)
#define __riscv_vlseg2e32_v_f32m2x2(base, vl) __riscv_th_vlseg2e32_v_f32m2x2(base, vl)
#define __riscv_vlseg2e32_v_f32m4x2(base, vl) __riscv_th_vlseg2e32_v_f32m4x2(base, vl)
#define __riscv_vlseg2e32_v_i32m1x2(base, vl) __riscv_th_vlseg2e32_v_i32m1x2(base, vl)
#define __riscv_vlseg2e32_v_i32m2x2(base, vl) __riscv_th_vlseg2e32_v_i32m2x2(base, vl)
#define __riscv_vlseg2e32_v_i32m4x2(base, vl) __riscv_th_vlseg2e32_v_i32m4x2(base, vl)
#define __riscv_vlseg2e32_v_u32m1x2(base, vl) __riscv_th_vlseg2e32_v_u32m1x2(base, vl)
#define __riscv_vlseg2e32_v_u32m2x2(base, vl) __riscv_th_vlseg2e32_v_u32m2x2(base, vl)
#define __riscv_vlseg2e32_v_u32m4x2(base, vl) __riscv_th_vlseg2e32_v_u32m4x2(base, vl)
#define __riscv_vlseg2e32ff_v_f32m1x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_f32m1x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_f32m2x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_f32m2x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_f32m4x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_f32m4x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_i32m1x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_i32m1x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_i32m2x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_i32m2x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_i32m4x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_i32m4x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_u32m1x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_u32m1x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_u32m2x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_u32m2x2(base, new_vl, vl)
#define __riscv_vlseg2e32ff_v_u32m4x2(base, new_vl, vl) __riscv_th_vlseg2e32ff_v_u32m4x2(base, new_vl, vl)
#define __riscv_vlseg2e64_v_f64m1x2(base, vl) __riscv_th_vlseg2e64_v_f64m1x2(base, vl)
#define __riscv_vlseg2e64_v_f64m2x2(base, vl) __riscv_th_vlseg2e64_v_f64m2x2(base, vl)
#define __riscv_vlseg2e64_v_f64m4x2(base, vl) __riscv_th_vlseg2e64_v_f64m4x2(base, vl)
#define __riscv_vlseg2e64_v_i64m1x2(base, vl) __riscv_th_vlseg2e64_v_i64m1x2(base, vl)
#define __riscv_vlseg2e64_v_i64m2x2(base, vl) __riscv_th_vlseg2e64_v_i64m2x2(base, vl)
#define __riscv_vlseg2e64_v_i64m4x2(base, vl) __riscv_th_vlseg2e64_v_i64m4x2(base, vl)
#define __riscv_vlseg2e64_v_u64m1x2(base, vl) __riscv_th_vlseg2e64_v_u64m1x2(base, vl)
#define __riscv_vlseg2e64_v_u64m2x2(base, vl) __riscv_th_vlseg2e64_v_u64m2x2(base, vl)
#define __riscv_vlseg2e64_v_u64m4x2(base, vl) __riscv_th_vlseg2e64_v_u64m4x2(base, vl)
#define __riscv_vlseg2e64ff_v_f64m1x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_f64m1x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_f64m2x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_f64m2x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_f64m4x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_f64m4x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_i64m1x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_i64m1x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_i64m2x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_i64m2x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_i64m4x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_i64m4x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_u64m1x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_u64m1x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_u64m2x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_u64m2x2(base, new_vl, vl)
#define __riscv_vlseg2e64ff_v_u64m4x2(base, new_vl, vl) __riscv_th_vlseg2e64ff_v_u64m4x2(base, new_vl, vl)
#define __riscv_vlseg2e8_v_i8m1x2(base, vl) __riscv_th_vlseg2e8_v_i8m1x2(base, vl)
#define __riscv_vlseg2e8_v_i8m2x2(base, vl) __riscv_th_vlseg2e8_v_i8m2x2(base, vl)
#define __riscv_vlseg2e8_v_i8m4x2(base, vl) __riscv_th_vlseg2e8_v_i8m4x2(base, vl)
#define __riscv_vlseg2e8_v_u8m1x2(base, vl) __riscv_th_vlseg2e8_v_u8m1x2(base, vl)
#define __riscv_vlseg2e8_v_u8m2x2(base, vl) __riscv_th_vlseg2e8_v_u8m2x2(base, vl)
#define __riscv_vlseg2e8_v_u8m4x2(base, vl) __riscv_th_vlseg2e8_v_u8m4x2(base, vl)
#define __riscv_vlseg2e8ff_v_i8m1x2(base, new_vl, vl) __riscv_th_vlseg2e8ff_v_i8m1x2(base, new_vl, vl)
#define __riscv_vlseg2e8ff_v_i8m2x2(base, new_vl, vl) __riscv_th_vlseg2e8ff_v_i8m2x2(base, new_vl, vl)
#define __riscv_vlseg2e8ff_v_i8m4x2(base, new_vl, vl) __riscv_th_vlseg2e8ff_v_i8m4x2(base, new_vl, vl)
#define __riscv_vlseg2e8ff_v_u8m1x2(base, new_vl, vl) __riscv_th_vlseg2e8ff_v_u8m1x2(base, new_vl, vl)
#define __riscv_vlseg2e8ff_v_u8m2x2(base, new_vl, vl) __riscv_th_vlseg2e8ff_v_u8m2x2(base, new_vl, vl)
#define __riscv_vlseg2e8ff_v_u8m4x2(base, new_vl, vl) __riscv_th_vlseg2e8ff_v_u8m4x2(base, new_vl, vl)
#define __riscv_vlseg3e16_v_f16m1x3(base, vl) __riscv_th_vlseg3e16_v_f16m1x3(base, vl)
#define __riscv_vlseg3e16_v_f16m2x3(base, vl) __riscv_th_vlseg3e16_v_f16m2x3(base, vl)
#define __riscv_vlseg3e16_v_i16m1x3(base, vl) __riscv_th_vlseg3e16_v_i16m1x3(base, vl)
#define __riscv_vlseg3e16_v_i16m2x3(base, vl) __riscv_th_vlseg3e16_v_i16m2x3(base, vl)
#define __riscv_vlseg3e16_v_u16m1x3(base, vl) __riscv_th_vlseg3e16_v_u16m1x3(base, vl)
#define __riscv_vlseg3e16_v_u16m2x3(base, vl) __riscv_th_vlseg3e16_v_u16m2x3(base, vl)
#define __riscv_vlseg3e16ff_v_f16m1x3(base, new_vl, vl) __riscv_th_vlseg3e16ff_v_f16m1x3(base, new_vl, vl)
#define __riscv_vlseg3e16ff_v_f16m2x3(base, new_vl, vl) __riscv_th_vlseg3e16ff_v_f16m2x3(base, new_vl, vl)
#define __riscv_vlseg3e16ff_v_i16m1x3(base, new_vl, vl) __riscv_th_vlseg3e16ff_v_i16m1x3(base, new_vl, vl)
#define __riscv_vlseg3e16ff_v_i16m2x3(base, new_vl, vl) __riscv_th_vlseg3e16ff_v_i16m2x3(base, new_vl, vl)
#define __riscv_vlseg3e16ff_v_u16m1x3(base, new_vl, vl) __riscv_th_vlseg3e16ff_v_u16m1x3(base, new_vl, vl)
#define __riscv_vlseg3e16ff_v_u16m2x3(base, new_vl, vl) __riscv_th_vlseg3e16ff_v_u16m2x3(base, new_vl, vl)
#define __riscv_vlseg3e32_v_f32m1x3(base, vl) __riscv_th_vlseg3e32_v_f32m1x3(base, vl)
#define __riscv_vlseg3e32_v_f32m2x3(base, vl) __riscv_th_vlseg3e32_v_f32m2x3(base, vl)
#define __riscv_vlseg3e32_v_i32m1x3(base, vl) __riscv_th_vlseg3e32_v_i32m1x3(base, vl)
#define __riscv_vlseg3e32_v_i32m2x3(base, vl) __riscv_th_vlseg3e32_v_i32m2x3(base, vl)
#define __riscv_vlseg3e32_v_u32m1x3(base, vl) __riscv_th_vlseg3e32_v_u32m1x3(base, vl)
#define __riscv_vlseg3e32_v_u32m2x3(base, vl) __riscv_th_vlseg3e32_v_u32m2x3(base, vl)
#define __riscv_vlseg3e32ff_v_f32m1x3(base, new_vl, vl) __riscv_th_vlseg3e32ff_v_f32m1x3(base, new_vl, vl)
#define __riscv_vlseg3e32ff_v_f32m2x3(base, new_vl, vl) __riscv_th_vlseg3e32ff_v_f32m2x3(base, new_vl, vl)
#define __riscv_vlseg3e32ff_v_i32m1x3(base, new_vl, vl) __riscv_th_vlseg3e32ff_v_i32m1x3(base, new_vl, vl)
#define __riscv_vlseg3e32ff_v_i32m2x3(base, new_vl, vl) __riscv_th_vlseg3e32ff_v_i32m2x3(base, new_vl, vl)
#define __riscv_vlseg3e32ff_v_u32m1x3(base, new_vl, vl) __riscv_th_vlseg3e32ff_v_u32m1x3(base, new_vl, vl)
#define __riscv_vlseg3e32ff_v_u32m2x3(base, new_vl, vl) __riscv_th_vlseg3e32ff_v_u32m2x3(base, new_vl, vl)
#define __riscv_vlseg3e64_v_f64m1x3(base, vl) __riscv_th_vlseg3e64_v_f64m1x3(base, vl)
#define __riscv_vlseg3e64_v_f64m2x3(base, vl) __riscv_th_vlseg3e64_v_f64m2x3(base, vl)
#define __riscv_vlseg3e64_v_i64m1x3(base, vl) __riscv_th_vlseg3e64_v_i64m1x3(base, vl)
#define __riscv_vlseg3e64_v_i64m2x3(base, vl) __riscv_th_vlseg3e64_v_i64m2x3(base, vl)
#define __riscv_vlseg3e64_v_u64m1x3(base, vl) __riscv_th_vlseg3e64_v_u64m1x3(base, vl)
#define __riscv_vlseg3e64_v_u64m2x3(base, vl) __riscv_th_vlseg3e64_v_u64m2x3(base, vl)
#define __riscv_vlseg3e64ff_v_f64m1x3(base, new_vl, vl) __riscv_th_vlseg3e64ff_v_f64m1x3(base, new_vl, vl)
#define __riscv_vlseg3e64ff_v_f64m2x3(base, new_vl, vl) __riscv_th_vlseg3e64ff_v_f64m2x3(base, new_vl, vl)
#define __riscv_vlseg3e64ff_v_i64m1x3(base, new_vl, vl) __riscv_th_vlseg3e64ff_v_i64m1x3(base, new_vl, vl)
#define __riscv_vlseg3e64ff_v_i64m2x3(base, new_vl, vl) __riscv_th_vlseg3e64ff_v_i64m2x3(base, new_vl, vl)
#define __riscv_vlseg3e64ff_v_u64m1x3(base, new_vl, vl) __riscv_th_vlseg3e64ff_v_u64m1x3(base, new_vl, vl)
#define __riscv_vlseg3e64ff_v_u64m2x3(base, new_vl, vl) __riscv_th_vlseg3e64ff_v_u64m2x3(base, new_vl, vl)
#define __riscv_vlseg3e8_v_i8m1x3(base, vl) __riscv_th_vlseg3e8_v_i8m1x3(base, vl)
#define __riscv_vlseg3e8_v_i8m2x3(base, vl) __riscv_th_vlseg3e8_v_i8m2x3(base, vl)
#define __riscv_vlseg3e8_v_u8m1x3(base, vl) __riscv_th_vlseg3e8_v_u8m1x3(base, vl)
#define __riscv_vlseg3e8_v_u8m2x3(base, vl) __riscv_th_vlseg3e8_v_u8m2x3(base, vl)
#define __riscv_vlseg3e8ff_v_i8m1x3(base, new_vl, vl) __riscv_th_vlseg3e8ff_v_i8m1x3(base, new_vl, vl)
#define __riscv_vlseg3e8ff_v_i8m2x3(base, new_vl, vl) __riscv_th_vlseg3e8ff_v_i8m2x3(base, new_vl, vl)
#define __riscv_vlseg3e8ff_v_u8m1x3(base, new_vl, vl) __riscv_th_vlseg3e8ff_v_u8m1x3(base, new_vl, vl)
#define __riscv_vlseg3e8ff_v_u8m2x3(base, new_vl, vl) __riscv_th_vlseg3e8ff_v_u8m2x3(base, new_vl, vl)
#define __riscv_vlseg4e16_v_f16m1x4(base, vl) __riscv_th_vlseg4e16_v_f16m1x4(base, vl)
#define __riscv_vlseg4e16_v_f16m2x4(base, vl) __riscv_th_vlseg4e16_v_f16m2x4(base, vl)
#define __riscv_vlseg4e16_v_i16m1x4(base, vl) __riscv_th_vlseg4e16_v_i16m1x4(base, vl)
#define __riscv_vlseg4e16_v_i16m2x4(base, vl) __riscv_th_vlseg4e16_v_i16m2x4(base, vl)
#define __riscv_vlseg4e16_v_u16m1x4(base, vl) __riscv_th_vlseg4e16_v_u16m1x4(base, vl)
#define __riscv_vlseg4e16_v_u16m2x4(base, vl) __riscv_th_vlseg4e16_v_u16m2x4(base, vl)
#define __riscv_vlseg4e16ff_v_f16m1x4(base, new_vl, vl) __riscv_th_vlseg4e16ff_v_f16m1x4(base, new_vl, vl)
#define __riscv_vlseg4e16ff_v_f16m2x4(base, new_vl, vl) __riscv_th_vlseg4e16ff_v_f16m2x4(base, new_vl, vl)
#define __riscv_vlseg4e16ff_v_i16m1x4(base, new_vl, vl) __riscv_th_vlseg4e16ff_v_i16m1x4(base, new_vl, vl)
#define __riscv_vlseg4e16ff_v_i16m2x4(base, new_vl, vl) __riscv_th_vlseg4e16ff_v_i16m2x4(base, new_vl, vl)
#define __riscv_vlseg4e16ff_v_u16m1x4(base, new_vl, vl) __riscv_th_vlseg4e16ff_v_u16m1x4(base, new_vl, vl)
#define __riscv_vlseg4e16ff_v_u16m2x4(base, new_vl, vl) __riscv_th_vlseg4e16ff_v_u16m2x4(base, new_vl, vl)
#define __riscv_vlseg4e32_v_f32m1x4(base, vl) __riscv_th_vlseg4e32_v_f32m1x4(base, vl)
#define __riscv_vlseg4e32_v_f32m2x4(base, vl) __riscv_th_vlseg4e32_v_f32m2x4(base, vl)
#define __riscv_vlseg4e32_v_i32m1x4(base, vl) __riscv_th_vlseg4e32_v_i32m1x4(base, vl)
#define __riscv_vlseg4e32_v_i32m2x4(base, vl) __riscv_th_vlseg4e32_v_i32m2x4(base, vl)
#define __riscv_vlseg4e32_v_u32m1x4(base, vl) __riscv_th_vlseg4e32_v_u32m1x4(base, vl)
#define __riscv_vlseg4e32_v_u32m2x4(base, vl) __riscv_th_vlseg4e32_v_u32m2x4(base, vl)
#define __riscv_vlseg4e32ff_v_f32m1x4(base, new_vl, vl) __riscv_th_vlseg4e32ff_v_f32m1x4(base, new_vl, vl)
#define __riscv_vlseg4e32ff_v_f32m2x4(base, new_vl, vl) __riscv_th_vlseg4e32ff_v_f32m2x4(base, new_vl, vl)
#define __riscv_vlseg4e32ff_v_i32m1x4(base, new_vl, vl) __riscv_th_vlseg4e32ff_v_i32m1x4(base, new_vl, vl)
#define __riscv_vlseg4e32ff_v_i32m2x4(base, new_vl, vl) __riscv_th_vlseg4e32ff_v_i32m2x4(base, new_vl, vl)
#define __riscv_vlseg4e32ff_v_u32m1x4(base, new_vl, vl) __riscv_th_vlseg4e32ff_v_u32m1x4(base, new_vl, vl)
#define __riscv_vlseg4e32ff_v_u32m2x4(base, new_vl, vl) __riscv_th_vlseg4e32ff_v_u32m2x4(base, new_vl, vl)
#define __riscv_vlseg4e64_v_f64m1x4(base, vl) __riscv_th_vlseg4e64_v_f64m1x4(base, vl)
#define __riscv_vlseg4e64_v_f64m2x4(base, vl) __riscv_th_vlseg4e64_v_f64m2x4(base, vl)
#define __riscv_vlseg4e64_v_i64m1x4(base, vl) __riscv_th_vlseg4e64_v_i64m1x4(base, vl)
#define __riscv_vlseg4e64_v_i64m2x4(base, vl) __riscv_th_vlseg4e64_v_i64m2x4(base, vl)
#define __riscv_vlseg4e64_v_u64m1x4(base, vl) __riscv_th_vlseg4e64_v_u64m1x4(base, vl)
#define __riscv_vlseg4e64_v_u64m2x4(base, vl) __riscv_th_vlseg4e64_v_u64m2x4(base, vl)
#define __riscv_vlseg4e64ff_v_f64m1x4(base, new_vl, vl) __riscv_th_vlseg4e64ff_v_f64m1x4(base, new_vl, vl)
#define __riscv_vlseg4e64ff_v_f64m2x4(base, new_vl, vl) __riscv_th_vlseg4e64ff_v_f64m2x4(base, new_vl, vl)
#define __riscv_vlseg4e64ff_v_i64m1x4(base, new_vl, vl) __riscv_th_vlseg4e64ff_v_i64m1x4(base, new_vl, vl)
#define __riscv_vlseg4e64ff_v_i64m2x4(base, new_vl, vl) __riscv_th_vlseg4e64ff_v_i64m2x4(base, new_vl, vl)
#define __riscv_vlseg4e64ff_v_u64m1x4(base, new_vl, vl) __riscv_th_vlseg4e64ff_v_u64m1x4(base, new_vl, vl)
#define __riscv_vlseg4e64ff_v_u64m2x4(base, new_vl, vl) __riscv_th_vlseg4e64ff_v_u64m2x4(base, new_vl, vl)
#define __riscv_vlseg4e8_v_i8m1x4(base, vl) __riscv_th_vlseg4e8_v_i8m1x4(base, vl)
#define __riscv_vlseg4e8_v_i8m2x4(base, vl) __riscv_th_vlseg4e8_v_i8m2x4(base, vl)
#define __riscv_vlseg4e8_v_u8m1x4(base, vl) __riscv_th_vlseg4e8_v_u8m1x4(base, vl)
#define __riscv_vlseg4e8_v_u8m2x4(base, vl) __riscv_th_vlseg4e8_v_u8m2x4(base, vl)
#define __riscv_vlseg4e8ff_v_i8m1x4(base, new_vl, vl) __riscv_th_vlseg4e8ff_v_i8m1x4(base, new_vl, vl)
#define __riscv_vlseg4e8ff_v_i8m2x4(base, new_vl, vl) __riscv_th_vlseg4e8ff_v_i8m2x4(base, new_vl, vl)
#define __riscv_vlseg4e8ff_v_u8m1x4(base, new_vl, vl) __riscv_th_vlseg4e8ff_v_u8m1x4(base, new_vl, vl)
#define __riscv_vlseg4e8ff_v_u8m2x4(base, new_vl, vl) __riscv_th_vlseg4e8ff_v_u8m2x4(base, new_vl, vl)
#define __riscv_vlseg5e16_v_f16m1x5(base, vl) __riscv_th_vlseg5e16_v_f16m1x5(base, vl)
#define __riscv_vlseg5e16_v_i16m1x5(base, vl) __riscv_th_vlseg5e16_v_i16m1x5(base, vl)
#define __riscv_vlseg5e16_v_u16m1x5(base, vl) __riscv_th_vlseg5e16_v_u16m1x5(base, vl)
#define __riscv_vlseg5e16ff_v_f16m1x5(base, new_vl, vl) __riscv_th_vlseg5e16ff_v_f16m1x5(base, new_vl, vl)
#define __riscv_vlseg5e16ff_v_i16m1x5(base, new_vl, vl) __riscv_th_vlseg5e16ff_v_i16m1x5(base, new_vl, vl)
#define __riscv_vlseg5e16ff_v_u16m1x5(base, new_vl, vl) __riscv_th_vlseg5e16ff_v_u16m1x5(base, new_vl, vl)
#define __riscv_vlseg5e32_v_f32m1x5(base, vl) __riscv_th_vlseg5e32_v_f32m1x5(base, vl)
#define __riscv_vlseg5e32_v_i32m1x5(base, vl) __riscv_th_vlseg5e32_v_i32m1x5(base, vl)
#define __riscv_vlseg5e32_v_u32m1x5(base, vl) __riscv_th_vlseg5e32_v_u32m1x5(base, vl)
#define __riscv_vlseg5e32ff_v_f32m1x5(base, new_vl, vl) __riscv_th_vlseg5e32ff_v_f32m1x5(base, new_vl, vl)
#define __riscv_vlseg5e32ff_v_i32m1x5(base, new_vl, vl) __riscv_th_vlseg5e32ff_v_i32m1x5(base, new_vl, vl)
#define __riscv_vlseg5e32ff_v_u32m1x5(base, new_vl, vl) __riscv_th_vlseg5e32ff_v_u32m1x5(base, new_vl, vl)
#define __riscv_vlseg5e64_v_f64m1x5(base, vl) __riscv_th_vlseg5e64_v_f64m1x5(base, vl)
#define __riscv_vlseg5e64_v_i64m1x5(base, vl) __riscv_th_vlseg5e64_v_i64m1x5(base, vl)
#define __riscv_vlseg5e64_v_u64m1x5(base, vl) __riscv_th_vlseg5e64_v_u64m1x5(base, vl)
#define __riscv_vlseg5e64ff_v_f64m1x5(base, new_vl, vl) __riscv_th_vlseg5e64ff_v_f64m1x5(base, new_vl, vl)
#define __riscv_vlseg5e64ff_v_i64m1x5(base, new_vl, vl) __riscv_th_vlseg5e64ff_v_i64m1x5(base, new_vl, vl)
#define __riscv_vlseg5e64ff_v_u64m1x5(base, new_vl, vl) __riscv_th_vlseg5e64ff_v_u64m1x5(base, new_vl, vl)
#define __riscv_vlseg5e8_v_i8m1x5(base, vl) __riscv_th_vlseg5e8_v_i8m1x5(base, vl)
#define __riscv_vlseg5e8_v_u8m1x5(base, vl) __riscv_th_vlseg5e8_v_u8m1x5(base, vl)
#define __riscv_vlseg5e8ff_v_i8m1x5(base, new_vl, vl) __riscv_th_vlseg5e8ff_v_i8m1x5(base, new_vl, vl)
#define __riscv_vlseg5e8ff_v_u8m1x5(base, new_vl, vl) __riscv_th_vlseg5e8ff_v_u8m1x5(base, new_vl, vl)
#define __riscv_vlseg6e16_v_f16m1x6(base, vl) __riscv_th_vlseg6e16_v_f16m1x6(base, vl)
#define __riscv_vlseg6e16_v_i16m1x6(base, vl) __riscv_th_vlseg6e16_v_i16m1x6(base, vl)
#define __riscv_vlseg6e16_v_u16m1x6(base, vl) __riscv_th_vlseg6e16_v_u16m1x6(base, vl)
#define __riscv_vlseg6e16ff_v_f16m1x6(base, new_vl, vl) __riscv_th_vlseg6e16ff_v_f16m1x6(base, new_vl, vl)
#define __riscv_vlseg6e16ff_v_i16m1x6(base, new_vl, vl) __riscv_th_vlseg6e16ff_v_i16m1x6(base, new_vl, vl)
#define __riscv_vlseg6e16ff_v_u16m1x6(base, new_vl, vl) __riscv_th_vlseg6e16ff_v_u16m1x6(base, new_vl, vl)
#define __riscv_vlseg6e32_v_f32m1x6(base, vl) __riscv_th_vlseg6e32_v_f32m1x6(base, vl)
#define __riscv_vlseg6e32_v_i32m1x6(base, vl) __riscv_th_vlseg6e32_v_i32m1x6(base, vl)
#define __riscv_vlseg6e32_v_u32m1x6(base, vl) __riscv_th_vlseg6e32_v_u32m1x6(base, vl)
#define __riscv_vlseg6e32ff_v_f32m1x6(base, new_vl, vl) __riscv_th_vlseg6e32ff_v_f32m1x6(base, new_vl, vl)
#define __riscv_vlseg6e32ff_v_i32m1x6(base, new_vl, vl) __riscv_th_vlseg6e32ff_v_i32m1x6(base, new_vl, vl)
#define __riscv_vlseg6e32ff_v_u32m1x6(base, new_vl, vl) __riscv_th_vlseg6e32ff_v_u32m1x6(base, new_vl, vl)
#define __riscv_vlseg6e64_v_f64m1x6(base, vl) __riscv_th_vlseg6e64_v_f64m1x6(base, vl)
#define __riscv_vlseg6e64_v_i64m1x6(base, vl) __riscv_th_vlseg6e64_v_i64m1x6(base, vl)
#define __riscv_vlseg6e64_v_u64m1x6(base, vl) __riscv_th_vlseg6e64_v_u64m1x6(base, vl)
#define __riscv_vlseg6e64ff_v_f64m1x6(base, new_vl, vl) __riscv_th_vlseg6e64ff_v_f64m1x6(base, new_vl, vl)
#define __riscv_vlseg6e64ff_v_i64m1x6(base, new_vl, vl) __riscv_th_vlseg6e64ff_v_i64m1x6(base, new_vl, vl)
#define __riscv_vlseg6e64ff_v_u64m1x6(base, new_vl, vl) __riscv_th_vlseg6e64ff_v_u64m1x6(base, new_vl, vl)
#define __riscv_vlseg6e8_v_i8m1x6(base, vl) __riscv_th_vlseg6e8_v_i8m1x6(base, vl)
#define __riscv_vlseg6e8_v_u8m1x6(base, vl) __riscv_th_vlseg6e8_v_u8m1x6(base, vl)
#define __riscv_vlseg6e8ff_v_i8m1x6(base, new_vl, vl) __riscv_th_vlseg6e8ff_v_i8m1x6(base, new_vl, vl)
#define __riscv_vlseg6e8ff_v_u8m1x6(base, new_vl, vl) __riscv_th_vlseg6e8ff_v_u8m1x6(base, new_vl, vl)
#define __riscv_vlseg7e16_v_f16m1x7(base, vl) __riscv_th_vlseg7e16_v_f16m1x7(base, vl)
#define __riscv_vlseg7e16_v_i16m1x7(base, vl) __riscv_th_vlseg7e16_v_i16m1x7(base, vl)
#define __riscv_vlseg7e16_v_u16m1x7(base, vl) __riscv_th_vlseg7e16_v_u16m1x7(base, vl)
#define __riscv_vlseg7e16ff_v_f16m1x7(base, new_vl, vl) __riscv_th_vlseg7e16ff_v_f16m1x7(base, new_vl, vl)
#define __riscv_vlseg7e16ff_v_i16m1x7(base, new_vl, vl) __riscv_th_vlseg7e16ff_v_i16m1x7(base, new_vl, vl)
#define __riscv_vlseg7e16ff_v_u16m1x7(base, new_vl, vl) __riscv_th_vlseg7e16ff_v_u16m1x7(base, new_vl, vl)
#define __riscv_vlseg7e32_v_f32m1x7(base, vl) __riscv_th_vlseg7e32_v_f32m1x7(base, vl)
#define __riscv_vlseg7e32_v_i32m1x7(base, vl) __riscv_th_vlseg7e32_v_i32m1x7(base, vl)
#define __riscv_vlseg7e32_v_u32m1x7(base, vl) __riscv_th_vlseg7e32_v_u32m1x7(base, vl)
#define __riscv_vlseg7e32ff_v_f32m1x7(base, new_vl, vl) __riscv_th_vlseg7e32ff_v_f32m1x7(base, new_vl, vl)
#define __riscv_vlseg7e32ff_v_i32m1x7(base, new_vl, vl) __riscv_th_vlseg7e32ff_v_i32m1x7(base, new_vl, vl)
#define __riscv_vlseg7e32ff_v_u32m1x7(base, new_vl, vl) __riscv_th_vlseg7e32ff_v_u32m1x7(base, new_vl, vl)
#define __riscv_vlseg7e64_v_f64m1x7(base, vl) __riscv_th_vlseg7e64_v_f64m1x7(base, vl)
#define __riscv_vlseg7e64_v_i64m1x7(base, vl) __riscv_th_vlseg7e64_v_i64m1x7(base, vl)
#define __riscv_vlseg7e64_v_u64m1x7(base, vl) __riscv_th_vlseg7e64_v_u64m1x7(base, vl)
#define __riscv_vlseg7e64ff_v_f64m1x7(base, new_vl, vl) __riscv_th_vlseg7e64ff_v_f64m1x7(base, new_vl, vl)
#define __riscv_vlseg7e64ff_v_i64m1x7(base, new_vl, vl) __riscv_th_vlseg7e64ff_v_i64m1x7(base, new_vl, vl)
#define __riscv_vlseg7e64ff_v_u64m1x7(base, new_vl, vl) __riscv_th_vlseg7e64ff_v_u64m1x7(base, new_vl, vl)
#define __riscv_vlseg7e8_v_i8m1x7(base, vl) __riscv_th_vlseg7e8_v_i8m1x7(base, vl)
#define __riscv_vlseg7e8_v_u8m1x7(base, vl) __riscv_th_vlseg7e8_v_u8m1x7(base, vl)
#define __riscv_vlseg7e8ff_v_i8m1x7(base, new_vl, vl) __riscv_th_vlseg7e8ff_v_i8m1x7(base, new_vl, vl)
#define __riscv_vlseg7e8ff_v_u8m1x7(base, new_vl, vl) __riscv_th_vlseg7e8ff_v_u8m1x7(base, new_vl, vl)
#define __riscv_vlseg8e16_v_f16m1x8(base, vl) __riscv_th_vlseg8e16_v_f16m1x8(base, vl)
#define __riscv_vlseg8e16_v_i16m1x8(base, vl) __riscv_th_vlseg8e16_v_i16m1x8(base, vl)
#define __riscv_vlseg8e16_v_u16m1x8(base, vl) __riscv_th_vlseg8e16_v_u16m1x8(base, vl)
#define __riscv_vlseg8e16ff_v_f16m1x8(base, new_vl, vl) __riscv_th_vlseg8e16ff_v_f16m1x8(base, new_vl, vl)
#define __riscv_vlseg8e16ff_v_i16m1x8(base, new_vl, vl) __riscv_th_vlseg8e16ff_v_i16m1x8(base, new_vl, vl)
#define __riscv_vlseg8e16ff_v_u16m1x8(base, new_vl, vl) __riscv_th_vlseg8e16ff_v_u16m1x8(base, new_vl, vl)
#define __riscv_vlseg8e32_v_f32m1x8(base, vl) __riscv_th_vlseg8e32_v_f32m1x8(base, vl)
#define __riscv_vlseg8e32_v_i32m1x8(base, vl) __riscv_th_vlseg8e32_v_i32m1x8(base, vl)
#define __riscv_vlseg8e32_v_u32m1x8(base, vl) __riscv_th_vlseg8e32_v_u32m1x8(base, vl)
#define __riscv_vlseg8e32ff_v_f32m1x8(base, new_vl, vl) __riscv_th_vlseg8e32ff_v_f32m1x8(base, new_vl, vl)
#define __riscv_vlseg8e32ff_v_i32m1x8(base, new_vl, vl) __riscv_th_vlseg8e32ff_v_i32m1x8(base, new_vl, vl)
#define __riscv_vlseg8e32ff_v_u32m1x8(base, new_vl, vl) __riscv_th_vlseg8e32ff_v_u32m1x8(base, new_vl, vl)
#define __riscv_vlseg8e64_v_f64m1x8(base, vl) __riscv_th_vlseg8e64_v_f64m1x8(base, vl)
#define __riscv_vlseg8e64_v_i64m1x8(base, vl) __riscv_th_vlseg8e64_v_i64m1x8(base, vl)
#define __riscv_vlseg8e64_v_u64m1x8(base, vl) __riscv_th_vlseg8e64_v_u64m1x8(base, vl)
#define __riscv_vlseg8e64ff_v_f64m1x8(base, new_vl, vl) __riscv_th_vlseg8e64ff_v_f64m1x8(base, new_vl, vl)
#define __riscv_vlseg8e64ff_v_i64m1x8(base, new_vl, vl) __riscv_th_vlseg8e64ff_v_i64m1x8(base, new_vl, vl)
#define __riscv_vlseg8e64ff_v_u64m1x8(base, new_vl, vl) __riscv_th_vlseg8e64ff_v_u64m1x8(base, new_vl, vl)
#define __riscv_vlseg8e8_v_i8m1x8(base, vl) __riscv_th_vlseg8e8_v_i8m1x8(base, vl)
#define __riscv_vlseg8e8_v_u8m1x8(base, vl) __riscv_th_vlseg8e8_v_u8m1x8(base, vl)
#define __riscv_vlseg8e8ff_v_i8m1x8(base, new_vl, vl) __riscv_th_vlseg8e8ff_v_i8m1x8(base, new_vl, vl)
#define __riscv_vlseg8e8ff_v_u8m1x8(base, new_vl, vl) __riscv_th_vlseg8e8ff_v_u8m1x8(base, new_vl, vl)
#define __riscv_vsseg2e16_v_f16m1x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_f16m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_f16m2x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_f16m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_f16m4x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_f16m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_i16m1x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_i16m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_i16m2x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_i16m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_i16m4x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_i16m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_u16m1x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_u16m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_u16m2x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_u16m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_u16m4x2(base, v_tuple, vl) __riscv_th_vsseg2e16_v_u16m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e16_v_f16m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_f16m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_f16m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_f16m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_f16m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_f16m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_i16m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_i16m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_i16m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_i16m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_i16m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_i16m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_u16m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_u16m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_u16m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_u16m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e16_v_u16m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e16_v_u16m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_f32m1x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_f32m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_f32m2x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_f32m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_f32m4x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_f32m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_i32m1x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_i32m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_i32m2x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_i32m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_i32m4x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_i32m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_u32m1x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_u32m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_u32m2x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_u32m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_u32m4x2(base, v_tuple, vl) __riscv_th_vsseg2e32_v_u32m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e32_v_f32m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_f32m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_f32m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_f32m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_f32m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_f32m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_i32m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_i32m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_i32m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_i32m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_i32m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_i32m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_u32m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_u32m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_u32m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_u32m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e32_v_u32m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e32_v_u32m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_f64m1x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_f64m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_f64m2x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_f64m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_f64m4x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_f64m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_i64m1x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_i64m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_i64m2x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_i64m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_i64m4x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_i64m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_u64m1x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_u64m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_u64m2x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_u64m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_u64m4x2(base, v_tuple, vl) __riscv_th_vsseg2e64_v_u64m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e64_v_f64m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_f64m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_f64m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_f64m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_f64m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_f64m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_i64m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_i64m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_i64m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_i64m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_i64m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_i64m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_u64m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_u64m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_u64m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_u64m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e64_v_u64m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e64_v_u64m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e8_v_i8m1x2(base, v_tuple, vl) __riscv_th_vsseg2e8_v_i8m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e8_v_i8m2x2(base, v_tuple, vl) __riscv_th_vsseg2e8_v_i8m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e8_v_i8m4x2(base, v_tuple, vl) __riscv_th_vsseg2e8_v_i8m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e8_v_u8m1x2(base, v_tuple, vl) __riscv_th_vsseg2e8_v_u8m1x2(base, v_tuple, vl)
#define __riscv_vsseg2e8_v_u8m2x2(base, v_tuple, vl) __riscv_th_vsseg2e8_v_u8m2x2(base, v_tuple, vl)
#define __riscv_vsseg2e8_v_u8m4x2(base, v_tuple, vl) __riscv_th_vsseg2e8_v_u8m4x2(base, v_tuple, vl)
#define __riscv_vsseg2e8_v_i8m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e8_v_i8m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e8_v_i8m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e8_v_i8m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e8_v_i8m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e8_v_i8m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e8_v_u8m1x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e8_v_u8m1x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e8_v_u8m2x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e8_v_u8m2x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg2e8_v_u8m4x2_m(mask, base, v_tuple, vl) __riscv_th_vsseg2e8_v_u8m4x2_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e16_v_f16m1x3(base, v_tuple, vl) __riscv_th_vsseg3e16_v_f16m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e16_v_f16m2x3(base, v_tuple, vl) __riscv_th_vsseg3e16_v_f16m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e16_v_i16m1x3(base, v_tuple, vl) __riscv_th_vsseg3e16_v_i16m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e16_v_i16m2x3(base, v_tuple, vl) __riscv_th_vsseg3e16_v_i16m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e16_v_u16m1x3(base, v_tuple, vl) __riscv_th_vsseg3e16_v_u16m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e16_v_u16m2x3(base, v_tuple, vl) __riscv_th_vsseg3e16_v_u16m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e16_v_f16m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e16_v_f16m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e16_v_f16m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e16_v_f16m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e16_v_i16m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e16_v_i16m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e16_v_i16m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e16_v_i16m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e16_v_u16m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e16_v_u16m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e16_v_u16m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e16_v_u16m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e32_v_f32m1x3(base, v_tuple, vl) __riscv_th_vsseg3e32_v_f32m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e32_v_f32m2x3(base, v_tuple, vl) __riscv_th_vsseg3e32_v_f32m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e32_v_i32m1x3(base, v_tuple, vl) __riscv_th_vsseg3e32_v_i32m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e32_v_i32m2x3(base, v_tuple, vl) __riscv_th_vsseg3e32_v_i32m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e32_v_u32m1x3(base, v_tuple, vl) __riscv_th_vsseg3e32_v_u32m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e32_v_u32m2x3(base, v_tuple, vl) __riscv_th_vsseg3e32_v_u32m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e32_v_f32m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e32_v_f32m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e32_v_f32m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e32_v_f32m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e32_v_i32m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e32_v_i32m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e32_v_i32m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e32_v_i32m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e32_v_u32m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e32_v_u32m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e32_v_u32m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e32_v_u32m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e64_v_f64m1x3(base, v_tuple, vl) __riscv_th_vsseg3e64_v_f64m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e64_v_f64m2x3(base, v_tuple, vl) __riscv_th_vsseg3e64_v_f64m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e64_v_i64m1x3(base, v_tuple, vl) __riscv_th_vsseg3e64_v_i64m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e64_v_i64m2x3(base, v_tuple, vl) __riscv_th_vsseg3e64_v_i64m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e64_v_u64m1x3(base, v_tuple, vl) __riscv_th_vsseg3e64_v_u64m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e64_v_u64m2x3(base, v_tuple, vl) __riscv_th_vsseg3e64_v_u64m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e64_v_f64m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e64_v_f64m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e64_v_f64m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e64_v_f64m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e64_v_i64m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e64_v_i64m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e64_v_i64m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e64_v_i64m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e64_v_u64m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e64_v_u64m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e64_v_u64m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e64_v_u64m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e8_v_i8m1x3(base, v_tuple, vl) __riscv_th_vsseg3e8_v_i8m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e8_v_i8m2x3(base, v_tuple, vl) __riscv_th_vsseg3e8_v_i8m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e8_v_u8m1x3(base, v_tuple, vl) __riscv_th_vsseg3e8_v_u8m1x3(base, v_tuple, vl)
#define __riscv_vsseg3e8_v_u8m2x3(base, v_tuple, vl) __riscv_th_vsseg3e8_v_u8m2x3(base, v_tuple, vl)
#define __riscv_vsseg3e8_v_i8m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e8_v_i8m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e8_v_i8m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e8_v_i8m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e8_v_u8m1x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e8_v_u8m1x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg3e8_v_u8m2x3_m(mask, base, v_tuple, vl) __riscv_th_vsseg3e8_v_u8m2x3_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e16_v_f16m1x4(base, v_tuple, vl) __riscv_th_vsseg4e16_v_f16m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e16_v_f16m2x4(base, v_tuple, vl) __riscv_th_vsseg4e16_v_f16m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e16_v_i16m1x4(base, v_tuple, vl) __riscv_th_vsseg4e16_v_i16m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e16_v_i16m2x4(base, v_tuple, vl) __riscv_th_vsseg4e16_v_i16m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e16_v_u16m1x4(base, v_tuple, vl) __riscv_th_vsseg4e16_v_u16m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e16_v_u16m2x4(base, v_tuple, vl) __riscv_th_vsseg4e16_v_u16m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e16_v_f16m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e16_v_f16m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e16_v_f16m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e16_v_f16m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e16_v_i16m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e16_v_i16m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e16_v_i16m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e16_v_i16m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e16_v_u16m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e16_v_u16m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e16_v_u16m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e16_v_u16m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e32_v_f32m1x4(base, v_tuple, vl) __riscv_th_vsseg4e32_v_f32m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e32_v_f32m2x4(base, v_tuple, vl) __riscv_th_vsseg4e32_v_f32m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e32_v_i32m1x4(base, v_tuple, vl) __riscv_th_vsseg4e32_v_i32m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e32_v_i32m2x4(base, v_tuple, vl) __riscv_th_vsseg4e32_v_i32m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e32_v_u32m1x4(base, v_tuple, vl) __riscv_th_vsseg4e32_v_u32m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e32_v_u32m2x4(base, v_tuple, vl) __riscv_th_vsseg4e32_v_u32m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e32_v_f32m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e32_v_f32m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e32_v_f32m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e32_v_f32m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e32_v_i32m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e32_v_i32m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e32_v_i32m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e32_v_i32m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e32_v_u32m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e32_v_u32m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e32_v_u32m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e32_v_u32m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e64_v_f64m1x4(base, v_tuple, vl) __riscv_th_vsseg4e64_v_f64m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e64_v_f64m2x4(base, v_tuple, vl) __riscv_th_vsseg4e64_v_f64m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e64_v_i64m1x4(base, v_tuple, vl) __riscv_th_vsseg4e64_v_i64m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e64_v_i64m2x4(base, v_tuple, vl) __riscv_th_vsseg4e64_v_i64m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e64_v_u64m1x4(base, v_tuple, vl) __riscv_th_vsseg4e64_v_u64m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e64_v_u64m2x4(base, v_tuple, vl) __riscv_th_vsseg4e64_v_u64m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e64_v_f64m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e64_v_f64m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e64_v_f64m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e64_v_f64m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e64_v_i64m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e64_v_i64m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e64_v_i64m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e64_v_i64m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e64_v_u64m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e64_v_u64m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e64_v_u64m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e64_v_u64m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e8_v_i8m1x4(base, v_tuple, vl) __riscv_th_vsseg4e8_v_i8m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e8_v_i8m2x4(base, v_tuple, vl) __riscv_th_vsseg4e8_v_i8m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e8_v_u8m1x4(base, v_tuple, vl) __riscv_th_vsseg4e8_v_u8m1x4(base, v_tuple, vl)
#define __riscv_vsseg4e8_v_u8m2x4(base, v_tuple, vl) __riscv_th_vsseg4e8_v_u8m2x4(base, v_tuple, vl)
#define __riscv_vsseg4e8_v_i8m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e8_v_i8m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e8_v_i8m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e8_v_i8m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e8_v_u8m1x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e8_v_u8m1x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg4e8_v_u8m2x4_m(mask, base, v_tuple, vl) __riscv_th_vsseg4e8_v_u8m2x4_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e16_v_f16m1x5(base, v_tuple, vl) __riscv_th_vsseg5e16_v_f16m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e16_v_i16m1x5(base, v_tuple, vl) __riscv_th_vsseg5e16_v_i16m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e16_v_u16m1x5(base, v_tuple, vl) __riscv_th_vsseg5e16_v_u16m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e16_v_f16m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e16_v_f16m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e16_v_i16m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e16_v_i16m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e16_v_u16m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e16_v_u16m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e32_v_f32m1x5(base, v_tuple, vl) __riscv_th_vsseg5e32_v_f32m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e32_v_i32m1x5(base, v_tuple, vl) __riscv_th_vsseg5e32_v_i32m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e32_v_u32m1x5(base, v_tuple, vl) __riscv_th_vsseg5e32_v_u32m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e32_v_f32m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e32_v_f32m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e32_v_i32m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e32_v_i32m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e32_v_u32m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e32_v_u32m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e64_v_f64m1x5(base, v_tuple, vl) __riscv_th_vsseg5e64_v_f64m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e64_v_i64m1x5(base, v_tuple, vl) __riscv_th_vsseg5e64_v_i64m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e64_v_u64m1x5(base, v_tuple, vl) __riscv_th_vsseg5e64_v_u64m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e64_v_f64m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e64_v_f64m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e64_v_i64m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e64_v_i64m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e64_v_u64m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e64_v_u64m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e8_v_i8m1x5(base, v_tuple, vl) __riscv_th_vsseg5e8_v_i8m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e8_v_u8m1x5(base, v_tuple, vl) __riscv_th_vsseg5e8_v_u8m1x5(base, v_tuple, vl)
#define __riscv_vsseg5e8_v_i8m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e8_v_i8m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg5e8_v_u8m1x5_m(mask, base, v_tuple, vl) __riscv_th_vsseg5e8_v_u8m1x5_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e16_v_f16m1x6(base, v_tuple, vl) __riscv_th_vsseg6e16_v_f16m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e16_v_i16m1x6(base, v_tuple, vl) __riscv_th_vsseg6e16_v_i16m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e16_v_u16m1x6(base, v_tuple, vl) __riscv_th_vsseg6e16_v_u16m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e16_v_f16m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e16_v_f16m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e16_v_i16m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e16_v_i16m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e16_v_u16m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e16_v_u16m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e32_v_f32m1x6(base, v_tuple, vl) __riscv_th_vsseg6e32_v_f32m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e32_v_i32m1x6(base, v_tuple, vl) __riscv_th_vsseg6e32_v_i32m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e32_v_u32m1x6(base, v_tuple, vl) __riscv_th_vsseg6e32_v_u32m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e32_v_f32m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e32_v_f32m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e32_v_i32m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e32_v_i32m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e32_v_u32m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e32_v_u32m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e64_v_f64m1x6(base, v_tuple, vl) __riscv_th_vsseg6e64_v_f64m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e64_v_i64m1x6(base, v_tuple, vl) __riscv_th_vsseg6e64_v_i64m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e64_v_u64m1x6(base, v_tuple, vl) __riscv_th_vsseg6e64_v_u64m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e64_v_f64m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e64_v_f64m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e64_v_i64m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e64_v_i64m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e64_v_u64m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e64_v_u64m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e8_v_i8m1x6(base, v_tuple, vl) __riscv_th_vsseg6e8_v_i8m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e8_v_u8m1x6(base, v_tuple, vl) __riscv_th_vsseg6e8_v_u8m1x6(base, v_tuple, vl)
#define __riscv_vsseg6e8_v_i8m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e8_v_i8m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg6e8_v_u8m1x6_m(mask, base, v_tuple, vl) __riscv_th_vsseg6e8_v_u8m1x6_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e16_v_f16m1x7(base, v_tuple, vl) __riscv_th_vsseg7e16_v_f16m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e16_v_i16m1x7(base, v_tuple, vl) __riscv_th_vsseg7e16_v_i16m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e16_v_u16m1x7(base, v_tuple, vl) __riscv_th_vsseg7e16_v_u16m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e16_v_f16m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e16_v_f16m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e16_v_i16m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e16_v_i16m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e16_v_u16m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e16_v_u16m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e32_v_f32m1x7(base, v_tuple, vl) __riscv_th_vsseg7e32_v_f32m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e32_v_i32m1x7(base, v_tuple, vl) __riscv_th_vsseg7e32_v_i32m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e32_v_u32m1x7(base, v_tuple, vl) __riscv_th_vsseg7e32_v_u32m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e32_v_f32m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e32_v_f32m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e32_v_i32m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e32_v_i32m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e32_v_u32m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e32_v_u32m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e64_v_f64m1x7(base, v_tuple, vl) __riscv_th_vsseg7e64_v_f64m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e64_v_i64m1x7(base, v_tuple, vl) __riscv_th_vsseg7e64_v_i64m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e64_v_u64m1x7(base, v_tuple, vl) __riscv_th_vsseg7e64_v_u64m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e64_v_f64m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e64_v_f64m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e64_v_i64m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e64_v_i64m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e64_v_u64m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e64_v_u64m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e8_v_i8m1x7(base, v_tuple, vl) __riscv_th_vsseg7e8_v_i8m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e8_v_u8m1x7(base, v_tuple, vl) __riscv_th_vsseg7e8_v_u8m1x7(base, v_tuple, vl)
#define __riscv_vsseg7e8_v_i8m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e8_v_i8m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg7e8_v_u8m1x7_m(mask, base, v_tuple, vl) __riscv_th_vsseg7e8_v_u8m1x7_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e16_v_f16m1x8(base, v_tuple, vl) __riscv_th_vsseg8e16_v_f16m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e16_v_i16m1x8(base, v_tuple, vl) __riscv_th_vsseg8e16_v_i16m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e16_v_u16m1x8(base, v_tuple, vl) __riscv_th_vsseg8e16_v_u16m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e16_v_f16m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e16_v_f16m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e16_v_i16m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e16_v_i16m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e16_v_u16m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e16_v_u16m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e32_v_f32m1x8(base, v_tuple, vl) __riscv_th_vsseg8e32_v_f32m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e32_v_i32m1x8(base, v_tuple, vl) __riscv_th_vsseg8e32_v_i32m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e32_v_u32m1x8(base, v_tuple, vl) __riscv_th_vsseg8e32_v_u32m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e32_v_f32m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e32_v_f32m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e32_v_i32m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e32_v_i32m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e32_v_u32m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e32_v_u32m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e64_v_f64m1x8(base, v_tuple, vl) __riscv_th_vsseg8e64_v_f64m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e64_v_i64m1x8(base, v_tuple, vl) __riscv_th_vsseg8e64_v_i64m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e64_v_u64m1x8(base, v_tuple, vl) __riscv_th_vsseg8e64_v_u64m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e64_v_f64m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e64_v_f64m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e64_v_i64m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e64_v_i64m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e64_v_u64m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e64_v_u64m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e8_v_i8m1x8(base, v_tuple, vl) __riscv_th_vsseg8e8_v_i8m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e8_v_u8m1x8(base, v_tuple, vl) __riscv_th_vsseg8e8_v_u8m1x8(base, v_tuple, vl)
#define __riscv_vsseg8e8_v_i8m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e8_v_i8m1x8_m(mask, base, v_tuple, vl)
#define __riscv_vsseg8e8_v_u8m1x8_m(mask, base, v_tuple, vl) __riscv_th_vsseg8e8_v_u8m1x8_m(mask, base, v_tuple, vl)

}] in
def th_zvlsseg_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Single-Width Integer Add and Subtract
#define __riscv_vadd_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vadd_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vsub_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vsub_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vrsub_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vneg_v_i8m1(op1_v, vl) __riscv_th_vneg_v_i8m1(op1_v, vl)
#define __riscv_vneg_v_i8m2(op1_v, vl) __riscv_th_vneg_v_i8m2(op1_v, vl)
#define __riscv_vneg_v_i8m4(op1_v, vl) __riscv_th_vneg_v_i8m4(op1_v, vl)
#define __riscv_vneg_v_i8m8(op1_v, vl) __riscv_th_vneg_v_i8m8(op1_v, vl)
#define __riscv_vneg_v_i16m1(op1_v, vl) __riscv_th_vneg_v_i16m1(op1_v, vl)
#define __riscv_vneg_v_i16m2(op1_v, vl) __riscv_th_vneg_v_i16m2(op1_v, vl)
#define __riscv_vneg_v_i16m4(op1_v, vl) __riscv_th_vneg_v_i16m4(op1_v, vl)
#define __riscv_vneg_v_i16m8(op1_v, vl) __riscv_th_vneg_v_i16m8(op1_v, vl)
#define __riscv_vneg_v_i32m1(op1_v, vl) __riscv_th_vneg_v_i32m1(op1_v, vl)
#define __riscv_vneg_v_i32m2(op1_v, vl) __riscv_th_vneg_v_i32m2(op1_v, vl)
#define __riscv_vneg_v_i32m4(op1_v, vl) __riscv_th_vneg_v_i32m4(op1_v, vl)
#define __riscv_vneg_v_i32m8(op1_v, vl) __riscv_th_vneg_v_i32m8(op1_v, vl)
#define __riscv_vneg_v_i64m1(op1_v, vl) __riscv_th_vneg_v_i64m1(op1_v, vl)
#define __riscv_vneg_v_i64m2(op1_v, vl) __riscv_th_vneg_v_i64m2(op1_v, vl)
#define __riscv_vneg_v_i64m4(op1_v, vl) __riscv_th_vneg_v_i64m4(op1_v, vl)
#define __riscv_vneg_v_i64m8(op1_v, vl) __riscv_th_vneg_v_i64m8(op1_v, vl)
#define __riscv_vneg_v_u8m1(op1_v, vl) __riscv_th_vneg_v_u8m1(op1_v, vl)
#define __riscv_vneg_v_u8m2(op1_v, vl) __riscv_th_vneg_v_u8m2(op1_v, vl)
#define __riscv_vneg_v_u8m4(op1_v, vl) __riscv_th_vneg_v_u8m4(op1_v, vl)
#define __riscv_vneg_v_u8m8(op1_v, vl) __riscv_th_vneg_v_u8m8(op1_v, vl)
#define __riscv_vneg_v_u16m1(op1_v, vl) __riscv_th_vneg_v_u16m1(op1_v, vl)
#define __riscv_vneg_v_u16m2(op1_v, vl) __riscv_th_vneg_v_u16m2(op1_v, vl)
#define __riscv_vneg_v_u16m4(op1_v, vl) __riscv_th_vneg_v_u16m4(op1_v, vl)
#define __riscv_vneg_v_u16m8(op1_v, vl) __riscv_th_vneg_v_u16m8(op1_v, vl)
#define __riscv_vneg_v_u32m1(op1_v, vl) __riscv_th_vneg_v_u32m1(op1_v, vl)
#define __riscv_vneg_v_u32m2(op1_v, vl) __riscv_th_vneg_v_u32m2(op1_v, vl)
#define __riscv_vneg_v_u32m4(op1_v, vl) __riscv_th_vneg_v_u32m4(op1_v, vl)
#define __riscv_vneg_v_u32m8(op1_v, vl) __riscv_th_vneg_v_u32m8(op1_v, vl)
#define __riscv_vneg_v_u64m1(op1_v, vl) __riscv_th_vneg_v_u64m1(op1_v, vl)
#define __riscv_vneg_v_u64m2(op1_v, vl) __riscv_th_vneg_v_u64m2(op1_v, vl)
#define __riscv_vneg_v_u64m4(op1_v, vl) __riscv_th_vneg_v_u64m4(op1_v, vl)
#define __riscv_vneg_v_u64m8(op1_v, vl) __riscv_th_vneg_v_u64m8(op1_v, vl)

}] in
def th_single_width_integer_add_wrapper_macros: RVVHeader;


let HeaderCode =
[{
// Vector Widening Integer Add and Subtract
#define __riscv_vwadd_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vwaddu_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vwadd_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vwaddu_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vwadd_wv_i16m2(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i16m2(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i16m4(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i16m4(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i16m8(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i16m8(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m1(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m1(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m2(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m2(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m4(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m4(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m8(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m8(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m1(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m1(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m2(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m2(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m4(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m4(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m8(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m8(op1_wv, op2_v, vl)

#define __riscv_vwaddu_wv_u16m2(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u16m2(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u16m4(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u16m4(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u16m8(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u16m8(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m1(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m1(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m2(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m2(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m4(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m4(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m8(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m8(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m1(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m1(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m2(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m2(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m4(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m4(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m8(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m8(op1_wv, op2_v, vl)

#define __riscv_vwadd_wx_i16m1(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m1(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i16m2(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m2(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i16m4(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m4(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i16m8(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m8(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m1(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m1(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m2(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m2(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m4(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m4(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m8(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m8(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m1(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m1(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m2(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m2(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m4(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m4(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m8(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m8(op1_wx, op2_x, vl)

#define __riscv_vwaddu_wx_u16m1(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m1(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u16m2(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m2(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u16m4(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m4(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u16m8(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m8(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m1(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m1(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m2(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m2(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m4(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m4(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m8(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m8(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m1(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m1(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m2(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m2(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m4(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m4(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m8(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m8(op1_wx, op2_x, vl)

#define __riscv_vwsub_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vwsubu_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vwsub_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vwsubu_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vwsub_wv_i16m2(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i16m2(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i16m4(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i16m4(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i16m8(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i16m8(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m1(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m1(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m2(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m2(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m4(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m4(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m8(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m8(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m1(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m1(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m2(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m2(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m4(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m4(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m8(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m8(op1_wv, op2_v, vl)

#define __riscv_vwsubu_wv_u16m2(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u16m2(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u16m4(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u16m4(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u16m8(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u16m8(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m1(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m1(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m2(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m2(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m4(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m4(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m8(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m8(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m1(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m1(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m2(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m2(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m4(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m4(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m8(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m8(op1_wv, op2_v, vl)

#define __riscv_vwsub_wx_i16m1(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m1(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i16m2(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m2(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i16m4(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m4(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i16m8(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m8(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m1(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m1(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m2(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m2(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m4(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m4(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m8(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m8(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m1(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m1(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m2(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m2(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m4(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m4(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m8(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m8(op1_wx, op2_x, vl)

#define __riscv_vwsubu_wx_u16m1(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m1(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u16m2(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m2(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u16m4(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m4(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u16m8(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m8(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m1(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m1(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m2(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m2(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m4(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m4(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m8(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m8(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m1(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m1(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m2(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m2(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m4(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m4(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m8(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m8(op1_wx, op2_x, vl)

}] in
def th_widening_integer_add_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Bitwise Logical Operations
#define __riscv_vand_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vand_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vand_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vand_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vor_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vor_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vor_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vor_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vxor_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vxor_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vxor_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vxor_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vnot_v_i8m1(op1_v, vl) __riscv_th_vnot_v_i8m1(op1_v, vl)
#define __riscv_vnot_v_i8m2(op1_v, vl) __riscv_th_vnot_v_i8m2(op1_v, vl)
#define __riscv_vnot_v_i8m4(op1_v, vl) __riscv_th_vnot_v_i8m4(op1_v, vl)
#define __riscv_vnot_v_i8m8(op1_v, vl) __riscv_th_vnot_v_i8m8(op1_v, vl)
#define __riscv_vnot_v_i16m1(op1_v, vl) __riscv_th_vnot_v_i16m1(op1_v, vl)
#define __riscv_vnot_v_i16m2(op1_v, vl) __riscv_th_vnot_v_i16m2(op1_v, vl)
#define __riscv_vnot_v_i16m4(op1_v, vl) __riscv_th_vnot_v_i16m4(op1_v, vl)
#define __riscv_vnot_v_i16m8(op1_v, vl) __riscv_th_vnot_v_i16m8(op1_v, vl)
#define __riscv_vnot_v_i32m1(op1_v, vl) __riscv_th_vnot_v_i32m1(op1_v, vl)
#define __riscv_vnot_v_i32m2(op1_v, vl) __riscv_th_vnot_v_i32m2(op1_v, vl)
#define __riscv_vnot_v_i32m4(op1_v, vl) __riscv_th_vnot_v_i32m4(op1_v, vl)
#define __riscv_vnot_v_i32m8(op1_v, vl) __riscv_th_vnot_v_i32m8(op1_v, vl)
#define __riscv_vnot_v_i64m1(op1_v, vl) __riscv_th_vnot_v_i64m1(op1_v, vl)
#define __riscv_vnot_v_i64m2(op1_v, vl) __riscv_th_vnot_v_i64m2(op1_v, vl)
#define __riscv_vnot_v_i64m4(op1_v, vl) __riscv_th_vnot_v_i64m4(op1_v, vl)
#define __riscv_vnot_v_i64m8(op1_v, vl) __riscv_th_vnot_v_i64m8(op1_v, vl)

#define __riscv_vnot_v_u8m1(op1_v, vl) __riscv_th_vnot_v_u8m1(op1_v, vl)
#define __riscv_vnot_v_u8m2(op1_v, vl) __riscv_th_vnot_v_u8m2(op1_v, vl)
#define __riscv_vnot_v_u8m4(op1_v, vl) __riscv_th_vnot_v_u8m4(op1_v, vl)
#define __riscv_vnot_v_u8m8(op1_v, vl) __riscv_th_vnot_v_u8m8(op1_v, vl)
#define __riscv_vnot_v_u16m1(op1_v, vl) __riscv_th_vnot_v_u16m1(op1_v, vl)
#define __riscv_vnot_v_u16m2(op1_v, vl) __riscv_th_vnot_v_u16m2(op1_v, vl)
#define __riscv_vnot_v_u16m4(op1_v, vl) __riscv_th_vnot_v_u16m4(op1_v, vl)
#define __riscv_vnot_v_u16m8(op1_v, vl) __riscv_th_vnot_v_u16m8(op1_v, vl)
#define __riscv_vnot_v_u32m1(op1_v, vl) __riscv_th_vnot_v_u32m1(op1_v, vl)
#define __riscv_vnot_v_u32m2(op1_v, vl) __riscv_th_vnot_v_u32m2(op1_v, vl)
#define __riscv_vnot_v_u32m4(op1_v, vl) __riscv_th_vnot_v_u32m4(op1_v, vl)
#define __riscv_vnot_v_u32m8(op1_v, vl) __riscv_th_vnot_v_u32m8(op1_v, vl)
#define __riscv_vnot_v_u64m1(op1_v, vl) __riscv_th_vnot_v_u64m1(op1_v, vl)
#define __riscv_vnot_v_u64m2(op1_v, vl) __riscv_th_vnot_v_u64m2(op1_v, vl)
#define __riscv_vnot_v_u64m4(op1_v, vl) __riscv_th_vnot_v_u64m4(op1_v, vl)
#define __riscv_vnot_v_u64m8(op1_v, vl) __riscv_th_vnot_v_u64m8(op1_v, vl)

}] in
def th_bitwise_logical_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Single Width Integer Bit Shift Operations
#define __riscv_vsll_vv_i8m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i8m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i8m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i8m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m8(op1_v, shift_v, vl)

#define __riscv_vsll_vv_u8m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u8m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u8m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u8m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m8(op1_v, shift_v, vl)

#define __riscv_vsll_vx_i8m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i8m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i8m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i8m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m8(op1_v, shift_x, vl)

#define __riscv_vsll_vx_u8m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u8m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u8m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u8m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m8(op1_v, shift_x, vl)

#define __riscv_vsrl_vv_u8m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u8m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u8m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u8m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m8(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m8(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m8(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m8(op1_v, shift_v, vl)

#define __riscv_vsrl_vx_u8m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u8m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u8m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u8m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m8(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m8(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m8(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m8(op1_v, shift_x, vl)

#define __riscv_vsra_vv_i8m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i8m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i8m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i8m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m8(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m8(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m8(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m8(op1_v, shift_v, vl)

#define __riscv_vsra_vx_i8m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i8m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i8m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i8m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m8(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m8(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m8(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m8(op1_v, shift_x, vl)

}] in
def th_single_width_integer_bit_shift_wrapper_macros: RVVHeader;

// 12.6. Vector Narrowing Integer Right Shift Operations

let HeaderCode =
[{
// Vector Narrowing Integer Right Shift Operations
#define __riscv_vnsrl_wv_u8m1(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u8m1(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u8m2(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u8m2(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u8m4(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u8m4(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u16m1(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u16m1(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u16m2(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u16m2(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u16m4(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u16m4(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u32m1(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u32m1(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u32m2(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u32m2(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u32m4(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u32m4(op1_v, shift_v, vl)

#define __riscv_vnsrl_wx_u8m1(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u8m1(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u8m2(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u8m2(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u8m4(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u8m4(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u16m1(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u16m1(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u16m2(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u16m2(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u16m4(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u16m4(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u32m1(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u32m1(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u32m2(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u32m2(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u32m4(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u32m4(op1_v, shift_x, vl)

#define __riscv_vnsra_wv_i8m1(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i8m1(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i8m2(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i8m2(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i8m4(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i8m4(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i16m1(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i16m1(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i16m2(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i16m2(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i16m4(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i16m4(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i32m1(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i32m1(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i32m2(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i32m2(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i32m4(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i32m4(op1_v, shift_v, vl)

#define __riscv_vnsra_wx_i8m1(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i8m1(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i8m2(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i8m2(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i8m4(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i8m4(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i16m1(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i16m1(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i16m2(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i16m2(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i16m4(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i16m4(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i32m1(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i32m1(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i32m2(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i32m2(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i32m4(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i32m4(op1_v, shift_x, vl)

}] in
def th_narrowing_integer_right_shift_wrapper_macros: RVVHeader;

// 12.7. Vector Integer Comparison Operations
let HeaderCode =
[{
// Vector Integer Comparison Operations
#define __riscv_vmseq_vv_i8m1_b8(op1, op2, vl) __riscv_th_vmseq_vv_i8m1_b8(op1, op2, vl)
#define __riscv_vmseq_vv_i8m2_b4(op1, op2, vl) __riscv_th_vmseq_vv_i8m2_b4(op1, op2, vl)
#define __riscv_vmseq_vv_i8m4_b2(op1, op2, vl) __riscv_th_vmseq_vv_i8m4_b2(op1, op2, vl)
#define __riscv_vmseq_vv_i8m8_b1(op1, op2, vl) __riscv_th_vmseq_vv_i8m8_b1(op1, op2, vl)
#define __riscv_vmseq_vv_i16m1_b16(op1, op2, vl) __riscv_th_vmseq_vv_i16m1_b16(op1, op2, vl)
#define __riscv_vmseq_vv_i16m2_b8(op1, op2, vl) __riscv_th_vmseq_vv_i16m2_b8(op1, op2, vl)
#define __riscv_vmseq_vv_i16m4_b4(op1, op2, vl) __riscv_th_vmseq_vv_i16m4_b4(op1, op2, vl)
#define __riscv_vmseq_vv_i16m8_b2(op1, op2, vl) __riscv_th_vmseq_vv_i16m8_b2(op1, op2, vl)
#define __riscv_vmseq_vv_i32m1_b32(op1, op2, vl) __riscv_th_vmseq_vv_i32m1_b32(op1, op2, vl)
#define __riscv_vmseq_vv_i32m2_b16(op1, op2, vl) __riscv_th_vmseq_vv_i32m2_b16(op1, op2, vl)
#define __riscv_vmseq_vv_i32m4_b8(op1, op2, vl) __riscv_th_vmseq_vv_i32m4_b8(op1, op2, vl)
#define __riscv_vmseq_vv_i32m8_b4(op1, op2, vl) __riscv_th_vmseq_vv_i32m8_b4(op1, op2, vl)
#define __riscv_vmseq_vv_i64m1_b64(op1, op2, vl) __riscv_th_vmseq_vv_i64m1_b64(op1, op2, vl)
#define __riscv_vmseq_vv_i64m2_b32(op1, op2, vl) __riscv_th_vmseq_vv_i64m2_b32(op1, op2, vl)
#define __riscv_vmseq_vv_i64m4_b16(op1, op2, vl) __riscv_th_vmseq_vv_i64m4_b16(op1, op2, vl)
#define __riscv_vmseq_vv_i64m8_b8(op1, op2, vl) __riscv_th_vmseq_vv_i64m8_b8(op1, op2, vl)

#define __riscv_vmseq_vv_u8m1_b8(op1, op2, vl) __riscv_th_vmseq_vv_u8m1_b8(op1, op2, vl)
#define __riscv_vmseq_vv_u8m2_b4(op1, op2, vl) __riscv_th_vmseq_vv_u8m2_b4(op1, op2, vl)
#define __riscv_vmseq_vv_u8m4_b2(op1, op2, vl) __riscv_th_vmseq_vv_u8m4_b2(op1, op2, vl)
#define __riscv_vmseq_vv_u8m8_b1(op1, op2, vl) __riscv_th_vmseq_vv_u8m8_b1(op1, op2, vl)
#define __riscv_vmseq_vv_u16m1_b16(op1, op2, vl) __riscv_th_vmseq_vv_u16m1_b16(op1, op2, vl)
#define __riscv_vmseq_vv_u16m2_b8(op1, op2, vl) __riscv_th_vmseq_vv_u16m2_b8(op1, op2, vl)
#define __riscv_vmseq_vv_u16m4_b4(op1, op2, vl) __riscv_th_vmseq_vv_u16m4_b4(op1, op2, vl)
#define __riscv_vmseq_vv_u16m8_b2(op1, op2, vl) __riscv_th_vmseq_vv_u16m8_b2(op1, op2, vl)
#define __riscv_vmseq_vv_u32m1_b32(op1, op2, vl) __riscv_th_vmseq_vv_u32m1_b32(op1, op2, vl)
#define __riscv_vmseq_vv_u32m2_b16(op1, op2, vl) __riscv_th_vmseq_vv_u32m2_b16(op1, op2, vl)
#define __riscv_vmseq_vv_u32m4_b8(op1, op2, vl) __riscv_th_vmseq_vv_u32m4_b8(op1, op2, vl)
#define __riscv_vmseq_vv_u32m8_b4(op1, op2, vl) __riscv_th_vmseq_vv_u32m8_b4(op1, op2, vl)
#define __riscv_vmseq_vv_u64m1_b64(op1, op2, vl) __riscv_th_vmseq_vv_u64m1_b64(op1, op2, vl)
#define __riscv_vmseq_vv_u64m2_b32(op1, op2, vl) __riscv_th_vmseq_vv_u64m2_b32(op1, op2, vl)
#define __riscv_vmseq_vv_u64m4_b16(op1, op2, vl) __riscv_th_vmseq_vv_u64m4_b16(op1, op2, vl)
#define __riscv_vmseq_vv_u64m8_b8(op1, op2, vl) __riscv_th_vmseq_vv_u64m8_b8(op1, op2, vl)

#define __riscv_vmseq_vx_i8m1_b8(op1, op2, vl) __riscv_th_vmseq_vx_i8m1_b8(op1, op2, vl)
#define __riscv_vmseq_vx_i8m2_b4(op1, op2, vl) __riscv_th_vmseq_vx_i8m2_b4(op1, op2, vl)
#define __riscv_vmseq_vx_i8m4_b2(op1, op2, vl) __riscv_th_vmseq_vx_i8m4_b2(op1, op2, vl)
#define __riscv_vmseq_vx_i8m8_b1(op1, op2, vl) __riscv_th_vmseq_vx_i8m8_b1(op1, op2, vl)
#define __riscv_vmseq_vx_i16m1_b16(op1, op2, vl) __riscv_th_vmseq_vx_i16m1_b16(op1, op2, vl)
#define __riscv_vmseq_vx_i16m2_b8(op1, op2, vl) __riscv_th_vmseq_vx_i16m2_b8(op1, op2, vl)
#define __riscv_vmseq_vx_i16m4_b4(op1, op2, vl) __riscv_th_vmseq_vx_i16m4_b4(op1, op2, vl)
#define __riscv_vmseq_vx_i16m8_b2(op1, op2, vl) __riscv_th_vmseq_vx_i16m8_b2(op1, op2, vl)
#define __riscv_vmseq_vx_i32m1_b32(op1, op2, vl) __riscv_th_vmseq_vx_i32m1_b32(op1, op2, vl)
#define __riscv_vmseq_vx_i32m2_b16(op1, op2, vl) __riscv_th_vmseq_vx_i32m2_b16(op1, op2, vl)
#define __riscv_vmseq_vx_i32m4_b8(op1, op2, vl) __riscv_th_vmseq_vx_i32m4_b8(op1, op2, vl)
#define __riscv_vmseq_vx_i32m8_b4(op1, op2, vl) __riscv_th_vmseq_vx_i32m8_b4(op1, op2, vl)
#define __riscv_vmseq_vx_i64m1_b64(op1, op2, vl) __riscv_th_vmseq_vx_i64m1_b64(op1, op2, vl)
#define __riscv_vmseq_vx_i64m2_b32(op1, op2, vl) __riscv_th_vmseq_vx_i64m2_b32(op1, op2, vl)
#define __riscv_vmseq_vx_i64m4_b16(op1, op2, vl) __riscv_th_vmseq_vx_i64m4_b16(op1, op2, vl)
#define __riscv_vmseq_vx_i64m8_b8(op1, op2, vl) __riscv_th_vmseq_vx_i64m8_b8(op1, op2, vl)

#define __riscv_vmseq_vx_u8m1_b8(op1, op2, vl) __riscv_th_vmseq_vx_u8m1_b8(op1, op2, vl)
#define __riscv_vmseq_vx_u8m2_b4(op1, op2, vl) __riscv_th_vmseq_vx_u8m2_b4(op1, op2, vl)
#define __riscv_vmseq_vx_u8m4_b2(op1, op2, vl) __riscv_th_vmseq_vx_u8m4_b2(op1, op2, vl)
#define __riscv_vmseq_vx_u8m8_b1(op1, op2, vl) __riscv_th_vmseq_vx_u8m8_b1(op1, op2, vl)
#define __riscv_vmseq_vx_u16m1_b16(op1, op2, vl) __riscv_th_vmseq_vx_u16m1_b16(op1, op2, vl)
#define __riscv_vmseq_vx_u16m2_b8(op1, op2, vl) __riscv_th_vmseq_vx_u16m2_b8(op1, op2, vl)
#define __riscv_vmseq_vx_u16m4_b4(op1, op2, vl) __riscv_th_vmseq_vx_u16m4_b4(op1, op2, vl)
#define __riscv_vmseq_vx_u16m8_b2(op1, op2, vl) __riscv_th_vmseq_vx_u16m8_b2(op1, op2, vl)
#define __riscv_vmseq_vx_u32m1_b32(op1, op2, vl) __riscv_th_vmseq_vx_u32m1_b32(op1, op2, vl)
#define __riscv_vmseq_vx_u32m2_b16(op1, op2, vl) __riscv_th_vmseq_vx_u32m2_b16(op1, op2, vl)
#define __riscv_vmseq_vx_u32m4_b8(op1, op2, vl) __riscv_th_vmseq_vx_u32m4_b8(op1, op2, vl)
#define __riscv_vmseq_vx_u32m8_b4(op1, op2, vl) __riscv_th_vmseq_vx_u32m8_b4(op1, op2, vl)
#define __riscv_vmseq_vx_u64m1_b64(op1, op2, vl) __riscv_th_vmseq_vx_u64m1_b64(op1, op2, vl)
#define __riscv_vmseq_vx_u64m2_b32(op1, op2, vl) __riscv_th_vmseq_vx_u64m2_b32(op1, op2, vl)
#define __riscv_vmseq_vx_u64m4_b16(op1, op2, vl) __riscv_th_vmseq_vx_u64m4_b16(op1, op2, vl)
#define __riscv_vmseq_vx_u64m8_b8(op1, op2, vl) __riscv_th_vmseq_vx_u64m8_b8(op1, op2, vl)

#define __riscv_vmseq_vv_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmseq_vv_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vv_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vv_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmseq_vx_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmseq_vx_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmseq_vx_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmseq_vx_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsne_vv_i8m1_b8(op1, op2, vl) __riscv_th_vmsne_vv_i8m1_b8(op1, op2, vl)
#define __riscv_vmsne_vv_i8m2_b4(op1, op2, vl) __riscv_th_vmsne_vv_i8m2_b4(op1, op2, vl)
#define __riscv_vmsne_vv_i8m4_b2(op1, op2, vl) __riscv_th_vmsne_vv_i8m4_b2(op1, op2, vl)
#define __riscv_vmsne_vv_i8m8_b1(op1, op2, vl) __riscv_th_vmsne_vv_i8m8_b1(op1, op2, vl)
#define __riscv_vmsne_vv_i16m1_b16(op1, op2, vl) __riscv_th_vmsne_vv_i16m1_b16(op1, op2, vl)
#define __riscv_vmsne_vv_i16m2_b8(op1, op2, vl) __riscv_th_vmsne_vv_i16m2_b8(op1, op2, vl)
#define __riscv_vmsne_vv_i16m4_b4(op1, op2, vl) __riscv_th_vmsne_vv_i16m4_b4(op1, op2, vl)
#define __riscv_vmsne_vv_i16m8_b2(op1, op2, vl) __riscv_th_vmsne_vv_i16m8_b2(op1, op2, vl)
#define __riscv_vmsne_vv_i32m1_b32(op1, op2, vl) __riscv_th_vmsne_vv_i32m1_b32(op1, op2, vl)
#define __riscv_vmsne_vv_i32m2_b16(op1, op2, vl) __riscv_th_vmsne_vv_i32m2_b16(op1, op2, vl)
#define __riscv_vmsne_vv_i32m4_b8(op1, op2, vl) __riscv_th_vmsne_vv_i32m4_b8(op1, op2, vl)
#define __riscv_vmsne_vv_i32m8_b4(op1, op2, vl) __riscv_th_vmsne_vv_i32m8_b4(op1, op2, vl)
#define __riscv_vmsne_vv_i64m1_b64(op1, op2, vl) __riscv_th_vmsne_vv_i64m1_b64(op1, op2, vl)
#define __riscv_vmsne_vv_i64m2_b32(op1, op2, vl) __riscv_th_vmsne_vv_i64m2_b32(op1, op2, vl)
#define __riscv_vmsne_vv_i64m4_b16(op1, op2, vl) __riscv_th_vmsne_vv_i64m4_b16(op1, op2, vl)
#define __riscv_vmsne_vv_i64m8_b8(op1, op2, vl) __riscv_th_vmsne_vv_i64m8_b8(op1, op2, vl)

#define __riscv_vmsne_vv_u8m1_b8(op1, op2, vl) __riscv_th_vmsne_vv_u8m1_b8(op1, op2, vl)
#define __riscv_vmsne_vv_u8m2_b4(op1, op2, vl) __riscv_th_vmsne_vv_u8m2_b4(op1, op2, vl)
#define __riscv_vmsne_vv_u8m4_b2(op1, op2, vl) __riscv_th_vmsne_vv_u8m4_b2(op1, op2, vl)
#define __riscv_vmsne_vv_u8m8_b1(op1, op2, vl) __riscv_th_vmsne_vv_u8m8_b1(op1, op2, vl)
#define __riscv_vmsne_vv_u16m1_b16(op1, op2, vl) __riscv_th_vmsne_vv_u16m1_b16(op1, op2, vl)
#define __riscv_vmsne_vv_u16m2_b8(op1, op2, vl) __riscv_th_vmsne_vv_u16m2_b8(op1, op2, vl)
#define __riscv_vmsne_vv_u16m4_b4(op1, op2, vl) __riscv_th_vmsne_vv_u16m4_b4(op1, op2, vl)
#define __riscv_vmsne_vv_u16m8_b2(op1, op2, vl) __riscv_th_vmsne_vv_u16m8_b2(op1, op2, vl)
#define __riscv_vmsne_vv_u32m1_b32(op1, op2, vl) __riscv_th_vmsne_vv_u32m1_b32(op1, op2, vl)
#define __riscv_vmsne_vv_u32m2_b16(op1, op2, vl) __riscv_th_vmsne_vv_u32m2_b16(op1, op2, vl)
#define __riscv_vmsne_vv_u32m4_b8(op1, op2, vl) __riscv_th_vmsne_vv_u32m4_b8(op1, op2, vl)
#define __riscv_vmsne_vv_u32m8_b4(op1, op2, vl) __riscv_th_vmsne_vv_u32m8_b4(op1, op2, vl)
#define __riscv_vmsne_vv_u64m1_b64(op1, op2, vl) __riscv_th_vmsne_vv_u64m1_b64(op1, op2, vl)
#define __riscv_vmsne_vv_u64m2_b32(op1, op2, vl) __riscv_th_vmsne_vv_u64m2_b32(op1, op2, vl)
#define __riscv_vmsne_vv_u64m4_b16(op1, op2, vl) __riscv_th_vmsne_vv_u64m4_b16(op1, op2, vl)
#define __riscv_vmsne_vv_u64m8_b8(op1, op2, vl) __riscv_th_vmsne_vv_u64m8_b8(op1, op2, vl)

#define __riscv_vmsne_vx_i8m1_b8(op1, op2, vl) __riscv_th_vmsne_vx_i8m1_b8(op1, op2, vl)
#define __riscv_vmsne_vx_i8m2_b4(op1, op2, vl) __riscv_th_vmsne_vx_i8m2_b4(op1, op2, vl)
#define __riscv_vmsne_vx_i8m4_b2(op1, op2, vl) __riscv_th_vmsne_vx_i8m4_b2(op1, op2, vl)
#define __riscv_vmsne_vx_i8m8_b1(op1, op2, vl) __riscv_th_vmsne_vx_i8m8_b1(op1, op2, vl)
#define __riscv_vmsne_vx_i16m1_b16(op1, op2, vl) __riscv_th_vmsne_vx_i16m1_b16(op1, op2, vl)
#define __riscv_vmsne_vx_i16m2_b8(op1, op2, vl) __riscv_th_vmsne_vx_i16m2_b8(op1, op2, vl)
#define __riscv_vmsne_vx_i16m4_b4(op1, op2, vl) __riscv_th_vmsne_vx_i16m4_b4(op1, op2, vl)
#define __riscv_vmsne_vx_i16m8_b2(op1, op2, vl) __riscv_th_vmsne_vx_i16m8_b2(op1, op2, vl)
#define __riscv_vmsne_vx_i32m1_b32(op1, op2, vl) __riscv_th_vmsne_vx_i32m1_b32(op1, op2, vl)
#define __riscv_vmsne_vx_i32m2_b16(op1, op2, vl) __riscv_th_vmsne_vx_i32m2_b16(op1, op2, vl)
#define __riscv_vmsne_vx_i32m4_b8(op1, op2, vl) __riscv_th_vmsne_vx_i32m4_b8(op1, op2, vl)
#define __riscv_vmsne_vx_i32m8_b4(op1, op2, vl) __riscv_th_vmsne_vx_i32m8_b4(op1, op2, vl)
#define __riscv_vmsne_vx_i64m1_b64(op1, op2, vl) __riscv_th_vmsne_vx_i64m1_b64(op1, op2, vl)
#define __riscv_vmsne_vx_i64m2_b32(op1, op2, vl) __riscv_th_vmsne_vx_i64m2_b32(op1, op2, vl)
#define __riscv_vmsne_vx_i64m4_b16(op1, op2, vl) __riscv_th_vmsne_vx_i64m4_b16(op1, op2, vl)
#define __riscv_vmsne_vx_i64m8_b8(op1, op2, vl) __riscv_th_vmsne_vx_i64m8_b8(op1, op2, vl)

#define __riscv_vmsne_vx_u8m1_b8(op1, op2, vl) __riscv_th_vmsne_vx_u8m1_b8(op1, op2, vl)
#define __riscv_vmsne_vx_u8m2_b4(op1, op2, vl) __riscv_th_vmsne_vx_u8m2_b4(op1, op2, vl)
#define __riscv_vmsne_vx_u8m4_b2(op1, op2, vl) __riscv_th_vmsne_vx_u8m4_b2(op1, op2, vl)
#define __riscv_vmsne_vx_u8m8_b1(op1, op2, vl) __riscv_th_vmsne_vx_u8m8_b1(op1, op2, vl)
#define __riscv_vmsne_vx_u16m1_b16(op1, op2, vl) __riscv_th_vmsne_vx_u16m1_b16(op1, op2, vl)
#define __riscv_vmsne_vx_u16m2_b8(op1, op2, vl) __riscv_th_vmsne_vx_u16m2_b8(op1, op2, vl)
#define __riscv_vmsne_vx_u16m4_b4(op1, op2, vl) __riscv_th_vmsne_vx_u16m4_b4(op1, op2, vl)
#define __riscv_vmsne_vx_u16m8_b2(op1, op2, vl) __riscv_th_vmsne_vx_u16m8_b2(op1, op2, vl)
#define __riscv_vmsne_vx_u32m1_b32(op1, op2, vl) __riscv_th_vmsne_vx_u32m1_b32(op1, op2, vl)
#define __riscv_vmsne_vx_u32m2_b16(op1, op2, vl) __riscv_th_vmsne_vx_u32m2_b16(op1, op2, vl)
#define __riscv_vmsne_vx_u32m4_b8(op1, op2, vl) __riscv_th_vmsne_vx_u32m4_b8(op1, op2, vl)
#define __riscv_vmsne_vx_u32m8_b4(op1, op2, vl) __riscv_th_vmsne_vx_u32m8_b4(op1, op2, vl)
#define __riscv_vmsne_vx_u64m1_b64(op1, op2, vl) __riscv_th_vmsne_vx_u64m1_b64(op1, op2, vl)
#define __riscv_vmsne_vx_u64m2_b32(op1, op2, vl) __riscv_th_vmsne_vx_u64m2_b32(op1, op2, vl)
#define __riscv_vmsne_vx_u64m4_b16(op1, op2, vl) __riscv_th_vmsne_vx_u64m4_b16(op1, op2, vl)
#define __riscv_vmsne_vx_u64m8_b8(op1, op2, vl) __riscv_th_vmsne_vx_u64m8_b8(op1, op2, vl)

#define __riscv_vmsne_vv_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsne_vv_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vv_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vv_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsne_vx_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsne_vx_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsne_vx_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsne_vx_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsge_vv_i8m1_b8(op1, op2, vl) __riscv_th_vmsge_vv_i8m1_b8(op1, op2, vl)
#define __riscv_vmsge_vv_i8m2_b4(op1, op2, vl) __riscv_th_vmsge_vv_i8m2_b4(op1, op2, vl)
#define __riscv_vmsge_vv_i8m4_b2(op1, op2, vl) __riscv_th_vmsge_vv_i8m4_b2(op1, op2, vl)
#define __riscv_vmsge_vv_i8m8_b1(op1, op2, vl) __riscv_th_vmsge_vv_i8m8_b1(op1, op2, vl)
#define __riscv_vmsge_vv_i16m1_b16(op1, op2, vl) __riscv_th_vmsge_vv_i16m1_b16(op1, op2, vl)
#define __riscv_vmsge_vv_i16m2_b8(op1, op2, vl) __riscv_th_vmsge_vv_i16m2_b8(op1, op2, vl)
#define __riscv_vmsge_vv_i16m4_b4(op1, op2, vl) __riscv_th_vmsge_vv_i16m4_b4(op1, op2, vl)
#define __riscv_vmsge_vv_i16m8_b2(op1, op2, vl) __riscv_th_vmsge_vv_i16m8_b2(op1, op2, vl)
#define __riscv_vmsge_vv_i32m1_b32(op1, op2, vl) __riscv_th_vmsge_vv_i32m1_b32(op1, op2, vl)
#define __riscv_vmsge_vv_i32m2_b16(op1, op2, vl) __riscv_th_vmsge_vv_i32m2_b16(op1, op2, vl)
#define __riscv_vmsge_vv_i32m4_b8(op1, op2, vl) __riscv_th_vmsge_vv_i32m4_b8(op1, op2, vl)
#define __riscv_vmsge_vv_i32m8_b4(op1, op2, vl) __riscv_th_vmsge_vv_i32m8_b4(op1, op2, vl)
#define __riscv_vmsge_vv_i64m1_b64(op1, op2, vl) __riscv_th_vmsge_vv_i64m1_b64(op1, op2, vl)
#define __riscv_vmsge_vv_i64m2_b32(op1, op2, vl) __riscv_th_vmsge_vv_i64m2_b32(op1, op2, vl)
#define __riscv_vmsge_vv_i64m4_b16(op1, op2, vl) __riscv_th_vmsge_vv_i64m4_b16(op1, op2, vl)
#define __riscv_vmsge_vv_i64m8_b8(op1, op2, vl) __riscv_th_vmsge_vv_i64m8_b8(op1, op2, vl)

#define __riscv_vmsge_vx_i8m1_b8(op1, op2, vl) __riscv_th_vmsge_vx_i8m1_b8(op1, op2, vl)
#define __riscv_vmsge_vx_i8m2_b4(op1, op2, vl) __riscv_th_vmsge_vx_i8m2_b4(op1, op2, vl)
#define __riscv_vmsge_vx_i8m4_b2(op1, op2, vl) __riscv_th_vmsge_vx_i8m4_b2(op1, op2, vl)
#define __riscv_vmsge_vx_i8m8_b1(op1, op2, vl) __riscv_th_vmsge_vx_i8m8_b1(op1, op2, vl)
#define __riscv_vmsge_vx_i16m1_b16(op1, op2, vl) __riscv_th_vmsge_vx_i16m1_b16(op1, op2, vl)
#define __riscv_vmsge_vx_i16m2_b8(op1, op2, vl) __riscv_th_vmsge_vx_i16m2_b8(op1, op2, vl)
#define __riscv_vmsge_vx_i16m4_b4(op1, op2, vl) __riscv_th_vmsge_vx_i16m4_b4(op1, op2, vl)
#define __riscv_vmsge_vx_i16m8_b2(op1, op2, vl) __riscv_th_vmsge_vx_i16m8_b2(op1, op2, vl)
#define __riscv_vmsge_vx_i32m1_b32(op1, op2, vl) __riscv_th_vmsge_vx_i32m1_b32(op1, op2, vl)
#define __riscv_vmsge_vx_i32m2_b16(op1, op2, vl) __riscv_th_vmsge_vx_i32m2_b16(op1, op2, vl)
#define __riscv_vmsge_vx_i32m4_b8(op1, op2, vl) __riscv_th_vmsge_vx_i32m4_b8(op1, op2, vl)
#define __riscv_vmsge_vx_i32m8_b4(op1, op2, vl) __riscv_th_vmsge_vx_i32m8_b4(op1, op2, vl)
#define __riscv_vmsge_vx_i64m1_b64(op1, op2, vl) __riscv_th_vmsge_vx_i64m1_b64(op1, op2, vl)
#define __riscv_vmsge_vx_i64m2_b32(op1, op2, vl) __riscv_th_vmsge_vx_i64m2_b32(op1, op2, vl)
#define __riscv_vmsge_vx_i64m4_b16(op1, op2, vl) __riscv_th_vmsge_vx_i64m4_b16(op1, op2, vl)
#define __riscv_vmsge_vx_i64m8_b8(op1, op2, vl) __riscv_th_vmsge_vx_i64m8_b8(op1, op2, vl)

#define __riscv_vmsge_vv_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vv_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vv_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsge_vx_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsge_vx_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsge_vx_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsgt_vv_i8m1_b8(op1, op2, vl) __riscv_th_vmsgt_vv_i8m1_b8(op1, op2, vl)
#define __riscv_vmsgt_vv_i8m2_b4(op1, op2, vl) __riscv_th_vmsgt_vv_i8m2_b4(op1, op2, vl)
#define __riscv_vmsgt_vv_i8m4_b2(op1, op2, vl) __riscv_th_vmsgt_vv_i8m4_b2(op1, op2, vl)
#define __riscv_vmsgt_vv_i8m8_b1(op1, op2, vl) __riscv_th_vmsgt_vv_i8m8_b1(op1, op2, vl)
#define __riscv_vmsgt_vv_i16m1_b16(op1, op2, vl) __riscv_th_vmsgt_vv_i16m1_b16(op1, op2, vl)
#define __riscv_vmsgt_vv_i16m2_b8(op1, op2, vl) __riscv_th_vmsgt_vv_i16m2_b8(op1, op2, vl)
#define __riscv_vmsgt_vv_i16m4_b4(op1, op2, vl) __riscv_th_vmsgt_vv_i16m4_b4(op1, op2, vl)
#define __riscv_vmsgt_vv_i16m8_b2(op1, op2, vl) __riscv_th_vmsgt_vv_i16m8_b2(op1, op2, vl)
#define __riscv_vmsgt_vv_i32m1_b32(op1, op2, vl) __riscv_th_vmsgt_vv_i32m1_b32(op1, op2, vl)
#define __riscv_vmsgt_vv_i32m2_b16(op1, op2, vl) __riscv_th_vmsgt_vv_i32m2_b16(op1, op2, vl)
#define __riscv_vmsgt_vv_i32m4_b8(op1, op2, vl) __riscv_th_vmsgt_vv_i32m4_b8(op1, op2, vl)
#define __riscv_vmsgt_vv_i32m8_b4(op1, op2, vl) __riscv_th_vmsgt_vv_i32m8_b4(op1, op2, vl)
#define __riscv_vmsgt_vv_i64m1_b64(op1, op2, vl) __riscv_th_vmsgt_vv_i64m1_b64(op1, op2, vl)
#define __riscv_vmsgt_vv_i64m2_b32(op1, op2, vl) __riscv_th_vmsgt_vv_i64m2_b32(op1, op2, vl)
#define __riscv_vmsgt_vv_i64m4_b16(op1, op2, vl) __riscv_th_vmsgt_vv_i64m4_b16(op1, op2, vl)
#define __riscv_vmsgt_vv_i64m8_b8(op1, op2, vl) __riscv_th_vmsgt_vv_i64m8_b8(op1, op2, vl)

#define __riscv_vmsgt_vx_i8m1_b8(op1, op2, vl) __riscv_th_vmsgt_vx_i8m1_b8(op1, op2, vl)
#define __riscv_vmsgt_vx_i8m2_b4(op1, op2, vl) __riscv_th_vmsgt_vx_i8m2_b4(op1, op2, vl)
#define __riscv_vmsgt_vx_i8m4_b2(op1, op2, vl) __riscv_th_vmsgt_vx_i8m4_b2(op1, op2, vl)
#define __riscv_vmsgt_vx_i8m8_b1(op1, op2, vl) __riscv_th_vmsgt_vx_i8m8_b1(op1, op2, vl)
#define __riscv_vmsgt_vx_i16m1_b16(op1, op2, vl) __riscv_th_vmsgt_vx_i16m1_b16(op1, op2, vl)
#define __riscv_vmsgt_vx_i16m2_b8(op1, op2, vl) __riscv_th_vmsgt_vx_i16m2_b8(op1, op2, vl)
#define __riscv_vmsgt_vx_i16m4_b4(op1, op2, vl) __riscv_th_vmsgt_vx_i16m4_b4(op1, op2, vl)
#define __riscv_vmsgt_vx_i16m8_b2(op1, op2, vl) __riscv_th_vmsgt_vx_i16m8_b2(op1, op2, vl)
#define __riscv_vmsgt_vx_i32m1_b32(op1, op2, vl) __riscv_th_vmsgt_vx_i32m1_b32(op1, op2, vl)
#define __riscv_vmsgt_vx_i32m2_b16(op1, op2, vl) __riscv_th_vmsgt_vx_i32m2_b16(op1, op2, vl)
#define __riscv_vmsgt_vx_i32m4_b8(op1, op2, vl) __riscv_th_vmsgt_vx_i32m4_b8(op1, op2, vl)
#define __riscv_vmsgt_vx_i32m8_b4(op1, op2, vl) __riscv_th_vmsgt_vx_i32m8_b4(op1, op2, vl)
#define __riscv_vmsgt_vx_i64m1_b64(op1, op2, vl) __riscv_th_vmsgt_vx_i64m1_b64(op1, op2, vl)
#define __riscv_vmsgt_vx_i64m2_b32(op1, op2, vl) __riscv_th_vmsgt_vx_i64m2_b32(op1, op2, vl)
#define __riscv_vmsgt_vx_i64m4_b16(op1, op2, vl) __riscv_th_vmsgt_vx_i64m4_b16(op1, op2, vl)
#define __riscv_vmsgt_vx_i64m8_b8(op1, op2, vl) __riscv_th_vmsgt_vx_i64m8_b8(op1, op2, vl)

#define __riscv_vmsgt_vv_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vv_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vv_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsgt_vx_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgt_vx_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgt_vx_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmslt_vv_i8m1_b8(op1, op2, vl) __riscv_th_vmslt_vv_i8m1_b8(op1, op2, vl)
#define __riscv_vmslt_vv_i8m2_b4(op1, op2, vl) __riscv_th_vmslt_vv_i8m2_b4(op1, op2, vl)
#define __riscv_vmslt_vv_i8m4_b2(op1, op2, vl) __riscv_th_vmslt_vv_i8m4_b2(op1, op2, vl)
#define __riscv_vmslt_vv_i8m8_b1(op1, op2, vl) __riscv_th_vmslt_vv_i8m8_b1(op1, op2, vl)
#define __riscv_vmslt_vv_i16m1_b16(op1, op2, vl) __riscv_th_vmslt_vv_i16m1_b16(op1, op2, vl)
#define __riscv_vmslt_vv_i16m2_b8(op1, op2, vl) __riscv_th_vmslt_vv_i16m2_b8(op1, op2, vl)
#define __riscv_vmslt_vv_i16m4_b4(op1, op2, vl) __riscv_th_vmslt_vv_i16m4_b4(op1, op2, vl)
#define __riscv_vmslt_vv_i16m8_b2(op1, op2, vl) __riscv_th_vmslt_vv_i16m8_b2(op1, op2, vl)
#define __riscv_vmslt_vv_i32m1_b32(op1, op2, vl) __riscv_th_vmslt_vv_i32m1_b32(op1, op2, vl)
#define __riscv_vmslt_vv_i32m2_b16(op1, op2, vl) __riscv_th_vmslt_vv_i32m2_b16(op1, op2, vl)
#define __riscv_vmslt_vv_i32m4_b8(op1, op2, vl) __riscv_th_vmslt_vv_i32m4_b8(op1, op2, vl)
#define __riscv_vmslt_vv_i32m8_b4(op1, op2, vl) __riscv_th_vmslt_vv_i32m8_b4(op1, op2, vl)
#define __riscv_vmslt_vv_i64m1_b64(op1, op2, vl) __riscv_th_vmslt_vv_i64m1_b64(op1, op2, vl)
#define __riscv_vmslt_vv_i64m2_b32(op1, op2, vl) __riscv_th_vmslt_vv_i64m2_b32(op1, op2, vl)
#define __riscv_vmslt_vv_i64m4_b16(op1, op2, vl) __riscv_th_vmslt_vv_i64m4_b16(op1, op2, vl)
#define __riscv_vmslt_vv_i64m8_b8(op1, op2, vl) __riscv_th_vmslt_vv_i64m8_b8(op1, op2, vl)

#define __riscv_vmslt_vx_i8m1_b8(op1, op2, vl) __riscv_th_vmslt_vx_i8m1_b8(op1, op2, vl)
#define __riscv_vmslt_vx_i8m2_b4(op1, op2, vl) __riscv_th_vmslt_vx_i8m2_b4(op1, op2, vl)
#define __riscv_vmslt_vx_i8m4_b2(op1, op2, vl) __riscv_th_vmslt_vx_i8m4_b2(op1, op2, vl)
#define __riscv_vmslt_vx_i8m8_b1(op1, op2, vl) __riscv_th_vmslt_vx_i8m8_b1(op1, op2, vl)
#define __riscv_vmslt_vx_i16m1_b16(op1, op2, vl) __riscv_th_vmslt_vx_i16m1_b16(op1, op2, vl)
#define __riscv_vmslt_vx_i16m2_b8(op1, op2, vl) __riscv_th_vmslt_vx_i16m2_b8(op1, op2, vl)
#define __riscv_vmslt_vx_i16m4_b4(op1, op2, vl) __riscv_th_vmslt_vx_i16m4_b4(op1, op2, vl)
#define __riscv_vmslt_vx_i16m8_b2(op1, op2, vl) __riscv_th_vmslt_vx_i16m8_b2(op1, op2, vl)
#define __riscv_vmslt_vx_i32m1_b32(op1, op2, vl) __riscv_th_vmslt_vx_i32m1_b32(op1, op2, vl)
#define __riscv_vmslt_vx_i32m2_b16(op1, op2, vl) __riscv_th_vmslt_vx_i32m2_b16(op1, op2, vl)
#define __riscv_vmslt_vx_i32m4_b8(op1, op2, vl) __riscv_th_vmslt_vx_i32m4_b8(op1, op2, vl)
#define __riscv_vmslt_vx_i32m8_b4(op1, op2, vl) __riscv_th_vmslt_vx_i32m8_b4(op1, op2, vl)
#define __riscv_vmslt_vx_i64m1_b64(op1, op2, vl) __riscv_th_vmslt_vx_i64m1_b64(op1, op2, vl)
#define __riscv_vmslt_vx_i64m2_b32(op1, op2, vl) __riscv_th_vmslt_vx_i64m2_b32(op1, op2, vl)
#define __riscv_vmslt_vx_i64m4_b16(op1, op2, vl) __riscv_th_vmslt_vx_i64m4_b16(op1, op2, vl)
#define __riscv_vmslt_vx_i64m8_b8(op1, op2, vl) __riscv_th_vmslt_vx_i64m8_b8(op1, op2, vl)

#define __riscv_vmslt_vv_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vv_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vv_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmslt_vx_i8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmslt_vx_i64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmslt_vx_i64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsgeu_vv_u8m1_b8(op1, op2, vl) __riscv_th_vmsgeu_vv_u8m1_b8(op1, op2, vl)
#define __riscv_vmsgeu_vv_u8m2_b4(op1, op2, vl) __riscv_th_vmsgeu_vv_u8m2_b4(op1, op2, vl)
#define __riscv_vmsgeu_vv_u8m4_b2(op1, op2, vl) __riscv_th_vmsgeu_vv_u8m4_b2(op1, op2, vl)
#define __riscv_vmsgeu_vv_u8m8_b1(op1, op2, vl) __riscv_th_vmsgeu_vv_u8m8_b1(op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m1_b16(op1, op2, vl) __riscv_th_vmsgeu_vv_u16m1_b16(op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m2_b8(op1, op2, vl) __riscv_th_vmsgeu_vv_u16m2_b8(op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m4_b4(op1, op2, vl) __riscv_th_vmsgeu_vv_u16m4_b4(op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m8_b2(op1, op2, vl) __riscv_th_vmsgeu_vv_u16m8_b2(op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m1_b32(op1, op2, vl) __riscv_th_vmsgeu_vv_u32m1_b32(op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m2_b16(op1, op2, vl) __riscv_th_vmsgeu_vv_u32m2_b16(op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m4_b8(op1, op2, vl) __riscv_th_vmsgeu_vv_u32m4_b8(op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m8_b4(op1, op2, vl) __riscv_th_vmsgeu_vv_u32m8_b4(op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m1_b64(op1, op2, vl) __riscv_th_vmsgeu_vv_u64m1_b64(op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m2_b32(op1, op2, vl) __riscv_th_vmsgeu_vv_u64m2_b32(op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m4_b16(op1, op2, vl) __riscv_th_vmsgeu_vv_u64m4_b16(op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m8_b8(op1, op2, vl) __riscv_th_vmsgeu_vv_u64m8_b8(op1, op2, vl)

#define __riscv_vmsgeu_vx_u8m1_b8(op1, op2, vl) __riscv_th_vmsgeu_vx_u8m1_b8(op1, op2, vl)
#define __riscv_vmsgeu_vx_u8m2_b4(op1, op2, vl) __riscv_th_vmsgeu_vx_u8m2_b4(op1, op2, vl)
#define __riscv_vmsgeu_vx_u8m4_b2(op1, op2, vl) __riscv_th_vmsgeu_vx_u8m4_b2(op1, op2, vl)
#define __riscv_vmsgeu_vx_u8m8_b1(op1, op2, vl) __riscv_th_vmsgeu_vx_u8m8_b1(op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m1_b16(op1, op2, vl) __riscv_th_vmsgeu_vx_u16m1_b16(op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m2_b8(op1, op2, vl) __riscv_th_vmsgeu_vx_u16m2_b8(op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m4_b4(op1, op2, vl) __riscv_th_vmsgeu_vx_u16m4_b4(op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m8_b2(op1, op2, vl) __riscv_th_vmsgeu_vx_u16m8_b2(op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m1_b32(op1, op2, vl) __riscv_th_vmsgeu_vx_u32m1_b32(op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m2_b16(op1, op2, vl) __riscv_th_vmsgeu_vx_u32m2_b16(op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m4_b8(op1, op2, vl) __riscv_th_vmsgeu_vx_u32m4_b8(op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m8_b4(op1, op2, vl) __riscv_th_vmsgeu_vx_u32m8_b4(op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m1_b64(op1, op2, vl) __riscv_th_vmsgeu_vx_u64m1_b64(op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m2_b32(op1, op2, vl) __riscv_th_vmsgeu_vx_u64m2_b32(op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m4_b16(op1, op2, vl) __riscv_th_vmsgeu_vx_u64m4_b16(op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m8_b8(op1, op2, vl) __riscv_th_vmsgeu_vx_u64m8_b8(op1, op2, vl)

#define __riscv_vmsgeu_vv_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vv_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vv_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsgeu_vx_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgeu_vx_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgeu_vx_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsgtu_vv_u8m1_b8(op1, op2, vl) __riscv_th_vmsgtu_vv_u8m1_b8(op1, op2, vl)
#define __riscv_vmsgtu_vv_u8m2_b4(op1, op2, vl) __riscv_th_vmsgtu_vv_u8m2_b4(op1, op2, vl)
#define __riscv_vmsgtu_vv_u8m4_b2(op1, op2, vl) __riscv_th_vmsgtu_vv_u8m4_b2(op1, op2, vl)
#define __riscv_vmsgtu_vv_u8m8_b1(op1, op2, vl) __riscv_th_vmsgtu_vv_u8m8_b1(op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m1_b16(op1, op2, vl) __riscv_th_vmsgtu_vv_u16m1_b16(op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m2_b8(op1, op2, vl) __riscv_th_vmsgtu_vv_u16m2_b8(op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m4_b4(op1, op2, vl) __riscv_th_vmsgtu_vv_u16m4_b4(op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m8_b2(op1, op2, vl) __riscv_th_vmsgtu_vv_u16m8_b2(op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m1_b32(op1, op2, vl) __riscv_th_vmsgtu_vv_u32m1_b32(op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m2_b16(op1, op2, vl) __riscv_th_vmsgtu_vv_u32m2_b16(op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m4_b8(op1, op2, vl) __riscv_th_vmsgtu_vv_u32m4_b8(op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m8_b4(op1, op2, vl) __riscv_th_vmsgtu_vv_u32m8_b4(op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m1_b64(op1, op2, vl) __riscv_th_vmsgtu_vv_u64m1_b64(op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m2_b32(op1, op2, vl) __riscv_th_vmsgtu_vv_u64m2_b32(op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m4_b16(op1, op2, vl) __riscv_th_vmsgtu_vv_u64m4_b16(op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m8_b8(op1, op2, vl) __riscv_th_vmsgtu_vv_u64m8_b8(op1, op2, vl)

#define __riscv_vmsgtu_vx_u8m1_b8(op1, op2, vl) __riscv_th_vmsgtu_vx_u8m1_b8(op1, op2, vl)
#define __riscv_vmsgtu_vx_u8m2_b4(op1, op2, vl) __riscv_th_vmsgtu_vx_u8m2_b4(op1, op2, vl)
#define __riscv_vmsgtu_vx_u8m4_b2(op1, op2, vl) __riscv_th_vmsgtu_vx_u8m4_b2(op1, op2, vl)
#define __riscv_vmsgtu_vx_u8m8_b1(op1, op2, vl) __riscv_th_vmsgtu_vx_u8m8_b1(op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m1_b16(op1, op2, vl) __riscv_th_vmsgtu_vx_u16m1_b16(op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m2_b8(op1, op2, vl) __riscv_th_vmsgtu_vx_u16m2_b8(op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m4_b4(op1, op2, vl) __riscv_th_vmsgtu_vx_u16m4_b4(op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m8_b2(op1, op2, vl) __riscv_th_vmsgtu_vx_u16m8_b2(op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m1_b32(op1, op2, vl) __riscv_th_vmsgtu_vx_u32m1_b32(op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m2_b16(op1, op2, vl) __riscv_th_vmsgtu_vx_u32m2_b16(op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m4_b8(op1, op2, vl) __riscv_th_vmsgtu_vx_u32m4_b8(op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m8_b4(op1, op2, vl) __riscv_th_vmsgtu_vx_u32m8_b4(op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m1_b64(op1, op2, vl) __riscv_th_vmsgtu_vx_u64m1_b64(op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m2_b32(op1, op2, vl) __riscv_th_vmsgtu_vx_u64m2_b32(op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m4_b16(op1, op2, vl) __riscv_th_vmsgtu_vx_u64m4_b16(op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m8_b8(op1, op2, vl) __riscv_th_vmsgtu_vx_u64m8_b8(op1, op2, vl)

#define __riscv_vmsgtu_vv_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vv_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vv_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsgtu_vx_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsgtu_vx_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsgtu_vx_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsltu_vv_u8m1_b8(op1, op2, vl) __riscv_th_vmsltu_vv_u8m1_b8(op1, op2, vl)
#define __riscv_vmsltu_vv_u8m2_b4(op1, op2, vl) __riscv_th_vmsltu_vv_u8m2_b4(op1, op2, vl)
#define __riscv_vmsltu_vv_u8m4_b2(op1, op2, vl) __riscv_th_vmsltu_vv_u8m4_b2(op1, op2, vl)
#define __riscv_vmsltu_vv_u8m8_b1(op1, op2, vl) __riscv_th_vmsltu_vv_u8m8_b1(op1, op2, vl)
#define __riscv_vmsltu_vv_u16m1_b16(op1, op2, vl) __riscv_th_vmsltu_vv_u16m1_b16(op1, op2, vl)
#define __riscv_vmsltu_vv_u16m2_b8(op1, op2, vl) __riscv_th_vmsltu_vv_u16m2_b8(op1, op2, vl)
#define __riscv_vmsltu_vv_u16m4_b4(op1, op2, vl) __riscv_th_vmsltu_vv_u16m4_b4(op1, op2, vl)
#define __riscv_vmsltu_vv_u16m8_b2(op1, op2, vl) __riscv_th_vmsltu_vv_u16m8_b2(op1, op2, vl)
#define __riscv_vmsltu_vv_u32m1_b32(op1, op2, vl) __riscv_th_vmsltu_vv_u32m1_b32(op1, op2, vl)
#define __riscv_vmsltu_vv_u32m2_b16(op1, op2, vl) __riscv_th_vmsltu_vv_u32m2_b16(op1, op2, vl)
#define __riscv_vmsltu_vv_u32m4_b8(op1, op2, vl) __riscv_th_vmsltu_vv_u32m4_b8(op1, op2, vl)
#define __riscv_vmsltu_vv_u32m8_b4(op1, op2, vl) __riscv_th_vmsltu_vv_u32m8_b4(op1, op2, vl)
#define __riscv_vmsltu_vv_u64m1_b64(op1, op2, vl) __riscv_th_vmsltu_vv_u64m1_b64(op1, op2, vl)
#define __riscv_vmsltu_vv_u64m2_b32(op1, op2, vl) __riscv_th_vmsltu_vv_u64m2_b32(op1, op2, vl)
#define __riscv_vmsltu_vv_u64m4_b16(op1, op2, vl) __riscv_th_vmsltu_vv_u64m4_b16(op1, op2, vl)
#define __riscv_vmsltu_vv_u64m8_b8(op1, op2, vl) __riscv_th_vmsltu_vv_u64m8_b8(op1, op2, vl)

#define __riscv_vmsltu_vx_u8m1_b8(op1, op2, vl) __riscv_th_vmsltu_vx_u8m1_b8(op1, op2, vl)
#define __riscv_vmsltu_vx_u8m2_b4(op1, op2, vl) __riscv_th_vmsltu_vx_u8m2_b4(op1, op2, vl)
#define __riscv_vmsltu_vx_u8m4_b2(op1, op2, vl) __riscv_th_vmsltu_vx_u8m4_b2(op1, op2, vl)
#define __riscv_vmsltu_vx_u8m8_b1(op1, op2, vl) __riscv_th_vmsltu_vx_u8m8_b1(op1, op2, vl)
#define __riscv_vmsltu_vx_u16m1_b16(op1, op2, vl) __riscv_th_vmsltu_vx_u16m1_b16(op1, op2, vl)
#define __riscv_vmsltu_vx_u16m2_b8(op1, op2, vl) __riscv_th_vmsltu_vx_u16m2_b8(op1, op2, vl)
#define __riscv_vmsltu_vx_u16m4_b4(op1, op2, vl) __riscv_th_vmsltu_vx_u16m4_b4(op1, op2, vl)
#define __riscv_vmsltu_vx_u16m8_b2(op1, op2, vl) __riscv_th_vmsltu_vx_u16m8_b2(op1, op2, vl)
#define __riscv_vmsltu_vx_u32m1_b32(op1, op2, vl) __riscv_th_vmsltu_vx_u32m1_b32(op1, op2, vl)
#define __riscv_vmsltu_vx_u32m2_b16(op1, op2, vl) __riscv_th_vmsltu_vx_u32m2_b16(op1, op2, vl)
#define __riscv_vmsltu_vx_u32m4_b8(op1, op2, vl) __riscv_th_vmsltu_vx_u32m4_b8(op1, op2, vl)
#define __riscv_vmsltu_vx_u32m8_b4(op1, op2, vl) __riscv_th_vmsltu_vx_u32m8_b4(op1, op2, vl)
#define __riscv_vmsltu_vx_u64m1_b64(op1, op2, vl) __riscv_th_vmsltu_vx_u64m1_b64(op1, op2, vl)
#define __riscv_vmsltu_vx_u64m2_b32(op1, op2, vl) __riscv_th_vmsltu_vx_u64m2_b32(op1, op2, vl)
#define __riscv_vmsltu_vx_u64m4_b16(op1, op2, vl) __riscv_th_vmsltu_vx_u64m4_b16(op1, op2, vl)
#define __riscv_vmsltu_vx_u64m8_b8(op1, op2, vl) __riscv_th_vmsltu_vx_u64m8_b8(op1, op2, vl)

#define __riscv_vmsltu_vv_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vv_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vv_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vmsltu_vx_u8m1_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u8m1_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u8m2_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u8m2_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u8m4_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u8m4_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u8m8_b1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u8m8_b1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmsltu_vx_u64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmsltu_vx_u64m8_b8_mu(mask, maskedoff, op1, op2, vl)
}] in
def th_integer_comparison_wrapper_macros: RVVHeader;

// 12.8. Vector Integer Min/Max Operations

// 12.9. Vector Single-Width Integer Multiply Operations

// 12.10. Vector Integer Divide Operations

// 12.11. Vector Widening Integer Multiply Operations

// 12.12. Vector Single-Width Integer Multiply-Add Operations

// 12.13. Vector Widening Integer Multiply-Add Operations

// 12.14. Vector Integer Merge Operations

// 12.15. Vector Integer Move Operations
let HeaderCode =
[{
// Vector Integer Merge Operations
#define __riscv_vmv_v_v_i8m1(src, vl) __riscv_th_vmv_v_v_i8m1(src, vl)
#define __riscv_vmv_v_x_i8m1(src, vl) __riscv_th_vmv_v_x_i8m1(src, vl)
#define __riscv_vmv_v_v_i8m2(src, vl) __riscv_th_vmv_v_v_i8m2(src, vl)
#define __riscv_vmv_v_x_i8m2(src, vl) __riscv_th_vmv_v_x_i8m2(src, vl)
#define __riscv_vmv_v_v_i8m4(src, vl) __riscv_th_vmv_v_v_i8m4(src, vl)
#define __riscv_vmv_v_x_i8m4(src, vl) __riscv_th_vmv_v_x_i8m4(src, vl)
#define __riscv_vmv_v_v_i8m8(src, vl) __riscv_th_vmv_v_v_i8m8(src, vl)
#define __riscv_vmv_v_x_i8m8(src, vl) __riscv_th_vmv_v_x_i8m8(src, vl)
#define __riscv_vmv_v_v_i16m1(src, vl) __riscv_th_vmv_v_v_i16m1(src, vl)
#define __riscv_vmv_v_x_i16m1(src, vl) __riscv_th_vmv_v_x_i16m1(src, vl)
#define __riscv_vmv_v_v_i16m2(src, vl) __riscv_th_vmv_v_v_i16m2(src, vl)
#define __riscv_vmv_v_x_i16m2(src, vl) __riscv_th_vmv_v_x_i16m2(src, vl)
#define __riscv_vmv_v_v_i16m4(src, vl) __riscv_th_vmv_v_v_i16m4(src, vl)
#define __riscv_vmv_v_x_i16m4(src, vl) __riscv_th_vmv_v_x_i16m4(src, vl)
#define __riscv_vmv_v_v_i16m8(src, vl) __riscv_th_vmv_v_v_i16m8(src, vl)
#define __riscv_vmv_v_x_i16m8(src, vl) __riscv_th_vmv_v_x_i16m8(src, vl)
#define __riscv_vmv_v_v_i32m1(src, vl) __riscv_th_vmv_v_v_i32m1(src, vl)
#define __riscv_vmv_v_x_i32m1(src, vl) __riscv_th_vmv_v_x_i32m1(src, vl)
#define __riscv_vmv_v_v_i32m2(src, vl) __riscv_th_vmv_v_v_i32m2(src, vl)
#define __riscv_vmv_v_x_i32m2(src, vl) __riscv_th_vmv_v_x_i32m2(src, vl)
#define __riscv_vmv_v_v_i32m4(src, vl) __riscv_th_vmv_v_v_i32m4(src, vl)
#define __riscv_vmv_v_x_i32m4(src, vl) __riscv_th_vmv_v_x_i32m4(src, vl)
#define __riscv_vmv_v_v_i32m8(src, vl) __riscv_th_vmv_v_v_i32m8(src, vl)
#define __riscv_vmv_v_x_i32m8(src, vl) __riscv_th_vmv_v_x_i32m8(src, vl)
#define __riscv_vmv_v_v_i64m1(src, vl) __riscv_th_vmv_v_v_i64m1(src, vl)
#define __riscv_vmv_v_x_i64m1(src, vl) __riscv_th_vmv_v_x_i64m1(src, vl)
#define __riscv_vmv_v_v_i64m2(src, vl) __riscv_th_vmv_v_v_i64m2(src, vl)
#define __riscv_vmv_v_x_i64m2(src, vl) __riscv_th_vmv_v_x_i64m2(src, vl)
#define __riscv_vmv_v_v_i64m4(src, vl) __riscv_th_vmv_v_v_i64m4(src, vl)
#define __riscv_vmv_v_x_i64m4(src, vl) __riscv_th_vmv_v_x_i64m4(src, vl)
#define __riscv_vmv_v_v_i64m8(src, vl) __riscv_th_vmv_v_v_i64m8(src, vl)
#define __riscv_vmv_v_x_i64m8(src, vl) __riscv_th_vmv_v_x_i64m8(src, vl)
#define __riscv_vmv_v_v_u8m1(src, vl) __riscv_th_vmv_v_v_u8m1(src, vl)
#define __riscv_vmv_v_x_u8m1(src, vl) __riscv_th_vmv_v_x_u8m1(src, vl)
#define __riscv_vmv_v_v_u8m2(src, vl) __riscv_th_vmv_v_v_u8m2(src, vl)
#define __riscv_vmv_v_x_u8m2(src, vl) __riscv_th_vmv_v_x_u8m2(src, vl)
#define __riscv_vmv_v_v_u8m4(src, vl) __riscv_th_vmv_v_v_u8m4(src, vl)
#define __riscv_vmv_v_x_u8m4(src, vl) __riscv_th_vmv_v_x_u8m4(src, vl)
#define __riscv_vmv_v_v_u8m8(src, vl) __riscv_th_vmv_v_v_u8m8(src, vl)
#define __riscv_vmv_v_x_u8m8(src, vl) __riscv_th_vmv_v_x_u8m8(src, vl)
#define __riscv_vmv_v_v_u16m1(src, vl) __riscv_th_vmv_v_v_u16m1(src, vl)
#define __riscv_vmv_v_x_u16m1(src, vl) __riscv_th_vmv_v_x_u16m1(src, vl)
#define __riscv_vmv_v_v_u16m2(src, vl) __riscv_th_vmv_v_v_u16m2(src, vl)
#define __riscv_vmv_v_x_u16m2(src, vl) __riscv_th_vmv_v_x_u16m2(src, vl)
#define __riscv_vmv_v_v_u16m4(src, vl) __riscv_th_vmv_v_v_u16m4(src, vl)
#define __riscv_vmv_v_x_u16m4(src, vl) __riscv_th_vmv_v_x_u16m4(src, vl)
#define __riscv_vmv_v_v_u16m8(src, vl) __riscv_th_vmv_v_v_u16m8(src, vl)
#define __riscv_vmv_v_x_u16m8(src, vl) __riscv_th_vmv_v_x_u16m8(src, vl)
#define __riscv_vmv_v_v_u32m1(src, vl) __riscv_th_vmv_v_v_u32m1(src, vl)
#define __riscv_vmv_v_x_u32m1(src, vl) __riscv_th_vmv_v_x_u32m1(src, vl)
#define __riscv_vmv_v_v_u32m2(src, vl) __riscv_th_vmv_v_v_u32m2(src, vl)
#define __riscv_vmv_v_x_u32m2(src, vl) __riscv_th_vmv_v_x_u32m2(src, vl)
#define __riscv_vmv_v_v_u32m4(src, vl) __riscv_th_vmv_v_v_u32m4(src, vl)
#define __riscv_vmv_v_x_u32m4(src, vl) __riscv_th_vmv_v_x_u32m4(src, vl)
#define __riscv_vmv_v_v_u32m8(src, vl) __riscv_th_vmv_v_v_u32m8(src, vl)
#define __riscv_vmv_v_x_u32m8(src, vl) __riscv_th_vmv_v_x_u32m8(src, vl)
#define __riscv_vmv_v_v_u64m1(src, vl) __riscv_th_vmv_v_v_u64m1(src, vl)
#define __riscv_vmv_v_x_u64m1(src, vl) __riscv_th_vmv_v_x_u64m1(src, vl)
#define __riscv_vmv_v_v_u64m2(src, vl) __riscv_th_vmv_v_v_u64m2(src, vl)
#define __riscv_vmv_v_x_u64m2(src, vl) __riscv_th_vmv_v_x_u64m2(src, vl)
#define __riscv_vmv_v_v_u64m4(src, vl) __riscv_th_vmv_v_v_u64m4(src, vl)
#define __riscv_vmv_v_x_u64m4(src, vl) __riscv_th_vmv_v_x_u64m4(src, vl)
#define __riscv_vmv_v_v_u64m8(src, vl) __riscv_th_vmv_v_v_u64m8(src, vl)
#define __riscv_vmv_v_x_u64m8(src, vl) __riscv_th_vmv_v_x_u64m8(src, vl)
#define __riscv_vmv_v_v_f16m1(src, vl) __riscv_th_vmv_v_v_f16m1(src, vl)
#define __riscv_vmv_v_v_f16m2(src, vl) __riscv_th_vmv_v_v_f16m2(src, vl)
#define __riscv_vmv_v_v_f16m4(src, vl) __riscv_th_vmv_v_v_f16m4(src, vl)
#define __riscv_vmv_v_v_f16m8(src, vl) __riscv_th_vmv_v_v_f16m8(src, vl)
#define __riscv_vmv_v_v_f32m1(src, vl) __riscv_th_vmv_v_v_f32m1(src, vl)
#define __riscv_vmv_v_v_f32m2(src, vl) __riscv_th_vmv_v_v_f32m2(src, vl)
#define __riscv_vmv_v_v_f32m4(src, vl) __riscv_th_vmv_v_v_f32m4(src, vl)
#define __riscv_vmv_v_v_f32m8(src, vl) __riscv_th_vmv_v_v_f32m8(src, vl)
#define __riscv_vmv_v_v_f64m1(src, vl) __riscv_th_vmv_v_v_f64m1(src, vl)
#define __riscv_vmv_v_v_f64m2(src, vl) __riscv_th_vmv_v_v_f64m2(src, vl)
#define __riscv_vmv_v_v_f64m4(src, vl) __riscv_th_vmv_v_v_f64m4(src, vl)
#define __riscv_vmv_v_v_f64m8(src, vl) __riscv_th_vmv_v_v_f64m8(src, vl)

}] in
def th_integer_merge_wrapper_macros: RVVHeader;

// 13.1. Vector Single-Width Saturating Add and Subtract

let HeaderCode =
[{
// Vector Single-Width Saturating Add and Subtract
#define __riscv_vsadd_vv_i8m1(op1, op2, vl) __riscv_th_vsadd_vv_i8m1(op1, op2, vl)
#define __riscv_vsadd_vx_i8m1(op1, op2, vl) __riscv_th_vsadd_vx_i8m1(op1, op2, vl)
#define __riscv_vsadd_vv_i8m2(op1, op2, vl) __riscv_th_vsadd_vv_i8m2(op1, op2, vl)
#define __riscv_vsadd_vx_i8m2(op1, op2, vl) __riscv_th_vsadd_vx_i8m2(op1, op2, vl)
#define __riscv_vsadd_vv_i8m4(op1, op2, vl) __riscv_th_vsadd_vv_i8m4(op1, op2, vl)
#define __riscv_vsadd_vx_i8m4(op1, op2, vl) __riscv_th_vsadd_vx_i8m4(op1, op2, vl)
#define __riscv_vsadd_vv_i8m8(op1, op2, vl) __riscv_th_vsadd_vv_i8m8(op1, op2, vl)
#define __riscv_vsadd_vx_i8m8(op1, op2, vl) __riscv_th_vsadd_vx_i8m8(op1, op2, vl)
#define __riscv_vsadd_vv_i16m1(op1, op2, vl) __riscv_th_vsadd_vv_i16m1(op1, op2, vl)
#define __riscv_vsadd_vx_i16m1(op1, op2, vl) __riscv_th_vsadd_vx_i16m1(op1, op2, vl)
#define __riscv_vsadd_vv_i16m2(op1, op2, vl) __riscv_th_vsadd_vv_i16m2(op1, op2, vl)
#define __riscv_vsadd_vx_i16m2(op1, op2, vl) __riscv_th_vsadd_vx_i16m2(op1, op2, vl)
#define __riscv_vsadd_vv_i16m4(op1, op2, vl) __riscv_th_vsadd_vv_i16m4(op1, op2, vl)
#define __riscv_vsadd_vx_i16m4(op1, op2, vl) __riscv_th_vsadd_vx_i16m4(op1, op2, vl)
#define __riscv_vsadd_vv_i16m8(op1, op2, vl) __riscv_th_vsadd_vv_i16m8(op1, op2, vl)
#define __riscv_vsadd_vx_i16m8(op1, op2, vl) __riscv_th_vsadd_vx_i16m8(op1, op2, vl)
#define __riscv_vsadd_vv_i32m1(op1, op2, vl) __riscv_th_vsadd_vv_i32m1(op1, op2, vl)
#define __riscv_vsadd_vx_i32m1(op1, op2, vl) __riscv_th_vsadd_vx_i32m1(op1, op2, vl)
#define __riscv_vsadd_vv_i32m2(op1, op2, vl) __riscv_th_vsadd_vv_i32m2(op1, op2, vl)
#define __riscv_vsadd_vx_i32m2(op1, op2, vl) __riscv_th_vsadd_vx_i32m2(op1, op2, vl)
#define __riscv_vsadd_vv_i32m4(op1, op2, vl) __riscv_th_vsadd_vv_i32m4(op1, op2, vl)
#define __riscv_vsadd_vx_i32m4(op1, op2, vl) __riscv_th_vsadd_vx_i32m4(op1, op2, vl)
#define __riscv_vsadd_vv_i32m8(op1, op2, vl) __riscv_th_vsadd_vv_i32m8(op1, op2, vl)
#define __riscv_vsadd_vx_i32m8(op1, op2, vl) __riscv_th_vsadd_vx_i32m8(op1, op2, vl)
#define __riscv_vsadd_vv_i64m1(op1, op2, vl) __riscv_th_vsadd_vv_i64m1(op1, op2, vl)
#define __riscv_vsadd_vx_i64m1(op1, op2, vl) __riscv_th_vsadd_vx_i64m1(op1, op2, vl)
#define __riscv_vsadd_vv_i64m2(op1, op2, vl) __riscv_th_vsadd_vv_i64m2(op1, op2, vl)
#define __riscv_vsadd_vx_i64m2(op1, op2, vl) __riscv_th_vsadd_vx_i64m2(op1, op2, vl)
#define __riscv_vsadd_vv_i64m4(op1, op2, vl) __riscv_th_vsadd_vv_i64m4(op1, op2, vl)
#define __riscv_vsadd_vx_i64m4(op1, op2, vl) __riscv_th_vsadd_vx_i64m4(op1, op2, vl)
#define __riscv_vsadd_vv_i64m8(op1, op2, vl) __riscv_th_vsadd_vv_i64m8(op1, op2, vl)
#define __riscv_vsadd_vx_i64m8(op1, op2, vl) __riscv_th_vsadd_vx_i64m8(op1, op2, vl)

#define __riscv_vsadd_vv_i8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vv_i64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vv_i64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsadd_vx_i64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsadd_vx_i64m8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vssub_vv_i8m1(op1, op2, vl) __riscv_th_vssub_vv_i8m1(op1, op2, vl)
#define __riscv_vssub_vx_i8m1(op1, op2, vl) __riscv_th_vssub_vx_i8m1(op1, op2, vl)
#define __riscv_vssub_vv_i8m2(op1, op2, vl) __riscv_th_vssub_vv_i8m2(op1, op2, vl)
#define __riscv_vssub_vx_i8m2(op1, op2, vl) __riscv_th_vssub_vx_i8m2(op1, op2, vl)
#define __riscv_vssub_vv_i8m4(op1, op2, vl) __riscv_th_vssub_vv_i8m4(op1, op2, vl)
#define __riscv_vssub_vx_i8m4(op1, op2, vl) __riscv_th_vssub_vx_i8m4(op1, op2, vl)
#define __riscv_vssub_vv_i8m8(op1, op2, vl) __riscv_th_vssub_vv_i8m8(op1, op2, vl)
#define __riscv_vssub_vx_i8m8(op1, op2, vl) __riscv_th_vssub_vx_i8m8(op1, op2, vl)
#define __riscv_vssub_vv_i16m1(op1, op2, vl) __riscv_th_vssub_vv_i16m1(op1, op2, vl)
#define __riscv_vssub_vx_i16m1(op1, op2, vl) __riscv_th_vssub_vx_i16m1(op1, op2, vl)
#define __riscv_vssub_vv_i16m2(op1, op2, vl) __riscv_th_vssub_vv_i16m2(op1, op2, vl)
#define __riscv_vssub_vx_i16m2(op1, op2, vl) __riscv_th_vssub_vx_i16m2(op1, op2, vl)
#define __riscv_vssub_vv_i16m4(op1, op2, vl) __riscv_th_vssub_vv_i16m4(op1, op2, vl)
#define __riscv_vssub_vx_i16m4(op1, op2, vl) __riscv_th_vssub_vx_i16m4(op1, op2, vl)
#define __riscv_vssub_vv_i16m8(op1, op2, vl) __riscv_th_vssub_vv_i16m8(op1, op2, vl)
#define __riscv_vssub_vx_i16m8(op1, op2, vl) __riscv_th_vssub_vx_i16m8(op1, op2, vl)
#define __riscv_vssub_vv_i32m1(op1, op2, vl) __riscv_th_vssub_vv_i32m1(op1, op2, vl)
#define __riscv_vssub_vx_i32m1(op1, op2, vl) __riscv_th_vssub_vx_i32m1(op1, op2, vl)
#define __riscv_vssub_vv_i32m2(op1, op2, vl) __riscv_th_vssub_vv_i32m2(op1, op2, vl)
#define __riscv_vssub_vx_i32m2(op1, op2, vl) __riscv_th_vssub_vx_i32m2(op1, op2, vl)
#define __riscv_vssub_vv_i32m4(op1, op2, vl) __riscv_th_vssub_vv_i32m4(op1, op2, vl)
#define __riscv_vssub_vx_i32m4(op1, op2, vl) __riscv_th_vssub_vx_i32m4(op1, op2, vl)
#define __riscv_vssub_vv_i32m8(op1, op2, vl) __riscv_th_vssub_vv_i32m8(op1, op2, vl)
#define __riscv_vssub_vx_i32m8(op1, op2, vl) __riscv_th_vssub_vx_i32m8(op1, op2, vl)
#define __riscv_vssub_vv_i64m1(op1, op2, vl) __riscv_th_vssub_vv_i64m1(op1, op2, vl)
#define __riscv_vssub_vx_i64m1(op1, op2, vl) __riscv_th_vssub_vx_i64m1(op1, op2, vl)
#define __riscv_vssub_vv_i64m2(op1, op2, vl) __riscv_th_vssub_vv_i64m2(op1, op2, vl)
#define __riscv_vssub_vx_i64m2(op1, op2, vl) __riscv_th_vssub_vx_i64m2(op1, op2, vl)
#define __riscv_vssub_vv_i64m4(op1, op2, vl) __riscv_th_vssub_vv_i64m4(op1, op2, vl)
#define __riscv_vssub_vx_i64m4(op1, op2, vl) __riscv_th_vssub_vx_i64m4(op1, op2, vl)
#define __riscv_vssub_vv_i64m8(op1, op2, vl) __riscv_th_vssub_vv_i64m8(op1, op2, vl)
#define __riscv_vssub_vx_i64m8(op1, op2, vl) __riscv_th_vssub_vx_i64m8(op1, op2, vl)

#define __riscv_vssub_vv_i8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vv_i64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vv_i64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssub_vx_i64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssub_vx_i64m8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vsaddu_vv_u8m1(op1, op2, vl) __riscv_th_vsaddu_vv_u8m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m1(op1, op2, vl) __riscv_th_vsaddu_vx_u8m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u8m2(op1, op2, vl) __riscv_th_vsaddu_vv_u8m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m2(op1, op2, vl) __riscv_th_vsaddu_vx_u8m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u8m4(op1, op2, vl) __riscv_th_vsaddu_vv_u8m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m4(op1, op2, vl) __riscv_th_vsaddu_vx_u8m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u8m8(op1, op2, vl) __riscv_th_vsaddu_vv_u8m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m8(op1, op2, vl) __riscv_th_vsaddu_vx_u8m8(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m1(op1, op2, vl) __riscv_th_vsaddu_vv_u16m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m1(op1, op2, vl) __riscv_th_vsaddu_vx_u16m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m2(op1, op2, vl) __riscv_th_vsaddu_vv_u16m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m2(op1, op2, vl) __riscv_th_vsaddu_vx_u16m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m4(op1, op2, vl) __riscv_th_vsaddu_vv_u16m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m4(op1, op2, vl) __riscv_th_vsaddu_vx_u16m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m8(op1, op2, vl) __riscv_th_vsaddu_vv_u16m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m8(op1, op2, vl) __riscv_th_vsaddu_vx_u16m8(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m1(op1, op2, vl) __riscv_th_vsaddu_vv_u32m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m1(op1, op2, vl) __riscv_th_vsaddu_vx_u32m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m2(op1, op2, vl) __riscv_th_vsaddu_vv_u32m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m2(op1, op2, vl) __riscv_th_vsaddu_vx_u32m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m4(op1, op2, vl) __riscv_th_vsaddu_vv_u32m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m4(op1, op2, vl) __riscv_th_vsaddu_vx_u32m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m8(op1, op2, vl) __riscv_th_vsaddu_vv_u32m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m8(op1, op2, vl) __riscv_th_vsaddu_vx_u32m8(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m1(op1, op2, vl) __riscv_th_vsaddu_vv_u64m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m1(op1, op2, vl) __riscv_th_vsaddu_vx_u64m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m2(op1, op2, vl) __riscv_th_vsaddu_vv_u64m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m2(op1, op2, vl) __riscv_th_vsaddu_vx_u64m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m4(op1, op2, vl) __riscv_th_vsaddu_vv_u64m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m4(op1, op2, vl) __riscv_th_vsaddu_vx_u64m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m8(op1, op2, vl) __riscv_th_vsaddu_vv_u64m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m8(op1, op2, vl) __riscv_th_vsaddu_vx_u64m8(op1, op2, vl)

#define __riscv_vsaddu_vv_u8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vv_u64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vsaddu_vx_u64m8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vssubu_vv_u8m1(op1, op2, vl) __riscv_th_vssubu_vv_u8m1(op1, op2, vl)
#define __riscv_vssubu_vx_u8m1(op1, op2, vl) __riscv_th_vssubu_vx_u8m1(op1, op2, vl)
#define __riscv_vssubu_vv_u8m2(op1, op2, vl) __riscv_th_vssubu_vv_u8m2(op1, op2, vl)
#define __riscv_vssubu_vx_u8m2(op1, op2, vl) __riscv_th_vssubu_vx_u8m2(op1, op2, vl)
#define __riscv_vssubu_vv_u8m4(op1, op2, vl) __riscv_th_vssubu_vv_u8m4(op1, op2, vl)
#define __riscv_vssubu_vx_u8m4(op1, op2, vl) __riscv_th_vssubu_vx_u8m4(op1, op2, vl)
#define __riscv_vssubu_vv_u8m8(op1, op2, vl) __riscv_th_vssubu_vv_u8m8(op1, op2, vl)
#define __riscv_vssubu_vx_u8m8(op1, op2, vl) __riscv_th_vssubu_vx_u8m8(op1, op2, vl)
#define __riscv_vssubu_vv_u16m1(op1, op2, vl) __riscv_th_vssubu_vv_u16m1(op1, op2, vl)
#define __riscv_vssubu_vx_u16m1(op1, op2, vl) __riscv_th_vssubu_vx_u16m1(op1, op2, vl)
#define __riscv_vssubu_vv_u16m2(op1, op2, vl) __riscv_th_vssubu_vv_u16m2(op1, op2, vl)
#define __riscv_vssubu_vx_u16m2(op1, op2, vl) __riscv_th_vssubu_vx_u16m2(op1, op2, vl)
#define __riscv_vssubu_vv_u16m4(op1, op2, vl) __riscv_th_vssubu_vv_u16m4(op1, op2, vl)
#define __riscv_vssubu_vx_u16m4(op1, op2, vl) __riscv_th_vssubu_vx_u16m4(op1, op2, vl)
#define __riscv_vssubu_vv_u16m8(op1, op2, vl) __riscv_th_vssubu_vv_u16m8(op1, op2, vl)
#define __riscv_vssubu_vx_u16m8(op1, op2, vl) __riscv_th_vssubu_vx_u16m8(op1, op2, vl)
#define __riscv_vssubu_vv_u32m1(op1, op2, vl) __riscv_th_vssubu_vv_u32m1(op1, op2, vl)
#define __riscv_vssubu_vx_u32m1(op1, op2, vl) __riscv_th_vssubu_vx_u32m1(op1, op2, vl)
#define __riscv_vssubu_vv_u32m2(op1, op2, vl) __riscv_th_vssubu_vv_u32m2(op1, op2, vl)
#define __riscv_vssubu_vx_u32m2(op1, op2, vl) __riscv_th_vssubu_vx_u32m2(op1, op2, vl)
#define __riscv_vssubu_vv_u32m4(op1, op2, vl) __riscv_th_vssubu_vv_u32m4(op1, op2, vl)
#define __riscv_vssubu_vx_u32m4(op1, op2, vl) __riscv_th_vssubu_vx_u32m4(op1, op2, vl)
#define __riscv_vssubu_vv_u32m8(op1, op2, vl) __riscv_th_vssubu_vv_u32m8(op1, op2, vl)
#define __riscv_vssubu_vx_u32m8(op1, op2, vl) __riscv_th_vssubu_vx_u32m8(op1, op2, vl)
#define __riscv_vssubu_vv_u64m1(op1, op2, vl) __riscv_th_vssubu_vv_u64m1(op1, op2, vl)
#define __riscv_vssubu_vx_u64m1(op1, op2, vl) __riscv_th_vssubu_vx_u64m1(op1, op2, vl)
#define __riscv_vssubu_vv_u64m2(op1, op2, vl) __riscv_th_vssubu_vv_u64m2(op1, op2, vl)
#define __riscv_vssubu_vx_u64m2(op1, op2, vl) __riscv_th_vssubu_vx_u64m2(op1, op2, vl)
#define __riscv_vssubu_vv_u64m4(op1, op2, vl) __riscv_th_vssubu_vv_u64m4(op1, op2, vl)
#define __riscv_vssubu_vx_u64m4(op1, op2, vl) __riscv_th_vssubu_vx_u64m4(op1, op2, vl)
#define __riscv_vssubu_vv_u64m8(op1, op2, vl) __riscv_th_vssubu_vv_u64m8(op1, op2, vl)
#define __riscv_vssubu_vx_u64m8(op1, op2, vl) __riscv_th_vssubu_vx_u64m8(op1, op2, vl)

#define __riscv_vssubu_vv_u8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u8m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u8m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u8m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u8m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u8m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u8m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u8m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u8m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vv_u64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vv_u64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vssubu_vx_u64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vssubu_vx_u64m8_mu(mask, maskedoff, op1, op2, vl)

}] in
def th_single_width_saturating_add_wrapper_macros: RVVHeader;

// 13.2. Vector Single-Width Averaging Add and Subtract

let HeaderCode =
[{
// Vector Single-Width Averaging Add and Subtract
#define __riscv_vaadd_vv_i8m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m8(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m8(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m8(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m8(op1, op2, rm, vl)

#define __riscv_vaadd_vv_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m8_mu(mask, maskedoff, op1, op2, rm, vl)

#define __riscv_vasub_vv_i8m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m8(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m8(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m8(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m8(op1, op2, rm, vl)

#define __riscv_vasub_vv_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m8_mu(mask, maskedoff, op1, op2, rm, vl)

}] in
def th_single_width_averaging_add_and_subtract_wrapper_macros: RVVHeader;

// 13.3.  Vector Single-Width Fractional Multiply with Rounding and Saturation

let HeaderCode =
[{

#define __riscv_vsmul_vv_i8m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m8(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m8(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m8(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m8(op1, op2, rm, vl)

#define __riscv_vsmul_vv_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m8_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m8_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m8_mu(mask, maskedoff, op1, op2, rm, vl)

}] in
def th_single_width_fractional_multiply_with_rounding_and_saturation_wrapper_macros: RVVHeader;


// 13.4. Vector Single-Width Scaling Shift Operations

let HeaderCode =
[{

#define __riscv_vssra_vv_i8m1(op1, shift, rm, vl) __riscv_th_vssra_vv_i8m1(op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m1(op1, shift, rm, vl) __riscv_th_vssra_vx_i8m1(op1, shift, rm, vl)
#define __riscv_vssra_vv_i8m2(op1, shift, rm, vl) __riscv_th_vssra_vv_i8m2(op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m2(op1, shift, rm, vl) __riscv_th_vssra_vx_i8m2(op1, shift, rm, vl)
#define __riscv_vssra_vv_i8m4(op1, shift, rm, vl) __riscv_th_vssra_vv_i8m4(op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m4(op1, shift, rm, vl) __riscv_th_vssra_vx_i8m4(op1, shift, rm, vl)
#define __riscv_vssra_vv_i8m8(op1, shift, rm, vl) __riscv_th_vssra_vv_i8m8(op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m8(op1, shift, rm, vl) __riscv_th_vssra_vx_i8m8(op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m1(op1, shift, rm, vl) __riscv_th_vssra_vv_i16m1(op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m1(op1, shift, rm, vl) __riscv_th_vssra_vx_i16m1(op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m2(op1, shift, rm, vl) __riscv_th_vssra_vv_i16m2(op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m2(op1, shift, rm, vl) __riscv_th_vssra_vx_i16m2(op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m4(op1, shift, rm, vl) __riscv_th_vssra_vv_i16m4(op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m4(op1, shift, rm, vl) __riscv_th_vssra_vx_i16m4(op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m8(op1, shift, rm, vl) __riscv_th_vssra_vv_i16m8(op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m8(op1, shift, rm, vl) __riscv_th_vssra_vx_i16m8(op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m1(op1, shift, rm, vl) __riscv_th_vssra_vv_i32m1(op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m1(op1, shift, rm, vl) __riscv_th_vssra_vx_i32m1(op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m2(op1, shift, rm, vl) __riscv_th_vssra_vv_i32m2(op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m2(op1, shift, rm, vl) __riscv_th_vssra_vx_i32m2(op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m4(op1, shift, rm, vl) __riscv_th_vssra_vv_i32m4(op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m4(op1, shift, rm, vl) __riscv_th_vssra_vx_i32m4(op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m8(op1, shift, rm, vl) __riscv_th_vssra_vv_i32m8(op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m8(op1, shift, rm, vl) __riscv_th_vssra_vx_i32m8(op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m1(op1, shift, rm, vl) __riscv_th_vssra_vv_i64m1(op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m1(op1, shift, rm, vl) __riscv_th_vssra_vx_i64m1(op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m2(op1, shift, rm, vl) __riscv_th_vssra_vv_i64m2(op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m2(op1, shift, rm, vl) __riscv_th_vssra_vx_i64m2(op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m4(op1, shift, rm, vl) __riscv_th_vssra_vv_i64m4(op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m4(op1, shift, rm, vl) __riscv_th_vssra_vx_i64m4(op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m8(op1, shift, rm, vl) __riscv_th_vssra_vv_i64m8(op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m8(op1, shift, rm, vl) __riscv_th_vssra_vx_i64m8(op1, shift, rm, vl)

#define __riscv_vssra_vv_i8m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i8m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i8m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i8m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i8m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i8m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i8m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i8m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i8m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i8m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i8m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i8m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i8m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i16m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i16m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i16m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i16m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i16m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i16m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i16m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i16m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i16m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i16m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i32m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i32m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i32m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i32m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i32m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i32m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i32m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i32m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i32m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i32m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i64m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i64m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i64m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i64m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i64m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i64m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vv_i64m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vv_i64m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssra_vx_i64m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssra_vx_i64m8_mu(mask, maskedoff, op1, shift, rm, vl)

#define __riscv_vssrl_vv_u8m1(op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m1(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m1(op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m1(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u8m2(op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m2(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m2(op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m2(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u8m4(op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m4(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m4(op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m4(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u8m8(op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m8(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m8(op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m8(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m1(op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m1(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m1(op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m1(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m2(op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m2(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m2(op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m2(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m4(op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m4(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m4(op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m4(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m8(op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m8(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m8(op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m8(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m1(op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m1(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m1(op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m1(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m2(op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m2(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m2(op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m2(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m4(op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m4(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m4(op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m4(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m8(op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m8(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m8(op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m8(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m1(op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m1(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m1(op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m1(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m2(op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m2(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m2(op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m2(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m4(op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m4(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m4(op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m4(op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m8(op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m8(op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m8(op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m8(op1, shift, rm, vl)

#define __riscv_vssrl_vv_u8m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u8m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u8m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u8m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u8m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u8m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u8m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u16m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u16m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u16m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u16m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u32m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u32m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u32m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u32m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m1_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m1_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m2_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m2_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m4_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m4_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vv_u64m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vv_u64m8_mu(mask, maskedoff, op1, shift, rm, vl)
#define __riscv_vssrl_vx_u64m8_m(mask, maskedoff, op1, shift, rm, vl) __riscv_th_vssrl_vx_u64m8_mu(mask, maskedoff, op1, shift, rm, vl)

}] in
def th_single_width_scaling_shift_wrapper_macros: RVVHeader;

// 13.5. Vector Narrowing Fixed-Point Clip Operations

let HeaderCode =
[{
// Vector Narrowing Fixed-Point Clip Operations
#define __riscv_vnclip_wv_i8m1(op1, op2, rm, vl) __riscv_th_vnclip_wv_i8m1(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i8m1(op1, op2, rm, vl) __riscv_th_vnclip_wx_i8m1(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i8m2(op1, op2, rm, vl) __riscv_th_vnclip_wv_i8m2(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i8m2(op1, op2, rm, vl) __riscv_th_vnclip_wx_i8m2(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i8m4(op1, op2, rm, vl) __riscv_th_vnclip_wv_i8m4(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i8m4(op1, op2, rm, vl) __riscv_th_vnclip_wx_i8m4(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i16m1(op1, op2, rm, vl) __riscv_th_vnclip_wv_i16m1(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i16m1(op1, op2, rm, vl) __riscv_th_vnclip_wx_i16m1(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i16m2(op1, op2, rm, vl) __riscv_th_vnclip_wv_i16m2(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i16m2(op1, op2, rm, vl) __riscv_th_vnclip_wx_i16m2(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i16m4(op1, op2, rm, vl) __riscv_th_vnclip_wv_i16m4(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i16m4(op1, op2, rm, vl) __riscv_th_vnclip_wx_i16m4(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i32m1(op1, op2, rm, vl) __riscv_th_vnclip_wv_i32m1(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i32m1(op1, op2, rm, vl) __riscv_th_vnclip_wx_i32m1(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i32m2(op1, op2, rm, vl) __riscv_th_vnclip_wv_i32m2(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i32m2(op1, op2, rm, vl) __riscv_th_vnclip_wx_i32m2(op1, op2, rm, vl)
#define __riscv_vnclip_wv_i32m4(op1, op2, rm, vl) __riscv_th_vnclip_wv_i32m4(op1, op2, rm, vl)
#define __riscv_vnclip_wx_i32m4(op1, op2, rm, vl) __riscv_th_vnclip_wx_i32m4(op1, op2, rm, vl)

#define __riscv_vnclip_wv_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wv_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wv_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclip_wx_i32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclip_wx_i32m4_mu(mask, maskedoff, op1, op2, rm, vl)

#define __riscv_vnclipu_wv_u8m1(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u8m1(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u8m1(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u8m1(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u8m2(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u8m2(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u8m2(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u8m2(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u8m4(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u8m4(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u8m4(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u8m4(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u16m1(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u16m1(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u16m1(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u16m1(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u16m2(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u16m2(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u16m2(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u16m2(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u16m4(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u16m4(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u16m4(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u16m4(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u32m1(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u32m1(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u32m1(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u32m1(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u32m2(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u32m2(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u32m2(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u32m2(op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u32m4(op1, op2, rm, vl) __riscv_th_vnclipu_wv_u32m4(op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u32m4(op1, op2, rm, vl) __riscv_th_vnclipu_wx_u32m4(op1, op2, rm, vl)

#define __riscv_vnclipu_wv_u8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u8m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u8m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u8m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u8m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u8m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u8m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u16m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u16m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u16m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u16m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u16m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u16m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u32m1_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u32m1_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u32m2_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u32m2_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wv_u32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wv_u32m4_mu(mask, maskedoff, op1, op2, rm, vl)
#define __riscv_vnclipu_wx_u32m4_m(mask, maskedoff, op1, op2, rm, vl) __riscv_th_vnclipu_wx_u32m4_mu(mask, maskedoff, op1, op2, rm, vl)

}] in
def th_narrowing_width_fixed_point_clip_wrapper_macros: RVVHeader;

// 14. Vector Floating-Point Operations

let HeaderCode =
[{
// Vector Floating-Point Operations
#define __riscv_vfadd_vv_f16m1(op1, op2, vl) __riscv_th_vfadd_vv_f16m1(op1, op2, vl)
#define __riscv_vfadd_vf_f16m1(op1, op2, vl) __riscv_th_vfadd_vf_f16m1(op1, op2, vl)
#define __riscv_vfadd_vv_f16m2(op1, op2, vl) __riscv_th_vfadd_vv_f16m2(op1, op2, vl)
#define __riscv_vfadd_vf_f16m2(op1, op2, vl) __riscv_th_vfadd_vf_f16m2(op1, op2, vl)
#define __riscv_vfadd_vv_f16m4(op1, op2, vl) __riscv_th_vfadd_vv_f16m4(op1, op2, vl)
#define __riscv_vfadd_vf_f16m4(op1, op2, vl) __riscv_th_vfadd_vf_f16m4(op1, op2, vl)
#define __riscv_vfadd_vv_f16m8(op1, op2, vl) __riscv_th_vfadd_vv_f16m8(op1, op2, vl)
#define __riscv_vfadd_vf_f16m8(op1, op2, vl) __riscv_th_vfadd_vf_f16m8(op1, op2, vl)
#define __riscv_vfadd_vv_f32m1(op1, op2, vl) __riscv_th_vfadd_vv_f32m1(op1, op2, vl)
#define __riscv_vfadd_vf_f32m1(op1, op2, vl) __riscv_th_vfadd_vf_f32m1(op1, op2, vl)
#define __riscv_vfadd_vv_f32m2(op1, op2, vl) __riscv_th_vfadd_vv_f32m2(op1, op2, vl)
#define __riscv_vfadd_vf_f32m2(op1, op2, vl) __riscv_th_vfadd_vf_f32m2(op1, op2, vl)
#define __riscv_vfadd_vv_f32m4(op1, op2, vl) __riscv_th_vfadd_vv_f32m4(op1, op2, vl)
#define __riscv_vfadd_vf_f32m4(op1, op2, vl) __riscv_th_vfadd_vf_f32m4(op1, op2, vl)
#define __riscv_vfadd_vv_f32m8(op1, op2, vl) __riscv_th_vfadd_vv_f32m8(op1, op2, vl)
#define __riscv_vfadd_vf_f32m8(op1, op2, vl) __riscv_th_vfadd_vf_f32m8(op1, op2, vl)
#define __riscv_vfadd_vv_f64m1(op1, op2, vl) __riscv_th_vfadd_vv_f64m1(op1, op2, vl)
#define __riscv_vfadd_vf_f64m1(op1, op2, vl) __riscv_th_vfadd_vf_f64m1(op1, op2, vl)
#define __riscv_vfadd_vv_f64m2(op1, op2, vl) __riscv_th_vfadd_vv_f64m2(op1, op2, vl)
#define __riscv_vfadd_vf_f64m2(op1, op2, vl) __riscv_th_vfadd_vf_f64m2(op1, op2, vl)
#define __riscv_vfadd_vv_f64m4(op1, op2, vl) __riscv_th_vfadd_vv_f64m4(op1, op2, vl)
#define __riscv_vfadd_vf_f64m4(op1, op2, vl) __riscv_th_vfadd_vf_f64m4(op1, op2, vl)
#define __riscv_vfadd_vv_f64m8(op1, op2, vl) __riscv_th_vfadd_vv_f64m8(op1, op2, vl)
#define __riscv_vfadd_vf_f64m8(op1, op2, vl) __riscv_th_vfadd_vf_f64m8(op1, op2, vl)
#define __riscv_vfadd_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfadd_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfadd_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f16m1(op1, op2, vl) __riscv_th_vfdiv_vv_f16m1(op1, op2, vl)
#define __riscv_vfdiv_vf_f16m1(op1, op2, vl) __riscv_th_vfdiv_vf_f16m1(op1, op2, vl)
#define __riscv_vfdiv_vv_f16m2(op1, op2, vl) __riscv_th_vfdiv_vv_f16m2(op1, op2, vl)
#define __riscv_vfdiv_vf_f16m2(op1, op2, vl) __riscv_th_vfdiv_vf_f16m2(op1, op2, vl)
#define __riscv_vfdiv_vv_f16m4(op1, op2, vl) __riscv_th_vfdiv_vv_f16m4(op1, op2, vl)
#define __riscv_vfdiv_vf_f16m4(op1, op2, vl) __riscv_th_vfdiv_vf_f16m4(op1, op2, vl)
#define __riscv_vfdiv_vv_f16m8(op1, op2, vl) __riscv_th_vfdiv_vv_f16m8(op1, op2, vl)
#define __riscv_vfdiv_vf_f16m8(op1, op2, vl) __riscv_th_vfdiv_vf_f16m8(op1, op2, vl)
#define __riscv_vfdiv_vv_f32m1(op1, op2, vl) __riscv_th_vfdiv_vv_f32m1(op1, op2, vl)
#define __riscv_vfdiv_vf_f32m1(op1, op2, vl) __riscv_th_vfdiv_vf_f32m1(op1, op2, vl)
#define __riscv_vfdiv_vv_f32m2(op1, op2, vl) __riscv_th_vfdiv_vv_f32m2(op1, op2, vl)
#define __riscv_vfdiv_vf_f32m2(op1, op2, vl) __riscv_th_vfdiv_vf_f32m2(op1, op2, vl)
#define __riscv_vfdiv_vv_f32m4(op1, op2, vl) __riscv_th_vfdiv_vv_f32m4(op1, op2, vl)
#define __riscv_vfdiv_vf_f32m4(op1, op2, vl) __riscv_th_vfdiv_vf_f32m4(op1, op2, vl)
#define __riscv_vfdiv_vv_f32m8(op1, op2, vl) __riscv_th_vfdiv_vv_f32m8(op1, op2, vl)
#define __riscv_vfdiv_vf_f32m8(op1, op2, vl) __riscv_th_vfdiv_vf_f32m8(op1, op2, vl)
#define __riscv_vfdiv_vv_f64m1(op1, op2, vl) __riscv_th_vfdiv_vv_f64m1(op1, op2, vl)
#define __riscv_vfdiv_vf_f64m1(op1, op2, vl) __riscv_th_vfdiv_vf_f64m1(op1, op2, vl)
#define __riscv_vfdiv_vv_f64m2(op1, op2, vl) __riscv_th_vfdiv_vv_f64m2(op1, op2, vl)
#define __riscv_vfdiv_vf_f64m2(op1, op2, vl) __riscv_th_vfdiv_vf_f64m2(op1, op2, vl)
#define __riscv_vfdiv_vv_f64m4(op1, op2, vl) __riscv_th_vfdiv_vv_f64m4(op1, op2, vl)
#define __riscv_vfdiv_vf_f64m4(op1, op2, vl) __riscv_th_vfdiv_vf_f64m4(op1, op2, vl)
#define __riscv_vfdiv_vv_f64m8(op1, op2, vl) __riscv_th_vfdiv_vv_f64m8(op1, op2, vl)
#define __riscv_vfdiv_vf_f64m8(op1, op2, vl) __riscv_th_vfdiv_vf_f64m8(op1, op2, vl)
#define __riscv_vfdiv_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfdiv_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfdiv_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f16m1(op1, op2, vl) __riscv_th_vfmul_vv_f16m1(op1, op2, vl)
#define __riscv_vfmul_vf_f16m1(op1, op2, vl) __riscv_th_vfmul_vf_f16m1(op1, op2, vl)
#define __riscv_vfmul_vv_f16m2(op1, op2, vl) __riscv_th_vfmul_vv_f16m2(op1, op2, vl)
#define __riscv_vfmul_vf_f16m2(op1, op2, vl) __riscv_th_vfmul_vf_f16m2(op1, op2, vl)
#define __riscv_vfmul_vv_f16m4(op1, op2, vl) __riscv_th_vfmul_vv_f16m4(op1, op2, vl)
#define __riscv_vfmul_vf_f16m4(op1, op2, vl) __riscv_th_vfmul_vf_f16m4(op1, op2, vl)
#define __riscv_vfmul_vv_f16m8(op1, op2, vl) __riscv_th_vfmul_vv_f16m8(op1, op2, vl)
#define __riscv_vfmul_vf_f16m8(op1, op2, vl) __riscv_th_vfmul_vf_f16m8(op1, op2, vl)
#define __riscv_vfmul_vv_f32m1(op1, op2, vl) __riscv_th_vfmul_vv_f32m1(op1, op2, vl)
#define __riscv_vfmul_vf_f32m1(op1, op2, vl) __riscv_th_vfmul_vf_f32m1(op1, op2, vl)
#define __riscv_vfmul_vv_f32m2(op1, op2, vl) __riscv_th_vfmul_vv_f32m2(op1, op2, vl)
#define __riscv_vfmul_vf_f32m2(op1, op2, vl) __riscv_th_vfmul_vf_f32m2(op1, op2, vl)
#define __riscv_vfmul_vv_f32m4(op1, op2, vl) __riscv_th_vfmul_vv_f32m4(op1, op2, vl)
#define __riscv_vfmul_vf_f32m4(op1, op2, vl) __riscv_th_vfmul_vf_f32m4(op1, op2, vl)
#define __riscv_vfmul_vv_f32m8(op1, op2, vl) __riscv_th_vfmul_vv_f32m8(op1, op2, vl)
#define __riscv_vfmul_vf_f32m8(op1, op2, vl) __riscv_th_vfmul_vf_f32m8(op1, op2, vl)
#define __riscv_vfmul_vv_f64m1(op1, op2, vl) __riscv_th_vfmul_vv_f64m1(op1, op2, vl)
#define __riscv_vfmul_vf_f64m1(op1, op2, vl) __riscv_th_vfmul_vf_f64m1(op1, op2, vl)
#define __riscv_vfmul_vv_f64m2(op1, op2, vl) __riscv_th_vfmul_vv_f64m2(op1, op2, vl)
#define __riscv_vfmul_vf_f64m2(op1, op2, vl) __riscv_th_vfmul_vf_f64m2(op1, op2, vl)
#define __riscv_vfmul_vv_f64m4(op1, op2, vl) __riscv_th_vfmul_vv_f64m4(op1, op2, vl)
#define __riscv_vfmul_vf_f64m4(op1, op2, vl) __riscv_th_vfmul_vf_f64m4(op1, op2, vl)
#define __riscv_vfmul_vv_f64m8(op1, op2, vl) __riscv_th_vfmul_vv_f64m8(op1, op2, vl)
#define __riscv_vfmul_vf_f64m8(op1, op2, vl) __riscv_th_vfmul_vf_f64m8(op1, op2, vl)
#define __riscv_vfmul_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmul_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmul_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m1(op1, op2, vl) __riscv_th_vfrdiv_vf_f16m1(op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m2(op1, op2, vl) __riscv_th_vfrdiv_vf_f16m2(op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m4(op1, op2, vl) __riscv_th_vfrdiv_vf_f16m4(op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m8(op1, op2, vl) __riscv_th_vfrdiv_vf_f16m8(op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m1(op1, op2, vl) __riscv_th_vfrdiv_vf_f32m1(op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m2(op1, op2, vl) __riscv_th_vfrdiv_vf_f32m2(op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m4(op1, op2, vl) __riscv_th_vfrdiv_vf_f32m4(op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m8(op1, op2, vl) __riscv_th_vfrdiv_vf_f32m8(op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m1(op1, op2, vl) __riscv_th_vfrdiv_vf_f64m1(op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m2(op1, op2, vl) __riscv_th_vfrdiv_vf_f64m2(op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m4(op1, op2, vl) __riscv_th_vfrdiv_vf_f64m4(op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m8(op1, op2, vl) __riscv_th_vfrdiv_vf_f64m8(op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrdiv_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrdiv_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f16m1(op1, op2, vl) __riscv_th_vfrsub_vf_f16m1(op1, op2, vl)
#define __riscv_vfrsub_vf_f16m2(op1, op2, vl) __riscv_th_vfrsub_vf_f16m2(op1, op2, vl)
#define __riscv_vfrsub_vf_f16m4(op1, op2, vl) __riscv_th_vfrsub_vf_f16m4(op1, op2, vl)
#define __riscv_vfrsub_vf_f16m8(op1, op2, vl) __riscv_th_vfrsub_vf_f16m8(op1, op2, vl)
#define __riscv_vfrsub_vf_f32m1(op1, op2, vl) __riscv_th_vfrsub_vf_f32m1(op1, op2, vl)
#define __riscv_vfrsub_vf_f32m2(op1, op2, vl) __riscv_th_vfrsub_vf_f32m2(op1, op2, vl)
#define __riscv_vfrsub_vf_f32m4(op1, op2, vl) __riscv_th_vfrsub_vf_f32m4(op1, op2, vl)
#define __riscv_vfrsub_vf_f32m8(op1, op2, vl) __riscv_th_vfrsub_vf_f32m8(op1, op2, vl)
#define __riscv_vfrsub_vf_f64m1(op1, op2, vl) __riscv_th_vfrsub_vf_f64m1(op1, op2, vl)
#define __riscv_vfrsub_vf_f64m2(op1, op2, vl) __riscv_th_vfrsub_vf_f64m2(op1, op2, vl)
#define __riscv_vfrsub_vf_f64m4(op1, op2, vl) __riscv_th_vfrsub_vf_f64m4(op1, op2, vl)
#define __riscv_vfrsub_vf_f64m8(op1, op2, vl) __riscv_th_vfrsub_vf_f64m8(op1, op2, vl)
#define __riscv_vfrsub_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfrsub_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfrsub_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f16m1(op1, op2, vl) __riscv_th_vfsub_vv_f16m1(op1, op2, vl)
#define __riscv_vfsub_vf_f16m1(op1, op2, vl) __riscv_th_vfsub_vf_f16m1(op1, op2, vl)
#define __riscv_vfsub_vv_f16m2(op1, op2, vl) __riscv_th_vfsub_vv_f16m2(op1, op2, vl)
#define __riscv_vfsub_vf_f16m2(op1, op2, vl) __riscv_th_vfsub_vf_f16m2(op1, op2, vl)
#define __riscv_vfsub_vv_f16m4(op1, op2, vl) __riscv_th_vfsub_vv_f16m4(op1, op2, vl)
#define __riscv_vfsub_vf_f16m4(op1, op2, vl) __riscv_th_vfsub_vf_f16m4(op1, op2, vl)
#define __riscv_vfsub_vv_f16m8(op1, op2, vl) __riscv_th_vfsub_vv_f16m8(op1, op2, vl)
#define __riscv_vfsub_vf_f16m8(op1, op2, vl) __riscv_th_vfsub_vf_f16m8(op1, op2, vl)
#define __riscv_vfsub_vv_f32m1(op1, op2, vl) __riscv_th_vfsub_vv_f32m1(op1, op2, vl)
#define __riscv_vfsub_vf_f32m1(op1, op2, vl) __riscv_th_vfsub_vf_f32m1(op1, op2, vl)
#define __riscv_vfsub_vv_f32m2(op1, op2, vl) __riscv_th_vfsub_vv_f32m2(op1, op2, vl)
#define __riscv_vfsub_vf_f32m2(op1, op2, vl) __riscv_th_vfsub_vf_f32m2(op1, op2, vl)
#define __riscv_vfsub_vv_f32m4(op1, op2, vl) __riscv_th_vfsub_vv_f32m4(op1, op2, vl)
#define __riscv_vfsub_vf_f32m4(op1, op2, vl) __riscv_th_vfsub_vf_f32m4(op1, op2, vl)
#define __riscv_vfsub_vv_f32m8(op1, op2, vl) __riscv_th_vfsub_vv_f32m8(op1, op2, vl)
#define __riscv_vfsub_vf_f32m8(op1, op2, vl) __riscv_th_vfsub_vf_f32m8(op1, op2, vl)
#define __riscv_vfsub_vv_f64m1(op1, op2, vl) __riscv_th_vfsub_vv_f64m1(op1, op2, vl)
#define __riscv_vfsub_vf_f64m1(op1, op2, vl) __riscv_th_vfsub_vf_f64m1(op1, op2, vl)
#define __riscv_vfsub_vv_f64m2(op1, op2, vl) __riscv_th_vfsub_vv_f64m2(op1, op2, vl)
#define __riscv_vfsub_vf_f64m2(op1, op2, vl) __riscv_th_vfsub_vf_f64m2(op1, op2, vl)
#define __riscv_vfsub_vv_f64m4(op1, op2, vl) __riscv_th_vfsub_vv_f64m4(op1, op2, vl)
#define __riscv_vfsub_vf_f64m4(op1, op2, vl) __riscv_th_vfsub_vf_f64m4(op1, op2, vl)
#define __riscv_vfsub_vv_f64m8(op1, op2, vl) __riscv_th_vfsub_vv_f64m8(op1, op2, vl)
#define __riscv_vfsub_vf_f64m8(op1, op2, vl) __riscv_th_vfsub_vf_f64m8(op1, op2, vl)
#define __riscv_vfsub_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsub_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsub_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f32m1(op1, op2, vl) __riscv_th_vfwadd_wf_f32m1(op1, op2, vl)
#define __riscv_vfwadd_vv_f32m2(op1, op2, vl) __riscv_th_vfwadd_vv_f32m2(op1, op2, vl)
#define __riscv_vfwadd_vf_f32m2(op1, op2, vl) __riscv_th_vfwadd_vf_f32m2(op1, op2, vl)
#define __riscv_vfwadd_wv_f32m2(op1, op2, vl) __riscv_th_vfwadd_wv_f32m2(op1, op2, vl)
#define __riscv_vfwadd_wf_f32m2(op1, op2, vl) __riscv_th_vfwadd_wf_f32m2(op1, op2, vl)
#define __riscv_vfwadd_vv_f32m4(op1, op2, vl) __riscv_th_vfwadd_vv_f32m4(op1, op2, vl)
#define __riscv_vfwadd_vf_f32m4(op1, op2, vl) __riscv_th_vfwadd_vf_f32m4(op1, op2, vl)
#define __riscv_vfwadd_wv_f32m4(op1, op2, vl) __riscv_th_vfwadd_wv_f32m4(op1, op2, vl)
#define __riscv_vfwadd_wf_f32m4(op1, op2, vl) __riscv_th_vfwadd_wf_f32m4(op1, op2, vl)
#define __riscv_vfwadd_vv_f32m8(op1, op2, vl) __riscv_th_vfwadd_vv_f32m8(op1, op2, vl)
#define __riscv_vfwadd_vf_f32m8(op1, op2, vl) __riscv_th_vfwadd_vf_f32m8(op1, op2, vl)
#define __riscv_vfwadd_wv_f32m8(op1, op2, vl) __riscv_th_vfwadd_wv_f32m8(op1, op2, vl)
#define __riscv_vfwadd_wf_f32m8(op1, op2, vl) __riscv_th_vfwadd_wf_f32m8(op1, op2, vl)
#define __riscv_vfwadd_wf_f64m1(op1, op2, vl) __riscv_th_vfwadd_wf_f64m1(op1, op2, vl)
#define __riscv_vfwadd_vv_f64m2(op1, op2, vl) __riscv_th_vfwadd_vv_f64m2(op1, op2, vl)
#define __riscv_vfwadd_vf_f64m2(op1, op2, vl) __riscv_th_vfwadd_vf_f64m2(op1, op2, vl)
#define __riscv_vfwadd_wv_f64m2(op1, op2, vl) __riscv_th_vfwadd_wv_f64m2(op1, op2, vl)
#define __riscv_vfwadd_wf_f64m2(op1, op2, vl) __riscv_th_vfwadd_wf_f64m2(op1, op2, vl)
#define __riscv_vfwadd_vv_f64m4(op1, op2, vl) __riscv_th_vfwadd_vv_f64m4(op1, op2, vl)
#define __riscv_vfwadd_vf_f64m4(op1, op2, vl) __riscv_th_vfwadd_vf_f64m4(op1, op2, vl)
#define __riscv_vfwadd_wv_f64m4(op1, op2, vl) __riscv_th_vfwadd_wv_f64m4(op1, op2, vl)
#define __riscv_vfwadd_wf_f64m4(op1, op2, vl) __riscv_th_vfwadd_wf_f64m4(op1, op2, vl)
#define __riscv_vfwadd_vv_f64m8(op1, op2, vl) __riscv_th_vfwadd_vv_f64m8(op1, op2, vl)
#define __riscv_vfwadd_vf_f64m8(op1, op2, vl) __riscv_th_vfwadd_vf_f64m8(op1, op2, vl)
#define __riscv_vfwadd_wv_f64m8(op1, op2, vl) __riscv_th_vfwadd_wv_f64m8(op1, op2, vl)
#define __riscv_vfwadd_wf_f64m8(op1, op2, vl) __riscv_th_vfwadd_wf_f64m8(op1, op2, vl)
#define __riscv_vfwadd_wf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwadd_wf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwadd_wf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f32m1(op1, op2, vl) __riscv_th_vfwsub_wf_f32m1(op1, op2, vl)
#define __riscv_vfwsub_vv_f32m2(op1, op2, vl) __riscv_th_vfwsub_vv_f32m2(op1, op2, vl)
#define __riscv_vfwsub_vf_f32m2(op1, op2, vl) __riscv_th_vfwsub_vf_f32m2(op1, op2, vl)
#define __riscv_vfwsub_wv_f32m2(op1, op2, vl) __riscv_th_vfwsub_wv_f32m2(op1, op2, vl)
#define __riscv_vfwsub_wf_f32m2(op1, op2, vl) __riscv_th_vfwsub_wf_f32m2(op1, op2, vl)
#define __riscv_vfwsub_vv_f32m4(op1, op2, vl) __riscv_th_vfwsub_vv_f32m4(op1, op2, vl)
#define __riscv_vfwsub_vf_f32m4(op1, op2, vl) __riscv_th_vfwsub_vf_f32m4(op1, op2, vl)
#define __riscv_vfwsub_wv_f32m4(op1, op2, vl) __riscv_th_vfwsub_wv_f32m4(op1, op2, vl)
#define __riscv_vfwsub_wf_f32m4(op1, op2, vl) __riscv_th_vfwsub_wf_f32m4(op1, op2, vl)
#define __riscv_vfwsub_vv_f32m8(op1, op2, vl) __riscv_th_vfwsub_vv_f32m8(op1, op2, vl)
#define __riscv_vfwsub_vf_f32m8(op1, op2, vl) __riscv_th_vfwsub_vf_f32m8(op1, op2, vl)
#define __riscv_vfwsub_wv_f32m8(op1, op2, vl) __riscv_th_vfwsub_wv_f32m8(op1, op2, vl)
#define __riscv_vfwsub_wf_f32m8(op1, op2, vl) __riscv_th_vfwsub_wf_f32m8(op1, op2, vl)
#define __riscv_vfwsub_wf_f64m1(op1, op2, vl) __riscv_th_vfwsub_wf_f64m1(op1, op2, vl)
#define __riscv_vfwsub_vv_f64m2(op1, op2, vl) __riscv_th_vfwsub_vv_f64m2(op1, op2, vl)
#define __riscv_vfwsub_vf_f64m2(op1, op2, vl) __riscv_th_vfwsub_vf_f64m2(op1, op2, vl)
#define __riscv_vfwsub_wv_f64m2(op1, op2, vl) __riscv_th_vfwsub_wv_f64m2(op1, op2, vl)
#define __riscv_vfwsub_wf_f64m2(op1, op2, vl) __riscv_th_vfwsub_wf_f64m2(op1, op2, vl)
#define __riscv_vfwsub_vv_f64m4(op1, op2, vl) __riscv_th_vfwsub_vv_f64m4(op1, op2, vl)
#define __riscv_vfwsub_vf_f64m4(op1, op2, vl) __riscv_th_vfwsub_vf_f64m4(op1, op2, vl)
#define __riscv_vfwsub_wv_f64m4(op1, op2, vl) __riscv_th_vfwsub_wv_f64m4(op1, op2, vl)
#define __riscv_vfwsub_wf_f64m4(op1, op2, vl) __riscv_th_vfwsub_wf_f64m4(op1, op2, vl)
#define __riscv_vfwsub_vv_f64m8(op1, op2, vl) __riscv_th_vfwsub_vv_f64m8(op1, op2, vl)
#define __riscv_vfwsub_vf_f64m8(op1, op2, vl) __riscv_th_vfwsub_vf_f64m8(op1, op2, vl)
#define __riscv_vfwsub_wv_f64m8(op1, op2, vl) __riscv_th_vfwsub_wv_f64m8(op1, op2, vl)
#define __riscv_vfwsub_wf_f64m8(op1, op2, vl) __riscv_th_vfwsub_wf_f64m8(op1, op2, vl)
#define __riscv_vfwsub_wf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwsub_wf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwsub_wf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vv_f32m2(op1, op2, vl) __riscv_th_vfwmul_vv_f32m2(op1, op2, vl)
#define __riscv_vfwmul_vf_f32m2(op1, op2, vl) __riscv_th_vfwmul_vf_f32m2(op1, op2, vl)
#define __riscv_vfwmul_vv_f32m4(op1, op2, vl) __riscv_th_vfwmul_vv_f32m4(op1, op2, vl)
#define __riscv_vfwmul_vf_f32m4(op1, op2, vl) __riscv_th_vfwmul_vf_f32m4(op1, op2, vl)
#define __riscv_vfwmul_vv_f32m8(op1, op2, vl) __riscv_th_vfwmul_vv_f32m8(op1, op2, vl)
#define __riscv_vfwmul_vf_f32m8(op1, op2, vl) __riscv_th_vfwmul_vf_f32m8(op1, op2, vl)
#define __riscv_vfwmul_vv_f64m2(op1, op2, vl) __riscv_th_vfwmul_vv_f64m2(op1, op2, vl)
#define __riscv_vfwmul_vf_f64m2(op1, op2, vl) __riscv_th_vfwmul_vf_f64m2(op1, op2, vl)
#define __riscv_vfwmul_vv_f64m4(op1, op2, vl) __riscv_th_vfwmul_vv_f64m4(op1, op2, vl)
#define __riscv_vfwmul_vf_f64m4(op1, op2, vl) __riscv_th_vfwmul_vf_f64m4(op1, op2, vl)
#define __riscv_vfwmul_vv_f64m8(op1, op2, vl) __riscv_th_vfwmul_vv_f64m8(op1, op2, vl)
#define __riscv_vfwmul_vf_f64m8(op1, op2, vl) __riscv_th_vfwmul_vf_f64m8(op1, op2, vl)
#define __riscv_vfwmul_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfwmul_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfwmul_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vfmacc_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmacc_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmacc_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmadd_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmadd_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmadd_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmadd_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsac_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsac_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfmsub_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfmsub_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfmsub_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfmsub_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmacc_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmacc_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmadd_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmadd_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmadd_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmadd_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsac_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsac_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m1(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m1(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m1(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m1(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m2(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m2(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m2(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m2(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m4(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m4(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m4(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m4(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m8(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m8(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m8(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m8(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m1(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m1(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m1(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m1(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m2(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m2(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m4(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m4(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m8(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m8(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m1(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m1(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m1(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m1(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m2(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m2(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m4(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m4(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m8(vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m8(vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f16m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f16m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f16m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f16m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f32m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f32m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m1_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m1_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m1_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m1_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m2_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m2_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m4_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m4_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfnmsub_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfnmsub_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfnmsub_vf_f64m8_m(mask, vd, rs1, vs2, vl) __riscv_th_vfnmsub_vf_f64m8_m(mask, vd, rs1, vs2, vl)
#define __riscv_vfwmacc_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmacc_vf_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmacc_vf_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwmsac_vf_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwmsac_vf_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmacc_vf_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmacc_vf_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f32m2(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f32m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f32m4(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f32m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f32m8(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f32m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f64m2(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f64m2(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f64m4(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f64m4(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f64m8(vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f64m8(vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f32m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f32m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f32m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f32m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f32m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f32m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f64m2_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f64m2_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f64m4_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f64m4_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vv_f64m8_m(mask, vd, vs1, vs2, vl)
#define __riscv_vfwnmsac_vf_f64m8_m(mask, vd, vs1, vs2, vl) __riscv_th_vfwnmsac_vf_f64m8_m(mask, vd, vs1, vs2, vl)

#define __riscv_vfabs_v_f16m1(op1, vl) __riscv_th_vfabs_v_f16m1(op1, vl)
#define __riscv_vfabs_v_f16m2(op1, vl) __riscv_th_vfabs_v_f16m2(op1, vl)
#define __riscv_vfabs_v_f16m4(op1, vl) __riscv_th_vfabs_v_f16m4(op1, vl)
#define __riscv_vfabs_v_f16m8(op1, vl) __riscv_th_vfabs_v_f16m8(op1, vl)
#define __riscv_vfabs_v_f32m1(op1, vl) __riscv_th_vfabs_v_f32m1(op1, vl)
#define __riscv_vfabs_v_f32m2(op1, vl) __riscv_th_vfabs_v_f32m2(op1, vl)
#define __riscv_vfabs_v_f32m4(op1, vl) __riscv_th_vfabs_v_f32m4(op1, vl)
#define __riscv_vfabs_v_f32m8(op1, vl) __riscv_th_vfabs_v_f32m8(op1, vl)
#define __riscv_vfabs_v_f64m1(op1, vl) __riscv_th_vfabs_v_f64m1(op1, vl)
#define __riscv_vfabs_v_f64m2(op1, vl) __riscv_th_vfabs_v_f64m2(op1, vl)
#define __riscv_vfabs_v_f64m4(op1, vl) __riscv_th_vfabs_v_f64m4(op1, vl)
#define __riscv_vfabs_v_f64m8(op1, vl) __riscv_th_vfabs_v_f64m8(op1, vl)
#define __riscv_vfabs_v_f16m1_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f16m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f16m2_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f16m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f16m4_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f16m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f16m8_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f16m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f32m1_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f32m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f32m2_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f32m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f32m4_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f32m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f32m8_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f32m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f64m1_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f64m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f64m2_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f64m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f64m4_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f64m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfabs_v_f64m8_m(mask, maskedoff, op1, vl) __riscv_th_vfabs_v_f64m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfmax_vv_f16m1(op1, op2, vl) __riscv_th_vfmax_vv_f16m1(op1, op2, vl)
#define __riscv_vfmax_vf_f16m1(op1, op2, vl) __riscv_th_vfmax_vf_f16m1(op1, op2, vl)
#define __riscv_vfmax_vv_f16m2(op1, op2, vl) __riscv_th_vfmax_vv_f16m2(op1, op2, vl)
#define __riscv_vfmax_vf_f16m2(op1, op2, vl) __riscv_th_vfmax_vf_f16m2(op1, op2, vl)
#define __riscv_vfmax_vv_f16m4(op1, op2, vl) __riscv_th_vfmax_vv_f16m4(op1, op2, vl)
#define __riscv_vfmax_vf_f16m4(op1, op2, vl) __riscv_th_vfmax_vf_f16m4(op1, op2, vl)
#define __riscv_vfmax_vv_f16m8(op1, op2, vl) __riscv_th_vfmax_vv_f16m8(op1, op2, vl)
#define __riscv_vfmax_vf_f16m8(op1, op2, vl) __riscv_th_vfmax_vf_f16m8(op1, op2, vl)
#define __riscv_vfmax_vv_f32m1(op1, op2, vl) __riscv_th_vfmax_vv_f32m1(op1, op2, vl)
#define __riscv_vfmax_vf_f32m1(op1, op2, vl) __riscv_th_vfmax_vf_f32m1(op1, op2, vl)
#define __riscv_vfmax_vv_f32m2(op1, op2, vl) __riscv_th_vfmax_vv_f32m2(op1, op2, vl)
#define __riscv_vfmax_vf_f32m2(op1, op2, vl) __riscv_th_vfmax_vf_f32m2(op1, op2, vl)
#define __riscv_vfmax_vv_f32m4(op1, op2, vl) __riscv_th_vfmax_vv_f32m4(op1, op2, vl)
#define __riscv_vfmax_vf_f32m4(op1, op2, vl) __riscv_th_vfmax_vf_f32m4(op1, op2, vl)
#define __riscv_vfmax_vv_f32m8(op1, op2, vl) __riscv_th_vfmax_vv_f32m8(op1, op2, vl)
#define __riscv_vfmax_vf_f32m8(op1, op2, vl) __riscv_th_vfmax_vf_f32m8(op1, op2, vl)
#define __riscv_vfmax_vv_f64m1(op1, op2, vl) __riscv_th_vfmax_vv_f64m1(op1, op2, vl)
#define __riscv_vfmax_vf_f64m1(op1, op2, vl) __riscv_th_vfmax_vf_f64m1(op1, op2, vl)
#define __riscv_vfmax_vv_f64m2(op1, op2, vl) __riscv_th_vfmax_vv_f64m2(op1, op2, vl)
#define __riscv_vfmax_vf_f64m2(op1, op2, vl) __riscv_th_vfmax_vf_f64m2(op1, op2, vl)
#define __riscv_vfmax_vv_f64m4(op1, op2, vl) __riscv_th_vfmax_vv_f64m4(op1, op2, vl)
#define __riscv_vfmax_vf_f64m4(op1, op2, vl) __riscv_th_vfmax_vf_f64m4(op1, op2, vl)
#define __riscv_vfmax_vv_f64m8(op1, op2, vl) __riscv_th_vfmax_vv_f64m8(op1, op2, vl)
#define __riscv_vfmax_vf_f64m8(op1, op2, vl) __riscv_th_vfmax_vf_f64m8(op1, op2, vl)
#define __riscv_vfmax_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmax_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmax_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f16m1(op1, op2, vl) __riscv_th_vfmin_vv_f16m1(op1, op2, vl)
#define __riscv_vfmin_vf_f16m1(op1, op2, vl) __riscv_th_vfmin_vf_f16m1(op1, op2, vl)
#define __riscv_vfmin_vv_f16m2(op1, op2, vl) __riscv_th_vfmin_vv_f16m2(op1, op2, vl)
#define __riscv_vfmin_vf_f16m2(op1, op2, vl) __riscv_th_vfmin_vf_f16m2(op1, op2, vl)
#define __riscv_vfmin_vv_f16m4(op1, op2, vl) __riscv_th_vfmin_vv_f16m4(op1, op2, vl)
#define __riscv_vfmin_vf_f16m4(op1, op2, vl) __riscv_th_vfmin_vf_f16m4(op1, op2, vl)
#define __riscv_vfmin_vv_f16m8(op1, op2, vl) __riscv_th_vfmin_vv_f16m8(op1, op2, vl)
#define __riscv_vfmin_vf_f16m8(op1, op2, vl) __riscv_th_vfmin_vf_f16m8(op1, op2, vl)
#define __riscv_vfmin_vv_f32m1(op1, op2, vl) __riscv_th_vfmin_vv_f32m1(op1, op2, vl)
#define __riscv_vfmin_vf_f32m1(op1, op2, vl) __riscv_th_vfmin_vf_f32m1(op1, op2, vl)
#define __riscv_vfmin_vv_f32m2(op1, op2, vl) __riscv_th_vfmin_vv_f32m2(op1, op2, vl)
#define __riscv_vfmin_vf_f32m2(op1, op2, vl) __riscv_th_vfmin_vf_f32m2(op1, op2, vl)
#define __riscv_vfmin_vv_f32m4(op1, op2, vl) __riscv_th_vfmin_vv_f32m4(op1, op2, vl)
#define __riscv_vfmin_vf_f32m4(op1, op2, vl) __riscv_th_vfmin_vf_f32m4(op1, op2, vl)
#define __riscv_vfmin_vv_f32m8(op1, op2, vl) __riscv_th_vfmin_vv_f32m8(op1, op2, vl)
#define __riscv_vfmin_vf_f32m8(op1, op2, vl) __riscv_th_vfmin_vf_f32m8(op1, op2, vl)
#define __riscv_vfmin_vv_f64m1(op1, op2, vl) __riscv_th_vfmin_vv_f64m1(op1, op2, vl)
#define __riscv_vfmin_vf_f64m1(op1, op2, vl) __riscv_th_vfmin_vf_f64m1(op1, op2, vl)
#define __riscv_vfmin_vv_f64m2(op1, op2, vl) __riscv_th_vfmin_vv_f64m2(op1, op2, vl)
#define __riscv_vfmin_vf_f64m2(op1, op2, vl) __riscv_th_vfmin_vf_f64m2(op1, op2, vl)
#define __riscv_vfmin_vv_f64m4(op1, op2, vl) __riscv_th_vfmin_vv_f64m4(op1, op2, vl)
#define __riscv_vfmin_vf_f64m4(op1, op2, vl) __riscv_th_vfmin_vf_f64m4(op1, op2, vl)
#define __riscv_vfmin_vv_f64m8(op1, op2, vl) __riscv_th_vfmin_vv_f64m8(op1, op2, vl)
#define __riscv_vfmin_vf_f64m8(op1, op2, vl) __riscv_th_vfmin_vf_f64m8(op1, op2, vl)
#define __riscv_vfmin_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfmin_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfmin_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m1(op1, op2, vl) __riscv_th_vfsgnj_vv_f16m1(op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m1(op1, op2, vl) __riscv_th_vfsgnj_vf_f16m1(op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m2(op1, op2, vl) __riscv_th_vfsgnj_vv_f16m2(op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m2(op1, op2, vl) __riscv_th_vfsgnj_vf_f16m2(op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m4(op1, op2, vl) __riscv_th_vfsgnj_vv_f16m4(op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m4(op1, op2, vl) __riscv_th_vfsgnj_vf_f16m4(op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m8(op1, op2, vl) __riscv_th_vfsgnj_vv_f16m8(op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m8(op1, op2, vl) __riscv_th_vfsgnj_vf_f16m8(op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m1(op1, op2, vl) __riscv_th_vfsgnj_vv_f32m1(op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m1(op1, op2, vl) __riscv_th_vfsgnj_vf_f32m1(op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m2(op1, op2, vl) __riscv_th_vfsgnj_vv_f32m2(op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m2(op1, op2, vl) __riscv_th_vfsgnj_vf_f32m2(op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m4(op1, op2, vl) __riscv_th_vfsgnj_vv_f32m4(op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m4(op1, op2, vl) __riscv_th_vfsgnj_vf_f32m4(op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m8(op1, op2, vl) __riscv_th_vfsgnj_vv_f32m8(op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m8(op1, op2, vl) __riscv_th_vfsgnj_vf_f32m8(op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m1(op1, op2, vl) __riscv_th_vfsgnj_vv_f64m1(op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m1(op1, op2, vl) __riscv_th_vfsgnj_vf_f64m1(op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m2(op1, op2, vl) __riscv_th_vfsgnj_vv_f64m2(op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m2(op1, op2, vl) __riscv_th_vfsgnj_vf_f64m2(op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m4(op1, op2, vl) __riscv_th_vfsgnj_vv_f64m4(op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m4(op1, op2, vl) __riscv_th_vfsgnj_vf_f64m4(op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m8(op1, op2, vl) __riscv_th_vfsgnj_vv_f64m8(op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m8(op1, op2, vl) __riscv_th_vfsgnj_vf_f64m8(op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnj_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnj_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m1(op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m1(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m1(op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m1(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m2(op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m2(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m2(op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m2(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m4(op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m4(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m4(op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m4(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m8(op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m8(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m8(op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m8(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m1(op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m1(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m1(op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m1(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m2(op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m2(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m2(op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m2(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m4(op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m4(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m4(op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m4(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m8(op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m8(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m8(op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m8(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m1(op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m1(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m1(op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m1(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m2(op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m2(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m2(op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m2(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m4(op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m4(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m4(op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m4(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m8(op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m8(op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m8(op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m8(op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjn_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjn_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m1(op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m1(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m1(op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m1(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m2(op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m2(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m2(op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m2(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m4(op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m4(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m4(op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m4(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m8(op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m8(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m8(op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m8(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m1(op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m1(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m1(op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m1(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m2(op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m2(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m2(op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m2(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m4(op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m4(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m4(op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m4(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m8(op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m8(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m8(op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m8(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m1(op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m1(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m1(op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m1(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m2(op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m2(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m2(op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m2(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m4(op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m4(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m4(op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m4(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m8(op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m8(op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m8(op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m8(op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f16m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f16m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f32m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f32m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m1_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m1_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vv_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vv_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfsgnjx_vf_f64m8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vfsgnjx_vf_f64m8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vfneg_v_f16m1(op1, vl) __riscv_th_vfneg_v_f16m1(op1, vl)
#define __riscv_vfneg_v_f16m2(op1, vl) __riscv_th_vfneg_v_f16m2(op1, vl)
#define __riscv_vfneg_v_f16m4(op1, vl) __riscv_th_vfneg_v_f16m4(op1, vl)
#define __riscv_vfneg_v_f16m8(op1, vl) __riscv_th_vfneg_v_f16m8(op1, vl)
#define __riscv_vfneg_v_f32m1(op1, vl) __riscv_th_vfneg_v_f32m1(op1, vl)
#define __riscv_vfneg_v_f32m2(op1, vl) __riscv_th_vfneg_v_f32m2(op1, vl)
#define __riscv_vfneg_v_f32m4(op1, vl) __riscv_th_vfneg_v_f32m4(op1, vl)
#define __riscv_vfneg_v_f32m8(op1, vl) __riscv_th_vfneg_v_f32m8(op1, vl)
#define __riscv_vfneg_v_f64m1(op1, vl) __riscv_th_vfneg_v_f64m1(op1, vl)
#define __riscv_vfneg_v_f64m2(op1, vl) __riscv_th_vfneg_v_f64m2(op1, vl)
#define __riscv_vfneg_v_f64m4(op1, vl) __riscv_th_vfneg_v_f64m4(op1, vl)
#define __riscv_vfneg_v_f64m8(op1, vl) __riscv_th_vfneg_v_f64m8(op1, vl)
#define __riscv_vfneg_v_f16m1_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f16m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f16m2_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f16m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f16m4_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f16m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f16m8_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f16m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f32m1_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f32m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f32m2_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f32m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f32m4_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f32m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f32m8_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f32m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f64m1_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f64m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f64m2_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f64m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f64m4_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f64m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfneg_v_f64m8_m(mask, maskedoff, op1, vl) __riscv_th_vfneg_v_f64m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f16m1(op1, vl) __riscv_th_vfsqrt_v_f16m1(op1, vl)
#define __riscv_vfsqrt_v_f16m2(op1, vl) __riscv_th_vfsqrt_v_f16m2(op1, vl)
#define __riscv_vfsqrt_v_f16m4(op1, vl) __riscv_th_vfsqrt_v_f16m4(op1, vl)
#define __riscv_vfsqrt_v_f16m8(op1, vl) __riscv_th_vfsqrt_v_f16m8(op1, vl)
#define __riscv_vfsqrt_v_f32m1(op1, vl) __riscv_th_vfsqrt_v_f32m1(op1, vl)
#define __riscv_vfsqrt_v_f32m2(op1, vl) __riscv_th_vfsqrt_v_f32m2(op1, vl)
#define __riscv_vfsqrt_v_f32m4(op1, vl) __riscv_th_vfsqrt_v_f32m4(op1, vl)
#define __riscv_vfsqrt_v_f32m8(op1, vl) __riscv_th_vfsqrt_v_f32m8(op1, vl)
#define __riscv_vfsqrt_v_f64m1(op1, vl) __riscv_th_vfsqrt_v_f64m1(op1, vl)
#define __riscv_vfsqrt_v_f64m2(op1, vl) __riscv_th_vfsqrt_v_f64m2(op1, vl)
#define __riscv_vfsqrt_v_f64m4(op1, vl) __riscv_th_vfsqrt_v_f64m4(op1, vl)
#define __riscv_vfsqrt_v_f64m8(op1, vl) __riscv_th_vfsqrt_v_f64m8(op1, vl)
#define __riscv_vfsqrt_v_f16m1_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f16m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f16m2_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f16m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f16m4_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f16m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f16m8_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f16m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f32m1_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f32m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f32m2_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f32m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f32m4_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f32m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f32m8_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f32m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f64m1_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f64m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f64m2_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f64m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f64m4_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f64m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfsqrt_v_f64m8_m(mask, maskedoff, op1, vl) __riscv_th_vfsqrt_v_f64m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u16m1(op1, vl) __riscv_th_vfclass_v_u16m1(op1, vl)
#define __riscv_vfclass_v_u16m2(op1, vl) __riscv_th_vfclass_v_u16m2(op1, vl)
#define __riscv_vfclass_v_u16m4(op1, vl) __riscv_th_vfclass_v_u16m4(op1, vl)
#define __riscv_vfclass_v_u16m8(op1, vl) __riscv_th_vfclass_v_u16m8(op1, vl)
#define __riscv_vfclass_v_u32m1(op1, vl) __riscv_th_vfclass_v_u32m1(op1, vl)
#define __riscv_vfclass_v_u32m2(op1, vl) __riscv_th_vfclass_v_u32m2(op1, vl)
#define __riscv_vfclass_v_u32m4(op1, vl) __riscv_th_vfclass_v_u32m4(op1, vl)
#define __riscv_vfclass_v_u32m8(op1, vl) __riscv_th_vfclass_v_u32m8(op1, vl)
#define __riscv_vfclass_v_u64m1(op1, vl) __riscv_th_vfclass_v_u64m1(op1, vl)
#define __riscv_vfclass_v_u64m2(op1, vl) __riscv_th_vfclass_v_u64m2(op1, vl)
#define __riscv_vfclass_v_u64m4(op1, vl) __riscv_th_vfclass_v_u64m4(op1, vl)
#define __riscv_vfclass_v_u64m8(op1, vl) __riscv_th_vfclass_v_u64m8(op1, vl)
#define __riscv_vfclass_v_u16m1_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u16m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u16m2_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u16m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u16m4_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u16m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u16m8_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u16m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u32m1_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u32m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u32m2_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u32m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u32m4_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u32m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u32m8_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u32m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u64m1_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u64m1_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u64m2_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u64m2_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u64m4_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u64m4_mu(mask, maskedoff, op1, vl)
#define __riscv_vfclass_v_u64m8_m(mask, maskedoff, op1, vl) __riscv_th_vfclass_v_u64m8_mu(mask, maskedoff, op1, vl)
#define __riscv_vfmerge_vfm_f16m1(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f16m1(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f16m2(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f16m2(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f16m4(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f16m4(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f16m8(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f16m8(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f32m1(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f32m1(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f32m2(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f32m2(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f32m4(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f32m4(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f32m8(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f32m8(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f64m1(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f64m1(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f64m2(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f64m2(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f64m4(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f64m4(op1, op2, mask, vl)
#define __riscv_vfmerge_vfm_f64m8(op1, op2, mask, vl) __riscv_th_vfmerge_vfm_f64m8(op1, op2, mask, vl)
#define __riscv_vfmv_v_f_f16m1(src, vl) __riscv_th_vfmv_v_f_f16m1(src, vl)
#define __riscv_vfmv_v_f_f16m2(src, vl) __riscv_th_vfmv_v_f_f16m2(src, vl)
#define __riscv_vfmv_v_f_f16m4(src, vl) __riscv_th_vfmv_v_f_f16m4(src, vl)
#define __riscv_vfmv_v_f_f16m8(src, vl) __riscv_th_vfmv_v_f_f16m8(src, vl)
#define __riscv_vfmv_v_f_f32m1(src, vl) __riscv_th_vfmv_v_f_f32m1(src, vl)
#define __riscv_vfmv_v_f_f32m2(src, vl) __riscv_th_vfmv_v_f_f32m2(src, vl)
#define __riscv_vfmv_v_f_f32m4(src, vl) __riscv_th_vfmv_v_f_f32m4(src, vl)
#define __riscv_vfmv_v_f_f32m8(src, vl) __riscv_th_vfmv_v_f_f32m8(src, vl)
#define __riscv_vfmv_v_f_f64m1(src, vl) __riscv_th_vfmv_v_f_f64m1(src, vl)
#define __riscv_vfmv_v_f_f64m2(src, vl) __riscv_th_vfmv_v_f_f64m2(src, vl)
#define __riscv_vfmv_v_f_f64m4(src, vl) __riscv_th_vfmv_v_f_f64m4(src, vl)
#define __riscv_vfmv_v_f_f64m8(src, vl) __riscv_th_vfmv_v_f_f64m8(src, vl)
#define __riscv_vmfeq_vv_f16m1_b16(op1, op2, vl) __riscv_th_vmfeq_vv_f16m1_b16(op1, op2, vl)
#define __riscv_vmfeq_vf_f16m1_b16(op1, op2, vl) __riscv_th_vmfeq_vf_f16m1_b16(op1, op2, vl)
#define __riscv_vmfeq_vv_f16m2_b8(op1, op2, vl) __riscv_th_vmfeq_vv_f16m2_b8(op1, op2, vl)
#define __riscv_vmfeq_vf_f16m2_b8(op1, op2, vl) __riscv_th_vmfeq_vf_f16m2_b8(op1, op2, vl)
#define __riscv_vmfeq_vv_f16m4_b4(op1, op2, vl) __riscv_th_vmfeq_vv_f16m4_b4(op1, op2, vl)
#define __riscv_vmfeq_vf_f16m4_b4(op1, op2, vl) __riscv_th_vmfeq_vf_f16m4_b4(op1, op2, vl)
#define __riscv_vmfeq_vv_f16m8_b2(op1, op2, vl) __riscv_th_vmfeq_vv_f16m8_b2(op1, op2, vl)
#define __riscv_vmfeq_vf_f16m8_b2(op1, op2, vl) __riscv_th_vmfeq_vf_f16m8_b2(op1, op2, vl)
#define __riscv_vmfeq_vv_f32m1_b32(op1, op2, vl) __riscv_th_vmfeq_vv_f32m1_b32(op1, op2, vl)
#define __riscv_vmfeq_vf_f32m1_b32(op1, op2, vl) __riscv_th_vmfeq_vf_f32m1_b32(op1, op2, vl)
#define __riscv_vmfeq_vv_f32m2_b16(op1, op2, vl) __riscv_th_vmfeq_vv_f32m2_b16(op1, op2, vl)
#define __riscv_vmfeq_vf_f32m2_b16(op1, op2, vl) __riscv_th_vmfeq_vf_f32m2_b16(op1, op2, vl)
#define __riscv_vmfeq_vv_f32m4_b8(op1, op2, vl) __riscv_th_vmfeq_vv_f32m4_b8(op1, op2, vl)
#define __riscv_vmfeq_vf_f32m4_b8(op1, op2, vl) __riscv_th_vmfeq_vf_f32m4_b8(op1, op2, vl)
#define __riscv_vmfeq_vv_f32m8_b4(op1, op2, vl) __riscv_th_vmfeq_vv_f32m8_b4(op1, op2, vl)
#define __riscv_vmfeq_vf_f32m8_b4(op1, op2, vl) __riscv_th_vmfeq_vf_f32m8_b4(op1, op2, vl)
#define __riscv_vmfeq_vv_f64m1_b64(op1, op2, vl) __riscv_th_vmfeq_vv_f64m1_b64(op1, op2, vl)
#define __riscv_vmfeq_vf_f64m1_b64(op1, op2, vl) __riscv_th_vmfeq_vf_f64m1_b64(op1, op2, vl)
#define __riscv_vmfeq_vv_f64m2_b32(op1, op2, vl) __riscv_th_vmfeq_vv_f64m2_b32(op1, op2, vl)
#define __riscv_vmfeq_vf_f64m2_b32(op1, op2, vl) __riscv_th_vmfeq_vf_f64m2_b32(op1, op2, vl)
#define __riscv_vmfeq_vv_f64m4_b16(op1, op2, vl) __riscv_th_vmfeq_vv_f64m4_b16(op1, op2, vl)
#define __riscv_vmfeq_vf_f64m4_b16(op1, op2, vl) __riscv_th_vmfeq_vf_f64m4_b16(op1, op2, vl)
#define __riscv_vmfeq_vv_f64m8_b8(op1, op2, vl) __riscv_th_vmfeq_vv_f64m8_b8(op1, op2, vl)
#define __riscv_vmfeq_vf_f64m8_b8(op1, op2, vl) __riscv_th_vmfeq_vf_f64m8_b8(op1, op2, vl)
#define __riscv_vmfeq_vv_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vv_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vv_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfeq_vf_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfeq_vf_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f16m1_b16(op1, op2, vl) __riscv_th_vmfne_vv_f16m1_b16(op1, op2, vl)
#define __riscv_vmfne_vf_f16m1_b16(op1, op2, vl) __riscv_th_vmfne_vf_f16m1_b16(op1, op2, vl)
#define __riscv_vmfne_vv_f16m2_b8(op1, op2, vl) __riscv_th_vmfne_vv_f16m2_b8(op1, op2, vl)
#define __riscv_vmfne_vf_f16m2_b8(op1, op2, vl) __riscv_th_vmfne_vf_f16m2_b8(op1, op2, vl)
#define __riscv_vmfne_vv_f16m4_b4(op1, op2, vl) __riscv_th_vmfne_vv_f16m4_b4(op1, op2, vl)
#define __riscv_vmfne_vf_f16m4_b4(op1, op2, vl) __riscv_th_vmfne_vf_f16m4_b4(op1, op2, vl)
#define __riscv_vmfne_vv_f16m8_b2(op1, op2, vl) __riscv_th_vmfne_vv_f16m8_b2(op1, op2, vl)
#define __riscv_vmfne_vf_f16m8_b2(op1, op2, vl) __riscv_th_vmfne_vf_f16m8_b2(op1, op2, vl)
#define __riscv_vmfne_vv_f32m1_b32(op1, op2, vl) __riscv_th_vmfne_vv_f32m1_b32(op1, op2, vl)
#define __riscv_vmfne_vf_f32m1_b32(op1, op2, vl) __riscv_th_vmfne_vf_f32m1_b32(op1, op2, vl)
#define __riscv_vmfne_vv_f32m2_b16(op1, op2, vl) __riscv_th_vmfne_vv_f32m2_b16(op1, op2, vl)
#define __riscv_vmfne_vf_f32m2_b16(op1, op2, vl) __riscv_th_vmfne_vf_f32m2_b16(op1, op2, vl)
#define __riscv_vmfne_vv_f32m4_b8(op1, op2, vl) __riscv_th_vmfne_vv_f32m4_b8(op1, op2, vl)
#define __riscv_vmfne_vf_f32m4_b8(op1, op2, vl) __riscv_th_vmfne_vf_f32m4_b8(op1, op2, vl)
#define __riscv_vmfne_vv_f32m8_b4(op1, op2, vl) __riscv_th_vmfne_vv_f32m8_b4(op1, op2, vl)
#define __riscv_vmfne_vf_f32m8_b4(op1, op2, vl) __riscv_th_vmfne_vf_f32m8_b4(op1, op2, vl)
#define __riscv_vmfne_vv_f64m1_b64(op1, op2, vl) __riscv_th_vmfne_vv_f64m1_b64(op1, op2, vl)
#define __riscv_vmfne_vf_f64m1_b64(op1, op2, vl) __riscv_th_vmfne_vf_f64m1_b64(op1, op2, vl)
#define __riscv_vmfne_vv_f64m2_b32(op1, op2, vl) __riscv_th_vmfne_vv_f64m2_b32(op1, op2, vl)
#define __riscv_vmfne_vf_f64m2_b32(op1, op2, vl) __riscv_th_vmfne_vf_f64m2_b32(op1, op2, vl)
#define __riscv_vmfne_vv_f64m4_b16(op1, op2, vl) __riscv_th_vmfne_vv_f64m4_b16(op1, op2, vl)
#define __riscv_vmfne_vf_f64m4_b16(op1, op2, vl) __riscv_th_vmfne_vf_f64m4_b16(op1, op2, vl)
#define __riscv_vmfne_vv_f64m8_b8(op1, op2, vl) __riscv_th_vmfne_vv_f64m8_b8(op1, op2, vl)
#define __riscv_vmfne_vf_f64m8_b8(op1, op2, vl) __riscv_th_vmfne_vf_f64m8_b8(op1, op2, vl)
#define __riscv_vmfne_vv_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vv_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vv_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfne_vf_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfne_vf_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f16m1_b16(op1, op2, vl) __riscv_th_vmflt_vv_f16m1_b16(op1, op2, vl)
#define __riscv_vmflt_vf_f16m1_b16(op1, op2, vl) __riscv_th_vmflt_vf_f16m1_b16(op1, op2, vl)
#define __riscv_vmflt_vv_f16m2_b8(op1, op2, vl) __riscv_th_vmflt_vv_f16m2_b8(op1, op2, vl)
#define __riscv_vmflt_vf_f16m2_b8(op1, op2, vl) __riscv_th_vmflt_vf_f16m2_b8(op1, op2, vl)
#define __riscv_vmflt_vv_f16m4_b4(op1, op2, vl) __riscv_th_vmflt_vv_f16m4_b4(op1, op2, vl)
#define __riscv_vmflt_vf_f16m4_b4(op1, op2, vl) __riscv_th_vmflt_vf_f16m4_b4(op1, op2, vl)
#define __riscv_vmflt_vv_f16m8_b2(op1, op2, vl) __riscv_th_vmflt_vv_f16m8_b2(op1, op2, vl)
#define __riscv_vmflt_vf_f16m8_b2(op1, op2, vl) __riscv_th_vmflt_vf_f16m8_b2(op1, op2, vl)
#define __riscv_vmflt_vv_f32m1_b32(op1, op2, vl) __riscv_th_vmflt_vv_f32m1_b32(op1, op2, vl)
#define __riscv_vmflt_vf_f32m1_b32(op1, op2, vl) __riscv_th_vmflt_vf_f32m1_b32(op1, op2, vl)
#define __riscv_vmflt_vv_f32m2_b16(op1, op2, vl) __riscv_th_vmflt_vv_f32m2_b16(op1, op2, vl)
#define __riscv_vmflt_vf_f32m2_b16(op1, op2, vl) __riscv_th_vmflt_vf_f32m2_b16(op1, op2, vl)
#define __riscv_vmflt_vv_f32m4_b8(op1, op2, vl) __riscv_th_vmflt_vv_f32m4_b8(op1, op2, vl)
#define __riscv_vmflt_vf_f32m4_b8(op1, op2, vl) __riscv_th_vmflt_vf_f32m4_b8(op1, op2, vl)
#define __riscv_vmflt_vv_f32m8_b4(op1, op2, vl) __riscv_th_vmflt_vv_f32m8_b4(op1, op2, vl)
#define __riscv_vmflt_vf_f32m8_b4(op1, op2, vl) __riscv_th_vmflt_vf_f32m8_b4(op1, op2, vl)
#define __riscv_vmflt_vv_f64m1_b64(op1, op2, vl) __riscv_th_vmflt_vv_f64m1_b64(op1, op2, vl)
#define __riscv_vmflt_vf_f64m1_b64(op1, op2, vl) __riscv_th_vmflt_vf_f64m1_b64(op1, op2, vl)
#define __riscv_vmflt_vv_f64m2_b32(op1, op2, vl) __riscv_th_vmflt_vv_f64m2_b32(op1, op2, vl)
#define __riscv_vmflt_vf_f64m2_b32(op1, op2, vl) __riscv_th_vmflt_vf_f64m2_b32(op1, op2, vl)
#define __riscv_vmflt_vv_f64m4_b16(op1, op2, vl) __riscv_th_vmflt_vv_f64m4_b16(op1, op2, vl)
#define __riscv_vmflt_vf_f64m4_b16(op1, op2, vl) __riscv_th_vmflt_vf_f64m4_b16(op1, op2, vl)
#define __riscv_vmflt_vv_f64m8_b8(op1, op2, vl) __riscv_th_vmflt_vv_f64m8_b8(op1, op2, vl)
#define __riscv_vmflt_vf_f64m8_b8(op1, op2, vl) __riscv_th_vmflt_vf_f64m8_b8(op1, op2, vl)
#define __riscv_vmflt_vv_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vv_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vv_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmflt_vf_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmflt_vf_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f16m1_b16(op1, op2, vl) __riscv_th_vmfle_vv_f16m1_b16(op1, op2, vl)
#define __riscv_vmfle_vf_f16m1_b16(op1, op2, vl) __riscv_th_vmfle_vf_f16m1_b16(op1, op2, vl)
#define __riscv_vmfle_vv_f16m2_b8(op1, op2, vl) __riscv_th_vmfle_vv_f16m2_b8(op1, op2, vl)
#define __riscv_vmfle_vf_f16m2_b8(op1, op2, vl) __riscv_th_vmfle_vf_f16m2_b8(op1, op2, vl)
#define __riscv_vmfle_vv_f16m4_b4(op1, op2, vl) __riscv_th_vmfle_vv_f16m4_b4(op1, op2, vl)
#define __riscv_vmfle_vf_f16m4_b4(op1, op2, vl) __riscv_th_vmfle_vf_f16m4_b4(op1, op2, vl)
#define __riscv_vmfle_vv_f16m8_b2(op1, op2, vl) __riscv_th_vmfle_vv_f16m8_b2(op1, op2, vl)
#define __riscv_vmfle_vf_f16m8_b2(op1, op2, vl) __riscv_th_vmfle_vf_f16m8_b2(op1, op2, vl)
#define __riscv_vmfle_vv_f32m1_b32(op1, op2, vl) __riscv_th_vmfle_vv_f32m1_b32(op1, op2, vl)
#define __riscv_vmfle_vf_f32m1_b32(op1, op2, vl) __riscv_th_vmfle_vf_f32m1_b32(op1, op2, vl)
#define __riscv_vmfle_vv_f32m2_b16(op1, op2, vl) __riscv_th_vmfle_vv_f32m2_b16(op1, op2, vl)
#define __riscv_vmfle_vf_f32m2_b16(op1, op2, vl) __riscv_th_vmfle_vf_f32m2_b16(op1, op2, vl)
#define __riscv_vmfle_vv_f32m4_b8(op1, op2, vl) __riscv_th_vmfle_vv_f32m4_b8(op1, op2, vl)
#define __riscv_vmfle_vf_f32m4_b8(op1, op2, vl) __riscv_th_vmfle_vf_f32m4_b8(op1, op2, vl)
#define __riscv_vmfle_vv_f32m8_b4(op1, op2, vl) __riscv_th_vmfle_vv_f32m8_b4(op1, op2, vl)
#define __riscv_vmfle_vf_f32m8_b4(op1, op2, vl) __riscv_th_vmfle_vf_f32m8_b4(op1, op2, vl)
#define __riscv_vmfle_vv_f64m1_b64(op1, op2, vl) __riscv_th_vmfle_vv_f64m1_b64(op1, op2, vl)
#define __riscv_vmfle_vf_f64m1_b64(op1, op2, vl) __riscv_th_vmfle_vf_f64m1_b64(op1, op2, vl)
#define __riscv_vmfle_vv_f64m2_b32(op1, op2, vl) __riscv_th_vmfle_vv_f64m2_b32(op1, op2, vl)
#define __riscv_vmfle_vf_f64m2_b32(op1, op2, vl) __riscv_th_vmfle_vf_f64m2_b32(op1, op2, vl)
#define __riscv_vmfle_vv_f64m4_b16(op1, op2, vl) __riscv_th_vmfle_vv_f64m4_b16(op1, op2, vl)
#define __riscv_vmfle_vf_f64m4_b16(op1, op2, vl) __riscv_th_vmfle_vf_f64m4_b16(op1, op2, vl)
#define __riscv_vmfle_vv_f64m8_b8(op1, op2, vl) __riscv_th_vmfle_vv_f64m8_b8(op1, op2, vl)
#define __riscv_vmfle_vf_f64m8_b8(op1, op2, vl) __riscv_th_vmfle_vf_f64m8_b8(op1, op2, vl)
#define __riscv_vmfle_vv_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vv_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vv_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfle_vf_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfle_vf_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f16m1_b16(op1, op2, vl) __riscv_th_vmfgt_vv_f16m1_b16(op1, op2, vl)
#define __riscv_vmfgt_vf_f16m1_b16(op1, op2, vl) __riscv_th_vmfgt_vf_f16m1_b16(op1, op2, vl)
#define __riscv_vmfgt_vv_f16m2_b8(op1, op2, vl) __riscv_th_vmfgt_vv_f16m2_b8(op1, op2, vl)
#define __riscv_vmfgt_vf_f16m2_b8(op1, op2, vl) __riscv_th_vmfgt_vf_f16m2_b8(op1, op2, vl)
#define __riscv_vmfgt_vv_f16m4_b4(op1, op2, vl) __riscv_th_vmfgt_vv_f16m4_b4(op1, op2, vl)
#define __riscv_vmfgt_vf_f16m4_b4(op1, op2, vl) __riscv_th_vmfgt_vf_f16m4_b4(op1, op2, vl)
#define __riscv_vmfgt_vv_f16m8_b2(op1, op2, vl) __riscv_th_vmfgt_vv_f16m8_b2(op1, op2, vl)
#define __riscv_vmfgt_vf_f16m8_b2(op1, op2, vl) __riscv_th_vmfgt_vf_f16m8_b2(op1, op2, vl)
#define __riscv_vmfgt_vv_f32m1_b32(op1, op2, vl) __riscv_th_vmfgt_vv_f32m1_b32(op1, op2, vl)
#define __riscv_vmfgt_vf_f32m1_b32(op1, op2, vl) __riscv_th_vmfgt_vf_f32m1_b32(op1, op2, vl)
#define __riscv_vmfgt_vv_f32m2_b16(op1, op2, vl) __riscv_th_vmfgt_vv_f32m2_b16(op1, op2, vl)
#define __riscv_vmfgt_vf_f32m2_b16(op1, op2, vl) __riscv_th_vmfgt_vf_f32m2_b16(op1, op2, vl)
#define __riscv_vmfgt_vv_f32m4_b8(op1, op2, vl) __riscv_th_vmfgt_vv_f32m4_b8(op1, op2, vl)
#define __riscv_vmfgt_vf_f32m4_b8(op1, op2, vl) __riscv_th_vmfgt_vf_f32m4_b8(op1, op2, vl)
#define __riscv_vmfgt_vv_f32m8_b4(op1, op2, vl) __riscv_th_vmfgt_vv_f32m8_b4(op1, op2, vl)
#define __riscv_vmfgt_vf_f32m8_b4(op1, op2, vl) __riscv_th_vmfgt_vf_f32m8_b4(op1, op2, vl)
#define __riscv_vmfgt_vv_f64m1_b64(op1, op2, vl) __riscv_th_vmfgt_vv_f64m1_b64(op1, op2, vl)
#define __riscv_vmfgt_vf_f64m1_b64(op1, op2, vl) __riscv_th_vmfgt_vf_f64m1_b64(op1, op2, vl)
#define __riscv_vmfgt_vv_f64m2_b32(op1, op2, vl) __riscv_th_vmfgt_vv_f64m2_b32(op1, op2, vl)
#define __riscv_vmfgt_vf_f64m2_b32(op1, op2, vl) __riscv_th_vmfgt_vf_f64m2_b32(op1, op2, vl)
#define __riscv_vmfgt_vv_f64m4_b16(op1, op2, vl) __riscv_th_vmfgt_vv_f64m4_b16(op1, op2, vl)
#define __riscv_vmfgt_vf_f64m4_b16(op1, op2, vl) __riscv_th_vmfgt_vf_f64m4_b16(op1, op2, vl)
#define __riscv_vmfgt_vv_f64m8_b8(op1, op2, vl) __riscv_th_vmfgt_vv_f64m8_b8(op1, op2, vl)
#define __riscv_vmfgt_vf_f64m8_b8(op1, op2, vl) __riscv_th_vmfgt_vf_f64m8_b8(op1, op2, vl)
#define __riscv_vmfgt_vv_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vv_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vv_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfgt_vf_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfgt_vf_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f16m1_b16(op1, op2, vl) __riscv_th_vmfge_vv_f16m1_b16(op1, op2, vl)
#define __riscv_vmfge_vf_f16m1_b16(op1, op2, vl) __riscv_th_vmfge_vf_f16m1_b16(op1, op2, vl)
#define __riscv_vmfge_vv_f16m2_b8(op1, op2, vl) __riscv_th_vmfge_vv_f16m2_b8(op1, op2, vl)
#define __riscv_vmfge_vf_f16m2_b8(op1, op2, vl) __riscv_th_vmfge_vf_f16m2_b8(op1, op2, vl)
#define __riscv_vmfge_vv_f16m4_b4(op1, op2, vl) __riscv_th_vmfge_vv_f16m4_b4(op1, op2, vl)
#define __riscv_vmfge_vf_f16m4_b4(op1, op2, vl) __riscv_th_vmfge_vf_f16m4_b4(op1, op2, vl)
#define __riscv_vmfge_vv_f16m8_b2(op1, op2, vl) __riscv_th_vmfge_vv_f16m8_b2(op1, op2, vl)
#define __riscv_vmfge_vf_f16m8_b2(op1, op2, vl) __riscv_th_vmfge_vf_f16m8_b2(op1, op2, vl)
#define __riscv_vmfge_vv_f32m1_b32(op1, op2, vl) __riscv_th_vmfge_vv_f32m1_b32(op1, op2, vl)
#define __riscv_vmfge_vf_f32m1_b32(op1, op2, vl) __riscv_th_vmfge_vf_f32m1_b32(op1, op2, vl)
#define __riscv_vmfge_vv_f32m2_b16(op1, op2, vl) __riscv_th_vmfge_vv_f32m2_b16(op1, op2, vl)
#define __riscv_vmfge_vf_f32m2_b16(op1, op2, vl) __riscv_th_vmfge_vf_f32m2_b16(op1, op2, vl)
#define __riscv_vmfge_vv_f32m4_b8(op1, op2, vl) __riscv_th_vmfge_vv_f32m4_b8(op1, op2, vl)
#define __riscv_vmfge_vf_f32m4_b8(op1, op2, vl) __riscv_th_vmfge_vf_f32m4_b8(op1, op2, vl)
#define __riscv_vmfge_vv_f32m8_b4(op1, op2, vl) __riscv_th_vmfge_vv_f32m8_b4(op1, op2, vl)
#define __riscv_vmfge_vf_f32m8_b4(op1, op2, vl) __riscv_th_vmfge_vf_f32m8_b4(op1, op2, vl)
#define __riscv_vmfge_vv_f64m1_b64(op1, op2, vl) __riscv_th_vmfge_vv_f64m1_b64(op1, op2, vl)
#define __riscv_vmfge_vf_f64m1_b64(op1, op2, vl) __riscv_th_vmfge_vf_f64m1_b64(op1, op2, vl)
#define __riscv_vmfge_vv_f64m2_b32(op1, op2, vl) __riscv_th_vmfge_vv_f64m2_b32(op1, op2, vl)
#define __riscv_vmfge_vf_f64m2_b32(op1, op2, vl) __riscv_th_vmfge_vf_f64m2_b32(op1, op2, vl)
#define __riscv_vmfge_vv_f64m4_b16(op1, op2, vl) __riscv_th_vmfge_vv_f64m4_b16(op1, op2, vl)
#define __riscv_vmfge_vf_f64m4_b16(op1, op2, vl) __riscv_th_vmfge_vf_f64m4_b16(op1, op2, vl)
#define __riscv_vmfge_vv_f64m8_b8(op1, op2, vl) __riscv_th_vmfge_vv_f64m8_b8(op1, op2, vl)
#define __riscv_vmfge_vf_f64m8_b8(op1, op2, vl) __riscv_th_vmfge_vf_f64m8_b8(op1, op2, vl)
#define __riscv_vmfge_vv_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f16m1_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f16m1_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f16m2_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f16m2_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f16m4_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f16m4_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f16m8_b2_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f16m8_b2_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f32m1_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f32m1_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f32m2_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f32m2_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f32m4_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f32m4_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f32m8_b4_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f32m8_b4_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f64m1_b64_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f64m1_b64_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f64m2_b32_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f64m2_b32_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f64m4_b16_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f64m4_b16_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vv_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vv_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)
#define __riscv_vmfge_vf_f64m8_b8_m(mask, maskedoff, op1, op2, vl) __riscv_th_vmfge_vf_f64m8_b8_mu(mask, maskedoff, op1, op2, vl)

#define __riscv_vfcvt_x_f_v_i16m1(src, vl) __riscv_th_vfcvt_x_f_v_i16m1(src, vl)
#define __riscv_vfcvt_x_f_v_i16m2(src, vl) __riscv_th_vfcvt_x_f_v_i16m2(src, vl)
#define __riscv_vfcvt_x_f_v_i16m4(src, vl) __riscv_th_vfcvt_x_f_v_i16m4(src, vl)
#define __riscv_vfcvt_x_f_v_i16m8(src, vl) __riscv_th_vfcvt_x_f_v_i16m8(src, vl)
#define __riscv_vfcvt_xu_f_v_u16m1(src, vl) __riscv_th_vfcvt_xu_f_v_u16m1(src, vl)
#define __riscv_vfcvt_xu_f_v_u16m2(src, vl) __riscv_th_vfcvt_xu_f_v_u16m2(src, vl)
#define __riscv_vfcvt_xu_f_v_u16m4(src, vl) __riscv_th_vfcvt_xu_f_v_u16m4(src, vl)
#define __riscv_vfcvt_xu_f_v_u16m8(src, vl) __riscv_th_vfcvt_xu_f_v_u16m8(src, vl)
#define __riscv_vfcvt_f_x_v_f16m1(src, vl) __riscv_th_vfcvt_f_x_v_f16m1(src, vl)
#define __riscv_vfcvt_f_x_v_f16m2(src, vl) __riscv_th_vfcvt_f_x_v_f16m2(src, vl)
#define __riscv_vfcvt_f_x_v_f16m4(src, vl) __riscv_th_vfcvt_f_x_v_f16m4(src, vl)
#define __riscv_vfcvt_f_x_v_f16m8(src, vl) __riscv_th_vfcvt_f_x_v_f16m8(src, vl)
#define __riscv_vfcvt_f_xu_v_f16m1(src, vl) __riscv_th_vfcvt_f_xu_v_f16m1(src, vl)
#define __riscv_vfcvt_f_xu_v_f16m2(src, vl) __riscv_th_vfcvt_f_xu_v_f16m2(src, vl)
#define __riscv_vfcvt_f_xu_v_f16m4(src, vl) __riscv_th_vfcvt_f_xu_v_f16m4(src, vl)
#define __riscv_vfcvt_f_xu_v_f16m8(src, vl) __riscv_th_vfcvt_f_xu_v_f16m8(src, vl)
#define __riscv_vfcvt_x_f_v_i32m1(src, vl) __riscv_th_vfcvt_x_f_v_i32m1(src, vl)
#define __riscv_vfcvt_x_f_v_i32m2(src, vl) __riscv_th_vfcvt_x_f_v_i32m2(src, vl)
#define __riscv_vfcvt_x_f_v_i32m4(src, vl) __riscv_th_vfcvt_x_f_v_i32m4(src, vl)
#define __riscv_vfcvt_x_f_v_i32m8(src, vl) __riscv_th_vfcvt_x_f_v_i32m8(src, vl)
#define __riscv_vfcvt_xu_f_v_u32m1(src, vl) __riscv_th_vfcvt_xu_f_v_u32m1(src, vl)
#define __riscv_vfcvt_xu_f_v_u32m2(src, vl) __riscv_th_vfcvt_xu_f_v_u32m2(src, vl)
#define __riscv_vfcvt_xu_f_v_u32m4(src, vl) __riscv_th_vfcvt_xu_f_v_u32m4(src, vl)
#define __riscv_vfcvt_xu_f_v_u32m8(src, vl) __riscv_th_vfcvt_xu_f_v_u32m8(src, vl)
#define __riscv_vfcvt_f_x_v_f32m1(src, vl) __riscv_th_vfcvt_f_x_v_f32m1(src, vl)
#define __riscv_vfcvt_f_x_v_f32m2(src, vl) __riscv_th_vfcvt_f_x_v_f32m2(src, vl)
#define __riscv_vfcvt_f_x_v_f32m4(src, vl) __riscv_th_vfcvt_f_x_v_f32m4(src, vl)
#define __riscv_vfcvt_f_x_v_f32m8(src, vl) __riscv_th_vfcvt_f_x_v_f32m8(src, vl)
#define __riscv_vfcvt_f_xu_v_f32m1(src, vl) __riscv_th_vfcvt_f_xu_v_f32m1(src, vl)
#define __riscv_vfcvt_f_xu_v_f32m2(src, vl) __riscv_th_vfcvt_f_xu_v_f32m2(src, vl)
#define __riscv_vfcvt_f_xu_v_f32m4(src, vl) __riscv_th_vfcvt_f_xu_v_f32m4(src, vl)
#define __riscv_vfcvt_f_xu_v_f32m8(src, vl) __riscv_th_vfcvt_f_xu_v_f32m8(src, vl)
#define __riscv_vfcvt_x_f_v_i64m1(src, vl) __riscv_th_vfcvt_x_f_v_i64m1(src, vl)
#define __riscv_vfcvt_x_f_v_i64m2(src, vl) __riscv_th_vfcvt_x_f_v_i64m2(src, vl)
#define __riscv_vfcvt_x_f_v_i64m4(src, vl) __riscv_th_vfcvt_x_f_v_i64m4(src, vl)
#define __riscv_vfcvt_x_f_v_i64m8(src, vl) __riscv_th_vfcvt_x_f_v_i64m8(src, vl)
#define __riscv_vfcvt_xu_f_v_u64m1(src, vl) __riscv_th_vfcvt_xu_f_v_u64m1(src, vl)
#define __riscv_vfcvt_xu_f_v_u64m2(src, vl) __riscv_th_vfcvt_xu_f_v_u64m2(src, vl)
#define __riscv_vfcvt_xu_f_v_u64m4(src, vl) __riscv_th_vfcvt_xu_f_v_u64m4(src, vl)
#define __riscv_vfcvt_xu_f_v_u64m8(src, vl) __riscv_th_vfcvt_xu_f_v_u64m8(src, vl)
#define __riscv_vfcvt_f_x_v_f64m1(src, vl) __riscv_th_vfcvt_f_x_v_f64m1(src, vl)
#define __riscv_vfcvt_f_x_v_f64m2(src, vl) __riscv_th_vfcvt_f_x_v_f64m2(src, vl)
#define __riscv_vfcvt_f_x_v_f64m4(src, vl) __riscv_th_vfcvt_f_x_v_f64m4(src, vl)
#define __riscv_vfcvt_f_x_v_f64m8(src, vl) __riscv_th_vfcvt_f_x_v_f64m8(src, vl)
#define __riscv_vfcvt_f_xu_v_f64m1(src, vl) __riscv_th_vfcvt_f_xu_v_f64m1(src, vl)
#define __riscv_vfcvt_f_xu_v_f64m2(src, vl) __riscv_th_vfcvt_f_xu_v_f64m2(src, vl)
#define __riscv_vfcvt_f_xu_v_f64m4(src, vl) __riscv_th_vfcvt_f_xu_v_f64m4(src, vl)
#define __riscv_vfcvt_f_xu_v_f64m8(src, vl) __riscv_th_vfcvt_f_xu_v_f64m8(src, vl)
#define __riscv_vfcvt_x_f_v_i16m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i16m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i16m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i16m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i16m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u16m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u16m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u16m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u16m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u16m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f16m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f16m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f16m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f16m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f16m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f16m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i32m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i32m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i32m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i32m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u32m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u32m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u32m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u32m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f32m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f32m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f32m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f32m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i64m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i64m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i64m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i64m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_x_f_v_i64m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_x_f_v_i64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u64m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u64m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u64m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u64m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_xu_f_v_u64m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_xu_f_v_u64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f64m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f64m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f64m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f64m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_x_v_f64m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_x_v_f64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f64m1_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f64m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f64m2_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f64m4_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfcvt_f_xu_v_f64m8_m(mask, maskedoff, src, vl) __riscv_th_vfcvt_f_xu_v_f64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i8m1(src, vl) __riscv_th_vfncvt_x_f_w_i8m1(src, vl)
#define __riscv_vfncvt_x_f_w_i8m2(src, vl) __riscv_th_vfncvt_x_f_w_i8m2(src, vl)
#define __riscv_vfncvt_x_f_w_i8m4(src, vl) __riscv_th_vfncvt_x_f_w_i8m4(src, vl)
#define __riscv_vfncvt_xu_f_w_u8m1(src, vl) __riscv_th_vfncvt_xu_f_w_u8m1(src, vl)
#define __riscv_vfncvt_xu_f_w_u8m2(src, vl) __riscv_th_vfncvt_xu_f_w_u8m2(src, vl)
#define __riscv_vfncvt_xu_f_w_u8m4(src, vl) __riscv_th_vfncvt_xu_f_w_u8m4(src, vl)
#define __riscv_vfncvt_x_f_w_i16m1(src, vl) __riscv_th_vfncvt_x_f_w_i16m1(src, vl)
#define __riscv_vfncvt_x_f_w_i16m2(src, vl) __riscv_th_vfncvt_x_f_w_i16m2(src, vl)
#define __riscv_vfncvt_x_f_w_i16m4(src, vl) __riscv_th_vfncvt_x_f_w_i16m4(src, vl)
#define __riscv_vfncvt_xu_f_w_u16m1(src, vl) __riscv_th_vfncvt_xu_f_w_u16m1(src, vl)
#define __riscv_vfncvt_xu_f_w_u16m2(src, vl) __riscv_th_vfncvt_xu_f_w_u16m2(src, vl)
#define __riscv_vfncvt_xu_f_w_u16m4(src, vl) __riscv_th_vfncvt_xu_f_w_u16m4(src, vl)
#define __riscv_vfncvt_f_x_w_f16m1(src, vl) __riscv_th_vfncvt_f_x_w_f16m1(src, vl)
#define __riscv_vfncvt_f_x_w_f16m2(src, vl) __riscv_th_vfncvt_f_x_w_f16m2(src, vl)
#define __riscv_vfncvt_f_x_w_f16m4(src, vl) __riscv_th_vfncvt_f_x_w_f16m4(src, vl)
#define __riscv_vfncvt_f_xu_w_f16m1(src, vl) __riscv_th_vfncvt_f_xu_w_f16m1(src, vl)
#define __riscv_vfncvt_f_xu_w_f16m2(src, vl) __riscv_th_vfncvt_f_xu_w_f16m2(src, vl)
#define __riscv_vfncvt_f_xu_w_f16m4(src, vl) __riscv_th_vfncvt_f_xu_w_f16m4(src, vl)
#define __riscv_vfncvt_f_f_w_f16m1(src, vl) __riscv_th_vfncvt_f_f_w_f16m1(src, vl)
#define __riscv_vfncvt_f_f_w_f16m2(src, vl) __riscv_th_vfncvt_f_f_w_f16m2(src, vl)
#define __riscv_vfncvt_f_f_w_f16m4(src, vl) __riscv_th_vfncvt_f_f_w_f16m4(src, vl)
#define __riscv_vfncvt_x_f_w_i32m1(src, vl) __riscv_th_vfncvt_x_f_w_i32m1(src, vl)
#define __riscv_vfncvt_x_f_w_i32m2(src, vl) __riscv_th_vfncvt_x_f_w_i32m2(src, vl)
#define __riscv_vfncvt_x_f_w_i32m4(src, vl) __riscv_th_vfncvt_x_f_w_i32m4(src, vl)
#define __riscv_vfncvt_xu_f_w_u32m1(src, vl) __riscv_th_vfncvt_xu_f_w_u32m1(src, vl)
#define __riscv_vfncvt_xu_f_w_u32m2(src, vl) __riscv_th_vfncvt_xu_f_w_u32m2(src, vl)
#define __riscv_vfncvt_xu_f_w_u32m4(src, vl) __riscv_th_vfncvt_xu_f_w_u32m4(src, vl)
#define __riscv_vfncvt_f_x_w_f32m1(src, vl) __riscv_th_vfncvt_f_x_w_f32m1(src, vl)
#define __riscv_vfncvt_f_x_w_f32m2(src, vl) __riscv_th_vfncvt_f_x_w_f32m2(src, vl)
#define __riscv_vfncvt_f_x_w_f32m4(src, vl) __riscv_th_vfncvt_f_x_w_f32m4(src, vl)
#define __riscv_vfncvt_f_xu_w_f32m1(src, vl) __riscv_th_vfncvt_f_xu_w_f32m1(src, vl)
#define __riscv_vfncvt_f_xu_w_f32m2(src, vl) __riscv_th_vfncvt_f_xu_w_f32m2(src, vl)
#define __riscv_vfncvt_f_xu_w_f32m4(src, vl) __riscv_th_vfncvt_f_xu_w_f32m4(src, vl)
#define __riscv_vfncvt_f_f_w_f32m1(src, vl) __riscv_th_vfncvt_f_f_w_f32m1(src, vl)
#define __riscv_vfncvt_f_f_w_f32m2(src, vl) __riscv_th_vfncvt_f_f_w_f32m2(src, vl)
#define __riscv_vfncvt_f_f_w_f32m4(src, vl) __riscv_th_vfncvt_f_f_w_f32m4(src, vl)
#define __riscv_vfncvt_x_f_w_i8m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i8m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i8m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i8m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i8m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i8m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u8m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u8m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u8m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u8m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u8m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u8m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i16m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i16m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i16m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u16m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u16m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u16m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_x_w_f16m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_x_w_f16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_x_w_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_x_w_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_x_w_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_x_w_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_xu_w_f16m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_xu_w_f16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_xu_w_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_xu_w_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_xu_w_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_xu_w_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_f_w_f16m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_f_w_f16m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_f_w_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_f_w_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_f_w_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_f_w_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i32m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i32m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_x_f_w_i32m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_x_f_w_i32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u32m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u32m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_xu_f_w_u32m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_xu_f_w_u32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_x_w_f32m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_x_w_f32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_x_w_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_x_w_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_x_w_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_x_w_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_xu_w_f32m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_xu_w_f32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_xu_w_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_xu_w_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_xu_w_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_xu_w_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_f_w_f32m1_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_f_w_f32m1_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_f_w_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_f_w_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfncvt_f_f_w_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfncvt_f_f_w_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f16m2(src, vl) __riscv_th_vfwcvt_f_x_v_f16m2(src, vl)
#define __riscv_vfwcvt_f_x_v_f16m4(src, vl) __riscv_th_vfwcvt_f_x_v_f16m4(src, vl)
#define __riscv_vfwcvt_f_x_v_f16m8(src, vl) __riscv_th_vfwcvt_f_x_v_f16m8(src, vl)
#define __riscv_vfwcvt_f_xu_v_f16m2(src, vl) __riscv_th_vfwcvt_f_xu_v_f16m2(src, vl)
#define __riscv_vfwcvt_f_xu_v_f16m4(src, vl) __riscv_th_vfwcvt_f_xu_v_f16m4(src, vl)
#define __riscv_vfwcvt_f_xu_v_f16m8(src, vl) __riscv_th_vfwcvt_f_xu_v_f16m8(src, vl)
#define __riscv_vfwcvt_f_x_v_f32m2(src, vl) __riscv_th_vfwcvt_f_x_v_f32m2(src, vl)
#define __riscv_vfwcvt_f_x_v_f32m4(src, vl) __riscv_th_vfwcvt_f_x_v_f32m4(src, vl)
#define __riscv_vfwcvt_f_x_v_f32m8(src, vl) __riscv_th_vfwcvt_f_x_v_f32m8(src, vl)
#define __riscv_vfwcvt_f_xu_v_f32m2(src, vl) __riscv_th_vfwcvt_f_xu_v_f32m2(src, vl)
#define __riscv_vfwcvt_f_xu_v_f32m4(src, vl) __riscv_th_vfwcvt_f_xu_v_f32m4(src, vl)
#define __riscv_vfwcvt_f_xu_v_f32m8(src, vl) __riscv_th_vfwcvt_f_xu_v_f32m8(src, vl)
#define __riscv_vfwcvt_f_f_v_f32m2(src, vl) __riscv_th_vfwcvt_f_f_v_f32m2(src, vl)
#define __riscv_vfwcvt_f_f_v_f32m4(src, vl) __riscv_th_vfwcvt_f_f_v_f32m4(src, vl)
#define __riscv_vfwcvt_f_f_v_f32m8(src, vl) __riscv_th_vfwcvt_f_f_v_f32m8(src, vl)
#define __riscv_vfwcvt_f_x_v_f64m2(src, vl) __riscv_th_vfwcvt_f_x_v_f64m2(src, vl)
#define __riscv_vfwcvt_f_x_v_f64m4(src, vl) __riscv_th_vfwcvt_f_x_v_f64m4(src, vl)
#define __riscv_vfwcvt_f_x_v_f64m8(src, vl) __riscv_th_vfwcvt_f_x_v_f64m8(src, vl)
#define __riscv_vfwcvt_f_xu_v_f64m2(src, vl) __riscv_th_vfwcvt_f_xu_v_f64m2(src, vl)
#define __riscv_vfwcvt_f_xu_v_f64m4(src, vl) __riscv_th_vfwcvt_f_xu_v_f64m4(src, vl)
#define __riscv_vfwcvt_f_xu_v_f64m8(src, vl) __riscv_th_vfwcvt_f_xu_v_f64m8(src, vl)
#define __riscv_vfwcvt_f_f_v_f64m2(src, vl) __riscv_th_vfwcvt_f_f_v_f64m2(src, vl)
#define __riscv_vfwcvt_f_f_v_f64m4(src, vl) __riscv_th_vfwcvt_f_f_v_f64m4(src, vl)
#define __riscv_vfwcvt_f_f_v_f64m8(src, vl) __riscv_th_vfwcvt_f_f_v_f64m8(src, vl)
#define __riscv_vfwcvt_f_x_v_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f16m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f16m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f16m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f16m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f16m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f16m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f16m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f16m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f32m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f32m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_f_v_f32m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_f_v_f32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_f_v_f32m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_f_v_f32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_f_v_f32m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_f_v_f32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f64m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f64m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_x_v_f64m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_x_v_f64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f64m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f64m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_xu_v_f64m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_xu_v_f64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_f_v_f64m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_f_v_f64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_f_v_f64m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_f_v_f64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_f_f_v_f64m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_f_f_v_f64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_x_f_v_i32m2(src, vl) __riscv_th_vfwcvt_x_f_v_i32m2(src, vl)
#define __riscv_vfwcvt_x_f_v_i32m4(src, vl) __riscv_th_vfwcvt_x_f_v_i32m4(src, vl)
#define __riscv_vfwcvt_x_f_v_i32m8(src, vl) __riscv_th_vfwcvt_x_f_v_i32m8(src, vl)
#define __riscv_vfwcvt_xu_f_v_u32m2(src, vl) __riscv_th_vfwcvt_xu_f_v_u32m2(src, vl)
#define __riscv_vfwcvt_xu_f_v_u32m4(src, vl) __riscv_th_vfwcvt_xu_f_v_u32m4(src, vl)
#define __riscv_vfwcvt_xu_f_v_u32m8(src, vl) __riscv_th_vfwcvt_xu_f_v_u32m8(src, vl)
#define __riscv_vfwcvt_x_f_v_i64m2(src, vl) __riscv_th_vfwcvt_x_f_v_i64m2(src, vl)
#define __riscv_vfwcvt_x_f_v_i64m4(src, vl) __riscv_th_vfwcvt_x_f_v_i64m4(src, vl)
#define __riscv_vfwcvt_x_f_v_i64m8(src, vl) __riscv_th_vfwcvt_x_f_v_i64m8(src, vl)
#define __riscv_vfwcvt_xu_f_v_u64m2(src, vl) __riscv_th_vfwcvt_xu_f_v_u64m2(src, vl)
#define __riscv_vfwcvt_xu_f_v_u64m4(src, vl) __riscv_th_vfwcvt_xu_f_v_u64m4(src, vl)
#define __riscv_vfwcvt_xu_f_v_u64m8(src, vl) __riscv_th_vfwcvt_xu_f_v_u64m8(src, vl)
#define __riscv_vfwcvt_x_f_v_i32m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_x_f_v_i32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_x_f_v_i32m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_x_f_v_i32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_x_f_v_i32m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_x_f_v_i32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_xu_f_v_u32m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_xu_f_v_u32m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_xu_f_v_u32m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_xu_f_v_u32m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_xu_f_v_u32m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_xu_f_v_u32m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_x_f_v_i64m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_x_f_v_i64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_x_f_v_i64m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_x_f_v_i64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_x_f_v_i64m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_x_f_v_i64m8_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_xu_f_v_u64m2_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_xu_f_v_u64m2_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_xu_f_v_u64m4_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_xu_f_v_u64m4_mu(mask, maskedoff, src, vl)
#define __riscv_vfwcvt_xu_f_v_u64m8_m(mask, maskedoff, src, vl) __riscv_th_vfwcvt_xu_f_v_u64m8_mu(mask, maskedoff, src, vl)
}] in
def th_vector_floating_point_operations_wrapper_macros: RVVHeader;

// 15. Vector Reduction Operations

let HeaderCode =
[{
// Vector Reduction Operations
#define __riscv_vfredmax_vs_f16m1_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m1_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m2_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m2_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m4_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m4_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m8_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m8_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m1_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m2_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m4_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m8_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m1_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m2_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m4_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m8_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m1_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m1_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m2_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m2_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m4_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m4_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f16m8_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f16m8_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f32m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f32m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmax_vs_f64m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmax_vs_f64m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m1_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m1_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m2_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m2_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m4_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m4_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m8_f16m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m8_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m1_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m2_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m4_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m8_f32m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m1_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m2_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m4_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m8_f64m1(dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m1_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m1_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m2_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m2_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m4_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m4_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f16m8_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f16m8_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f32m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f32m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredmin_vs_f64m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredmin_vs_f64m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m1_f16m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m1_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m2_f16m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m2_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m4_f16m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m4_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m8_f16m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m8_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m1_f32m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m2_f32m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m4_f32m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m8_f32m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m1_f64m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m2_f64m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m4_f64m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m8_f64m1(dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m1_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m1_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m2_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m2_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m4_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m4_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f16m8_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f16m8_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f32m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f32m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredosum_vs_f64m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredosum_vs_f64m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m1_f16m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m1_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m2_f16m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m2_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m4_f16m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m4_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m8_f16m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m8_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m1_f32m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m2_f32m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m4_f32m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m8_f32m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m1_f64m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m2_f64m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m4_f64m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m8_f64m1(dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m1_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m1_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m2_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m2_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m4_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m4_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f16m8_f16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f16m8_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f32m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f32m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredsum_vs_f64m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfredsum_vs_f64m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m1_f16m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m1_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m2_f16m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m2_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m4_f16m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m4_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m8_f16m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m8_f16m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m1_f32m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m2_f32m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m4_f32m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m8_f32m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m1_f64m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m2_f64m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m4_f64m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m8_f64m1(dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m1_f16m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m1_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m2_f16m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m2_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m4_f16m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m4_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f16m8_f16m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f16m8_f16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f32m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f32m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfredusum_vs_f64m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_vfredsum_vs_f64m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m1_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m2_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m4_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m8_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m1_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m2_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m4_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m8_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f16m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f16m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredosum_vs_f32m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredosum_vs_f32m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m1_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m1_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m2_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m2_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m4_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m4_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m8_f32m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m8_f32m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m1_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m1_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m2_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m2_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m4_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m4_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m8_f64m1(dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m8_f64m1_tu(dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m1_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m1_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m2_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m2_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m4_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m4_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f16m8_f32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f16m8_f32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m1_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m1_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m2_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m2_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m4_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m4_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vfwredsum_vs_f32m8_f64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vfwredsum_vs_f32m8_f64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m1_i8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m1_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m2_i8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m2_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m4_i8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m4_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m8_i8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m8_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m1_i16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m2_i16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m4_i16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m8_i16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m1_i32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m2_i32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m4_i32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m8_i32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m1_i64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m2_i64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m4_i64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m8_i64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m1_u8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m1_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m2_u8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m2_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m4_u8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m4_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m8_u8m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m8_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m1_u16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m2_u16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m4_u16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m8_u16m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m1_u32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m2_u32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m4_u32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m8_u32m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m1_u64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m2_u64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m4_u64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m8_u64m1(dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m1_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m1_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m2_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m2_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m4_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m4_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i8m8_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i8m8_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i16m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i16m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i32m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i32m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_i64m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_i64m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m1_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m1_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m2_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m2_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m4_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m4_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u8m8_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u8m8_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u16m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u16m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u32m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u32m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredand_vs_u64m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredand_vs_u64m8_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m1_i8m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m1_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m2_i8m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m2_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m4_i8m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m4_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m8_i8m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m8_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m1_i16m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m2_i16m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m4_i16m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m8_i16m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m1_i32m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m2_i32m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m4_i32m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m8_i32m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m1_i64m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m2_i64m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m4_i64m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m8_i64m1(dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m1_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m1_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m2_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m2_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m4_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m4_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i8m8_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i8m8_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i16m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i16m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i32m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i32m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmax_vs_i64m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmax_vs_i64m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m1_u8m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m1_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m2_u8m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m2_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m4_u8m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m4_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m8_u8m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m8_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m1_u16m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m2_u16m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m4_u16m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m8_u16m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m1_u32m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m2_u32m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m4_u32m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m8_u32m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m1_u64m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m2_u64m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m4_u64m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m8_u64m1(dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m1_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m1_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m2_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m2_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m4_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m4_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u8m8_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u8m8_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u16m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u16m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u32m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u32m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmaxu_vs_u64m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmaxu_vs_u64m8_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m1_i8m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m1_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m2_i8m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m2_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m4_i8m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m4_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m8_i8m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m8_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m1_i16m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m2_i16m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m4_i16m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m8_i16m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m1_i32m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m2_i32m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m4_i32m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m8_i32m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m1_i64m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m2_i64m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m4_i64m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m8_i64m1(dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m1_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m1_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m2_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m2_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m4_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m4_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i8m8_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i8m8_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i16m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i16m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i32m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i32m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredmin_vs_i64m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredmin_vs_i64m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m1_u8m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m1_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m2_u8m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m2_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m4_u8m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m4_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m8_u8m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m8_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m1_u16m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m2_u16m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m4_u16m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m8_u16m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m1_u32m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m2_u32m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m4_u32m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m8_u32m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m1_u64m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m2_u64m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m4_u64m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m8_u64m1(dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m1_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m1_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m2_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m2_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m4_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m4_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u8m8_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u8m8_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u16m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u16m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u32m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u32m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredminu_vs_u64m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredminu_vs_u64m8_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m1_i8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m1_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m2_i8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m2_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m4_i8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m4_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m8_i8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m8_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m1_i16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m2_i16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m4_i16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m8_i16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m1_i32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m2_i32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m4_i32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m8_i32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m1_i64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m2_i64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m4_i64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m8_i64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m1_u8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m1_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m2_u8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m2_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m4_u8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m4_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m8_u8m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m8_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m1_u16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m2_u16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m4_u16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m8_u16m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m1_u32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m2_u32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m4_u32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m8_u32m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m1_u64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m2_u64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m4_u64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m8_u64m1(dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m1_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m1_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m2_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m2_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m4_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m4_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i8m8_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i8m8_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i16m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i16m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i32m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i32m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_i64m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_i64m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m1_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m1_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m2_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m2_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m4_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m4_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u8m8_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u8m8_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u16m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u16m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u32m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u32m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredor_vs_u64m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredor_vs_u64m8_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m1_i8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m1_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m2_i8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m2_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m4_i8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m4_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m8_i8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m8_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m1_i16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m2_i16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m4_i16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m8_i16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m1_i32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m2_i32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m4_i32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m8_i32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m1_i64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m2_i64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m4_i64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m8_i64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m1_u8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m1_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m2_u8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m2_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m4_u8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m4_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m8_u8m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m8_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m1_u16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m2_u16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m4_u16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m8_u16m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m1_u32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m2_u32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m4_u32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m8_u32m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m1_u64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m2_u64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m4_u64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m8_u64m1(dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m1_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m1_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m2_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m2_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m4_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m4_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i8m8_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i8m8_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i16m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i16m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i32m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i32m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_i64m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_i64m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m1_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m1_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m2_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m2_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m4_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m4_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u8m8_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u8m8_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u16m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u16m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u32m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u32m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredsum_vs_u64m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredsum_vs_u64m8_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m1_i8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m1_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m2_i8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m2_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m4_i8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m4_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m8_i8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m8_i8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m1_i16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m2_i16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m4_i16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m8_i16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m1_i32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m2_i32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m4_i32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m8_i32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m1_i64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m2_i64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m4_i64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m8_i64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m1_u8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m1_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m2_u8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m2_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m4_u8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m4_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m8_u8m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m8_u8m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m1_u16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m2_u16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m4_u16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m8_u16m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m1_u32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m2_u32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m4_u32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m8_u32m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m1_u64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m2_u64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m4_u64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m8_u64m1(dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m1_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m1_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m2_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m2_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m4_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m4_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i8m8_i8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i8m8_i8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i16m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i16m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i32m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i32m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_i64m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_i64m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m1_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m1_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m2_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m2_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m4_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m4_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u8m8_u8m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u8m8_u8m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u16m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u16m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u32m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u32m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vredxor_vs_u64m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vredxor_vs_u64m8_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m1_i16m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m1_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m2_i16m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m2_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m4_i16m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m4_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m8_i16m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m8_i16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m1_i32m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m1_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m2_i32m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m2_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m4_i32m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m4_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m8_i32m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m8_i32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m1_i64m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m1_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m2_i64m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m2_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m4_i64m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m4_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m8_i64m1(dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m8_i64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m1_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m1_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m2_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m2_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m4_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m4_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i8m8_i16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i8m8_i16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m1_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m1_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m2_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m2_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m4_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m4_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i16m8_i32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i16m8_i32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m1_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m1_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m2_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m2_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m4_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m4_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsum_vs_i32m8_i64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsum_vs_i32m8_i64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m1_u16m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m1_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m2_u16m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m2_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m4_u16m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m4_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m8_u16m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m8_u16m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m1_u32m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m1_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m2_u32m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m2_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m4_u32m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m4_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m8_u32m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m8_u32m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m1_u64m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m1_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m2_u64m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m2_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m4_u64m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m4_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m8_u64m1(dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m8_u64m1_tu(dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m1_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m1_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m2_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m2_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m4_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m4_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u8m8_u16m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u8m8_u16m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m1_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m1_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m2_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m2_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m4_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m4_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u16m8_u32m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u16m8_u32m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m1_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m1_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m2_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m2_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m4_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m4_u64m1_tum(mask, dest, vector, scalar, vl)
#define __riscv_vwredsumu_vs_u32m8_u64m1_m(mask, dest, vector, scalar, vl) __riscv_th_vwredsumu_vs_u32m8_u64m1_tum(mask, dest, vector, scalar, vl)

}] in
def th_vector_reduction_operations_wrapper_macros: RVVHeader;

// 16. Vector Mask Instructions

let HeaderCode =
[{
// Vector Mask Instructions
#define __riscv_vmand_mm_b1(op1, op2, vl) __riscv_th_vmand_mm_b1(op1, op2, vl)
#define __riscv_vmand_mm_b2(op1, op2, vl) __riscv_th_vmand_mm_b2(op1, op2, vl)
#define __riscv_vmand_mm_b4(op1, op2, vl) __riscv_th_vmand_mm_b4(op1, op2, vl)
#define __riscv_vmand_mm_b8(op1, op2, vl) __riscv_th_vmand_mm_b8(op1, op2, vl)
#define __riscv_vmand_mm_b16(op1, op2, vl) __riscv_th_vmand_mm_b16(op1, op2, vl)
#define __riscv_vmand_mm_b32(op1, op2, vl) __riscv_th_vmand_mm_b32(op1, op2, vl)
#define __riscv_vmand_mm_b64(op1, op2, vl) __riscv_th_vmand_mm_b64(op1, op2, vl)
#define __riscv_vmandnot_mm_b1(op1, op2, vl) __riscv_th_vmandnot_mm_b1(op1, op2, vl)
#define __riscv_vmandnot_mm_b2(op1, op2, vl) __riscv_th_vmandnot_mm_b2(op1, op2, vl)
#define __riscv_vmandnot_mm_b4(op1, op2, vl) __riscv_th_vmandnot_mm_b4(op1, op2, vl)
#define __riscv_vmandnot_mm_b8(op1, op2, vl) __riscv_th_vmandnot_mm_b8(op1, op2, vl)
#define __riscv_vmandnot_mm_b16(op1, op2, vl) __riscv_th_vmandnot_mm_b16(op1, op2, vl)
#define __riscv_vmandnot_mm_b32(op1, op2, vl) __riscv_th_vmandnot_mm_b32(op1, op2, vl)
#define __riscv_vmandnot_mm_b64(op1, op2, vl) __riscv_th_vmandnot_mm_b64(op1, op2, vl)
#define __riscv_vmnand_mm_b1(op1, op2, vl) __riscv_th_vmnand_mm_b1(op1, op2, vl)
#define __riscv_vmnand_mm_b2(op1, op2, vl) __riscv_th_vmnand_mm_b2(op1, op2, vl)
#define __riscv_vmnand_mm_b4(op1, op2, vl) __riscv_th_vmnand_mm_b4(op1, op2, vl)
#define __riscv_vmnand_mm_b8(op1, op2, vl) __riscv_th_vmnand_mm_b8(op1, op2, vl)
#define __riscv_vmnand_mm_b16(op1, op2, vl) __riscv_th_vmnand_mm_b16(op1, op2, vl)
#define __riscv_vmnand_mm_b32(op1, op2, vl) __riscv_th_vmnand_mm_b32(op1, op2, vl)
#define __riscv_vmnand_mm_b64(op1, op2, vl) __riscv_th_vmnand_mm_b64(op1, op2, vl)
#define __riscv_vmnor_mm_b1(op1, op2, vl) __riscv_th_vmnor_mm_b1(op1, op2, vl)
#define __riscv_vmnor_mm_b2(op1, op2, vl) __riscv_th_vmnor_mm_b2(op1, op2, vl)
#define __riscv_vmnor_mm_b4(op1, op2, vl) __riscv_th_vmnor_mm_b4(op1, op2, vl)
#define __riscv_vmnor_mm_b8(op1, op2, vl) __riscv_th_vmnor_mm_b8(op1, op2, vl)
#define __riscv_vmnor_mm_b16(op1, op2, vl) __riscv_th_vmnor_mm_b16(op1, op2, vl)
#define __riscv_vmnor_mm_b32(op1, op2, vl) __riscv_th_vmnor_mm_b32(op1, op2, vl)
#define __riscv_vmnor_mm_b64(op1, op2, vl) __riscv_th_vmnor_mm_b64(op1, op2, vl)
#define __riscv_vmor_mm_b1(op1, op2, vl) __riscv_th_vmor_mm_b1(op1, op2, vl)
#define __riscv_vmor_mm_b2(op1, op2, vl) __riscv_th_vmor_mm_b2(op1, op2, vl)
#define __riscv_vmor_mm_b4(op1, op2, vl) __riscv_th_vmor_mm_b4(op1, op2, vl)
#define __riscv_vmor_mm_b8(op1, op2, vl) __riscv_th_vmor_mm_b8(op1, op2, vl)
#define __riscv_vmor_mm_b16(op1, op2, vl) __riscv_th_vmor_mm_b16(op1, op2, vl)
#define __riscv_vmor_mm_b32(op1, op2, vl) __riscv_th_vmor_mm_b32(op1, op2, vl)
#define __riscv_vmor_mm_b64(op1, op2, vl) __riscv_th_vmor_mm_b64(op1, op2, vl)
#define __riscv_vmornot_mm_b1(op1, op2, vl) __riscv_th_vmornot_mm_b1(op1, op2, vl)
#define __riscv_vmornot_mm_b2(op1, op2, vl) __riscv_th_vmornot_mm_b2(op1, op2, vl)
#define __riscv_vmornot_mm_b4(op1, op2, vl) __riscv_th_vmornot_mm_b4(op1, op2, vl)
#define __riscv_vmornot_mm_b8(op1, op2, vl) __riscv_th_vmornot_mm_b8(op1, op2, vl)
#define __riscv_vmornot_mm_b16(op1, op2, vl) __riscv_th_vmornot_mm_b16(op1, op2, vl)
#define __riscv_vmornot_mm_b32(op1, op2, vl) __riscv_th_vmornot_mm_b32(op1, op2, vl)
#define __riscv_vmornot_mm_b64(op1, op2, vl) __riscv_th_vmornot_mm_b64(op1, op2, vl)
#define __riscv_vmxnor_mm_b1(op1, op2, vl) __riscv_th_vmxnor_mm_b1(op1, op2, vl)
#define __riscv_vmxnor_mm_b2(op1, op2, vl) __riscv_th_vmxnor_mm_b2(op1, op2, vl)
#define __riscv_vmxnor_mm_b4(op1, op2, vl) __riscv_th_vmxnor_mm_b4(op1, op2, vl)
#define __riscv_vmxnor_mm_b8(op1, op2, vl) __riscv_th_vmxnor_mm_b8(op1, op2, vl)
#define __riscv_vmxnor_mm_b16(op1, op2, vl) __riscv_th_vmxnor_mm_b16(op1, op2, vl)
#define __riscv_vmxnor_mm_b32(op1, op2, vl) __riscv_th_vmxnor_mm_b32(op1, op2, vl)
#define __riscv_vmxnor_mm_b64(op1, op2, vl) __riscv_th_vmxnor_mm_b64(op1, op2, vl)
#define __riscv_vmxor_mm_b1(op1, op2, vl) __riscv_th_vmxor_mm_b1(op1, op2, vl)
#define __riscv_vmxor_mm_b2(op1, op2, vl) __riscv_th_vmxor_mm_b2(op1, op2, vl)
#define __riscv_vmxor_mm_b4(op1, op2, vl) __riscv_th_vmxor_mm_b4(op1, op2, vl)
#define __riscv_vmxor_mm_b8(op1, op2, vl) __riscv_th_vmxor_mm_b8(op1, op2, vl)
#define __riscv_vmxor_mm_b16(op1, op2, vl) __riscv_th_vmxor_mm_b16(op1, op2, vl)
#define __riscv_vmxor_mm_b32(op1, op2, vl) __riscv_th_vmxor_mm_b32(op1, op2, vl)
#define __riscv_vmxor_mm_b64(op1, op2, vl) __riscv_th_vmxor_mm_b64(op1, op2, vl)

#define __riscv_vmclr_m_b1(vl) __riscv_th_vmclr_m_b1(vl)
#define __riscv_vmclr_m_b2(vl) __riscv_th_vmclr_m_b2(vl)
#define __riscv_vmclr_m_b4(vl) __riscv_th_vmclr_m_b4(vl)
#define __riscv_vmclr_m_b8(vl) __riscv_th_vmclr_m_b8(vl)
#define __riscv_vmclr_m_b16(vl) __riscv_th_vmclr_m_b16(vl)
#define __riscv_vmclr_m_b32(vl) __riscv_th_vmclr_m_b32(vl)
#define __riscv_vmclr_m_b64(vl) __riscv_th_vmclr_m_b64(vl)
#define __riscv_vmset_m_b1(vl) __riscv_th_vmset_m_b1(vl)
#define __riscv_vmset_m_b2(vl) __riscv_th_vmset_m_b2(vl)
#define __riscv_vmset_m_b4(vl) __riscv_th_vmset_m_b4(vl)
#define __riscv_vmset_m_b8(vl) __riscv_th_vmset_m_b8(vl)
#define __riscv_vmset_m_b16(vl) __riscv_th_vmset_m_b16(vl)
#define __riscv_vmset_m_b32(vl) __riscv_th_vmset_m_b32(vl)
#define __riscv_vmset_m_b64(vl) __riscv_th_vmset_m_b64(vl)
#define __riscv_vmmv_m_b1(op1, vl) __riscv_th_vmmv_m_b1(op1, vl)
#define __riscv_vmmv_m_b2(op1, vl) __riscv_th_vmmv_m_b2(op1, vl)
#define __riscv_vmmv_m_b4(op1, vl) __riscv_th_vmmv_m_b4(op1, vl)
#define __riscv_vmmv_m_b8(op1, vl) __riscv_th_vmmv_m_b8(op1, vl)
#define __riscv_vmmv_m_b16(op1, vl) __riscv_th_vmmv_m_b16(op1, vl)
#define __riscv_vmmv_m_b32(op1, vl) __riscv_th_vmmv_m_b32(op1, vl)
#define __riscv_vmmv_m_b64(op1, vl) __riscv_th_vmmv_m_b64(op1, vl)
#define __riscv_vmnot_m_b1(op1, vl) __riscv_th_vmnot_m_b1(op1, vl)
#define __riscv_vmnot_m_b2(op1, vl) __riscv_th_vmnot_m_b2(op1, vl)
#define __riscv_vmnot_m_b4(op1, vl) __riscv_th_vmnot_m_b4(op1, vl)
#define __riscv_vmnot_m_b8(op1, vl) __riscv_th_vmnot_m_b8(op1, vl)
#define __riscv_vmnot_m_b16(op1, vl) __riscv_th_vmnot_m_b16(op1, vl)
#define __riscv_vmnot_m_b32(op1, vl) __riscv_th_vmnot_m_b32(op1, vl)
#define __riscv_vmnot_m_b64(op1, vl) __riscv_th_vmnot_m_b64(op1, vl)

// XTHeadVector uses `vpopc`
#define __riscv_vpopc_m_b1(op1, vl) __riscv_th_vcpop_m_b1(op1, vl)
#define __riscv_vpopc_m_b2(op1, vl) __riscv_th_vcpop_m_b2(op1, vl)
#define __riscv_vpopc_m_b4(op1, vl) __riscv_th_vcpop_m_b4(op1, vl)
#define __riscv_vpopc_m_b8(op1, vl) __riscv_th_vcpop_m_b8(op1, vl)
#define __riscv_vpopc_m_b16(op1, vl) __riscv_th_vcpop_m_b16(op1, vl)
#define __riscv_vpopc_m_b32(op1, vl) __riscv_th_vcpop_m_b32(op1, vl)
#define __riscv_vpopc_m_b64(op1, vl) __riscv_th_vcpop_m_b64(op1, vl)
#define __riscv_vpopc_m_b1_m(mask, op1, vl) __riscv_th_vcpop_m_b1_m(mask, op1, vl)
#define __riscv_vpopc_m_b2_m(mask, op1, vl) __riscv_th_vcpop_m_b2_m(mask, op1, vl)
#define __riscv_vpopc_m_b4_m(mask, op1, vl) __riscv_th_vcpop_m_b4_m(mask, op1, vl)
#define __riscv_vpopc_m_b8_m(mask, op1, vl) __riscv_th_vcpop_m_b8_m(mask, op1, vl)
#define __riscv_vpopc_m_b16_m(mask, op1, vl) __riscv_th_vcpop_m_b16_m(mask, op1, vl)
#define __riscv_vpopc_m_b32_m(mask, op1, vl) __riscv_th_vcpop_m_b32_m(mask, op1, vl)
#define __riscv_vpopc_m_b64_m(mask, op1, vl) __riscv_th_vcpop_m_b64_m(mask, op1, vl)
// RVV 1.0 uses `vcpop` instead of `vpopc`
#define __riscv_vcpop_m_b1(op1, vl) __riscv_th_vcpop_m_b1(op1, vl)
#define __riscv_vcpop_m_b2(op1, vl) __riscv_th_vcpop_m_b2(op1, vl)
#define __riscv_vcpop_m_b4(op1, vl) __riscv_th_vcpop_m_b4(op1, vl)
#define __riscv_vcpop_m_b8(op1, vl) __riscv_th_vcpop_m_b8(op1, vl)
#define __riscv_vcpop_m_b16(op1, vl) __riscv_th_vcpop_m_b16(op1, vl)
#define __riscv_vcpop_m_b32(op1, vl) __riscv_th_vcpop_m_b32(op1, vl)
#define __riscv_vcpop_m_b64(op1, vl) __riscv_th_vcpop_m_b64(op1, vl)
#define __riscv_vcpop_m_b1_m(mask, op1, vl) __riscv_th_vcpop_m_b1_m(mask, op1, vl)
#define __riscv_vcpop_m_b2_m(mask, op1, vl) __riscv_th_vcpop_m_b2_m(mask, op1, vl)
#define __riscv_vcpop_m_b4_m(mask, op1, vl) __riscv_th_vcpop_m_b4_m(mask, op1, vl)
#define __riscv_vcpop_m_b8_m(mask, op1, vl) __riscv_th_vcpop_m_b8_m(mask, op1, vl)
#define __riscv_vcpop_m_b16_m(mask, op1, vl) __riscv_th_vcpop_m_b16_m(mask, op1, vl)
#define __riscv_vcpop_m_b32_m(mask, op1, vl) __riscv_th_vcpop_m_b32_m(mask, op1, vl)
#define __riscv_vcpop_m_b64_m(mask, op1, vl) __riscv_th_vcpop_m_b64_m(mask, op1, vl)

#define __riscv_vfirst_m_b1(op1, vl) __riscv_th_vfirst_m_b1(op1, vl)
#define __riscv_vfirst_m_b2(op1, vl) __riscv_th_vfirst_m_b2(op1, vl)
#define __riscv_vfirst_m_b4(op1, vl) __riscv_th_vfirst_m_b4(op1, vl)
#define __riscv_vfirst_m_b8(op1, vl) __riscv_th_vfirst_m_b8(op1, vl)
#define __riscv_vfirst_m_b16(op1, vl) __riscv_th_vfirst_m_b16(op1, vl)
#define __riscv_vfirst_m_b32(op1, vl) __riscv_th_vfirst_m_b32(op1, vl)
#define __riscv_vfirst_m_b64(op1, vl) __riscv_th_vfirst_m_b64(op1, vl)
#define __riscv_vfirst_m_b1_m(mask, op1, vl) __riscv_th_vfirst_m_b1_m(mask, op1, vl)
#define __riscv_vfirst_m_b2_m(mask, op1, vl) __riscv_th_vfirst_m_b2_m(mask, op1, vl)
#define __riscv_vfirst_m_b4_m(mask, op1, vl) __riscv_th_vfirst_m_b4_m(mask, op1, vl)
#define __riscv_vfirst_m_b8_m(mask, op1, vl) __riscv_th_vfirst_m_b8_m(mask, op1, vl)
#define __riscv_vfirst_m_b16_m(mask, op1, vl) __riscv_th_vfirst_m_b16_m(mask, op1, vl)
#define __riscv_vfirst_m_b32_m(mask, op1, vl) __riscv_th_vfirst_m_b32_m(mask, op1, vl)
#define __riscv_vfirst_m_b64_m(mask, op1, vl) __riscv_th_vfirst_m_b64_m(mask, op1, vl)

#define __riscv_vmsbf_m_b1(op1, vl) __riscv_th_vmsbf_m_b1(op1, vl)
#define __riscv_vmsbf_m_b2(op1, vl) __riscv_th_vmsbf_m_b2(op1, vl)
#define __riscv_vmsbf_m_b4(op1, vl) __riscv_th_vmsbf_m_b4(op1, vl)
#define __riscv_vmsbf_m_b8(op1, vl) __riscv_th_vmsbf_m_b8(op1, vl)
#define __riscv_vmsbf_m_b16(op1, vl) __riscv_th_vmsbf_m_b16(op1, vl)
#define __riscv_vmsbf_m_b32(op1, vl) __riscv_th_vmsbf_m_b32(op1, vl)
#define __riscv_vmsbf_m_b64(op1, vl) __riscv_th_vmsbf_m_b64(op1, vl)
#define __riscv_vmsbf_m_b1_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b1_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsbf_m_b2_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b2_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsbf_m_b4_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b4_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsbf_m_b8_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b8_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsbf_m_b16_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b16_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsbf_m_b32_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b32_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsbf_m_b64_m(mask, maskedoff, op1, vl) __riscv_th_vmsbf_m_b64_mu(mask, maskedoff, op1, vl)

#define __riscv_vmsof_m_b1(op1, vl) __riscv_th_vmsof_m_b1(op1, vl)
#define __riscv_vmsof_m_b2(op1, vl) __riscv_th_vmsof_m_b2(op1, vl)
#define __riscv_vmsof_m_b4(op1, vl) __riscv_th_vmsof_m_b4(op1, vl)
#define __riscv_vmsof_m_b8(op1, vl) __riscv_th_vmsof_m_b8(op1, vl)
#define __riscv_vmsof_m_b16(op1, vl) __riscv_th_vmsof_m_b16(op1, vl)
#define __riscv_vmsof_m_b32(op1, vl) __riscv_th_vmsof_m_b32(op1, vl)
#define __riscv_vmsof_m_b64(op1, vl) __riscv_th_vmsof_m_b64(op1, vl)
#define __riscv_vmsof_m_b1_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b1_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsof_m_b2_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b2_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsof_m_b4_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b4_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsof_m_b8_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b8_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsof_m_b16_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b16_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsof_m_b32_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b32_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsof_m_b64_m(mask, maskedoff, op1, vl) __riscv_th_vmsof_m_b64_mu(mask, maskedoff, op1, vl)

#define __riscv_vmsif_m_b1(op1, vl) __riscv_th_vmsif_m_b1(op1, vl)
#define __riscv_vmsif_m_b2(op1, vl) __riscv_th_vmsif_m_b2(op1, vl)
#define __riscv_vmsif_m_b4(op1, vl) __riscv_th_vmsif_m_b4(op1, vl)
#define __riscv_vmsif_m_b8(op1, vl) __riscv_th_vmsif_m_b8(op1, vl)
#define __riscv_vmsif_m_b16(op1, vl) __riscv_th_vmsif_m_b16(op1, vl)
#define __riscv_vmsif_m_b32(op1, vl) __riscv_th_vmsif_m_b32(op1, vl)
#define __riscv_vmsif_m_b64(op1, vl) __riscv_th_vmsif_m_b64(op1, vl)
#define __riscv_vmsif_m_b1_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b1_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsif_m_b2_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b2_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsif_m_b4_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b4_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsif_m_b8_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b8_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsif_m_b16_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b16_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsif_m_b32_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b32_mu(mask, maskedoff, op1, vl)
#define __riscv_vmsif_m_b64_m(mask, maskedoff, op1, vl) __riscv_th_vmsif_m_b64_mu(mask, maskedoff, op1, vl)

#define __riscv_vid_v_u8m1(vl) __riscv_th_vid_v_u8m1(vl)
#define __riscv_vid_v_u8m2(vl) __riscv_th_vid_v_u8m2(vl)
#define __riscv_vid_v_u8m4(vl) __riscv_th_vid_v_u8m4(vl)
#define __riscv_vid_v_u8m8(vl) __riscv_th_vid_v_u8m8(vl)
#define __riscv_vid_v_u16m1(vl) __riscv_th_vid_v_u16m1(vl)
#define __riscv_vid_v_u16m2(vl) __riscv_th_vid_v_u16m2(vl)
#define __riscv_vid_v_u16m4(vl) __riscv_th_vid_v_u16m4(vl)
#define __riscv_vid_v_u16m8(vl) __riscv_th_vid_v_u16m8(vl)
#define __riscv_vid_v_u32m1(vl) __riscv_th_vid_v_u32m1(vl)
#define __riscv_vid_v_u32m2(vl) __riscv_th_vid_v_u32m2(vl)
#define __riscv_vid_v_u32m4(vl) __riscv_th_vid_v_u32m4(vl)
#define __riscv_vid_v_u32m8(vl) __riscv_th_vid_v_u32m8(vl)
#define __riscv_vid_v_u64m1(vl) __riscv_th_vid_v_u64m1(vl)
#define __riscv_vid_v_u64m2(vl) __riscv_th_vid_v_u64m2(vl)
#define __riscv_vid_v_u64m4(vl) __riscv_th_vid_v_u64m4(vl)
#define __riscv_vid_v_u64m8(vl) __riscv_th_vid_v_u64m8(vl)

#define __riscv_vid_v_u8m1_m(mask, maskedoff, vl) __riscv_th_vid_v_u8m1_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u8m2_m(mask, maskedoff, vl) __riscv_th_vid_v_u8m2_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u8m4_m(mask, maskedoff, vl) __riscv_th_vid_v_u8m4_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u8m8_m(mask, maskedoff, vl) __riscv_th_vid_v_u8m8_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u16m1_m(mask, maskedoff, vl) __riscv_th_vid_v_u16m1_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u16m2_m(mask, maskedoff, vl) __riscv_th_vid_v_u16m2_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u16m4_m(mask, maskedoff, vl) __riscv_th_vid_v_u16m4_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u16m8_m(mask, maskedoff, vl) __riscv_th_vid_v_u16m8_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u32m1_m(mask, maskedoff, vl) __riscv_th_vid_v_u32m1_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u32m2_m(mask, maskedoff, vl) __riscv_th_vid_v_u32m2_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u32m4_m(mask, maskedoff, vl) __riscv_th_vid_v_u32m4_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u32m8_m(mask, maskedoff, vl) __riscv_th_vid_v_u32m8_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u64m1_m(mask, maskedoff, vl) __riscv_th_vid_v_u64m1_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u64m2_m(mask, maskedoff, vl) __riscv_th_vid_v_u64m2_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u64m4_m(mask, maskedoff, vl) __riscv_th_vid_v_u64m4_mu(mask, maskedoff, vl)
#define __riscv_vid_v_u64m8_m(mask, maskedoff, vl) __riscv_th_vid_v_u64m8_mu(mask, maskedoff, vl)

#define __riscv_viota_m_u8m1(op1, vl) __riscv_th_viota_m_u8m1(op1, vl)
#define __riscv_viota_m_u8m2(op1, vl) __riscv_th_viota_m_u8m2(op1, vl)
#define __riscv_viota_m_u8m4(op1, vl) __riscv_th_viota_m_u8m4(op1, vl)
#define __riscv_viota_m_u8m8(op1, vl) __riscv_th_viota_m_u8m8(op1, vl)
#define __riscv_viota_m_u16m1(op1, vl) __riscv_th_viota_m_u16m1(op1, vl)
#define __riscv_viota_m_u16m2(op1, vl) __riscv_th_viota_m_u16m2(op1, vl)
#define __riscv_viota_m_u16m4(op1, vl) __riscv_th_viota_m_u16m4(op1, vl)
#define __riscv_viota_m_u16m8(op1, vl) __riscv_th_viota_m_u16m8(op1, vl)
#define __riscv_viota_m_u32m1(op1, vl) __riscv_th_viota_m_u32m1(op1, vl)
#define __riscv_viota_m_u32m2(op1, vl) __riscv_th_viota_m_u32m2(op1, vl)
#define __riscv_viota_m_u32m4(op1, vl) __riscv_th_viota_m_u32m4(op1, vl)
#define __riscv_viota_m_u32m8(op1, vl) __riscv_th_viota_m_u32m8(op1, vl)
#define __riscv_viota_m_u64m1(op1, vl) __riscv_th_viota_m_u64m1(op1, vl)
#define __riscv_viota_m_u64m2(op1, vl) __riscv_th_viota_m_u64m2(op1, vl)
#define __riscv_viota_m_u64m4(op1, vl) __riscv_th_viota_m_u64m4(op1, vl)
#define __riscv_viota_m_u64m8(op1, vl) __riscv_th_viota_m_u64m8(op1, vl)

#define __riscv_viota_m_u8m1_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u8m1_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u8m2_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u8m2_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u8m4_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u8m4_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u8m8_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u8m8_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u16m1_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u16m1_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u16m2_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u16m2_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u16m4_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u16m4_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u16m8_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u16m8_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u32m1_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u32m1_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u32m2_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u32m2_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u32m4_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u32m4_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u32m8_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u32m8_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u64m1_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u64m1_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u64m2_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u64m2_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u64m4_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u64m4_mu(mask, maskedoff, op1, vl)
#define __riscv_viota_m_u64m8_m(mask, maskedoff, op1, vl) __riscv_th_viota_m_u64m8_mu(mask, maskedoff, op1, vl)

}] in
def th_vector_mask_wrapper_macros: RVVHeader;

// 17. Vector Permutation Instructions

let HeaderCode =
[{
#define __riscv_vcompress_vm_f16m1(src, mask, vl) __riscv_th_vcompress_vm_f16m1(src, mask, vl)
#define __riscv_vcompress_vm_f16m2(src, mask, vl) __riscv_th_vcompress_vm_f16m2(src, mask, vl)
#define __riscv_vcompress_vm_f16m4(src, mask, vl) __riscv_th_vcompress_vm_f16m4(src, mask, vl)
#define __riscv_vcompress_vm_f16m8(src, mask, vl) __riscv_th_vcompress_vm_f16m8(src, mask, vl)
#define __riscv_vcompress_vm_f32m1(src, mask, vl) __riscv_th_vcompress_vm_f32m1(src, mask, vl)
#define __riscv_vcompress_vm_f32m2(src, mask, vl) __riscv_th_vcompress_vm_f32m2(src, mask, vl)
#define __riscv_vcompress_vm_f32m4(src, mask, vl) __riscv_th_vcompress_vm_f32m4(src, mask, vl)
#define __riscv_vcompress_vm_f32m8(src, mask, vl) __riscv_th_vcompress_vm_f32m8(src, mask, vl)
#define __riscv_vcompress_vm_f64m1(src, mask, vl) __riscv_th_vcompress_vm_f64m1(src, mask, vl)
#define __riscv_vcompress_vm_f64m2(src, mask, vl) __riscv_th_vcompress_vm_f64m2(src, mask, vl)
#define __riscv_vcompress_vm_f64m4(src, mask, vl) __riscv_th_vcompress_vm_f64m4(src, mask, vl)
#define __riscv_vcompress_vm_f64m8(src, mask, vl) __riscv_th_vcompress_vm_f64m8(src, mask, vl)
#define __riscv_vcompress_vm_i8m1(src, mask, vl) __riscv_th_vcompress_vm_i8m1(src, mask, vl)
#define __riscv_vcompress_vm_i8m2(src, mask, vl) __riscv_th_vcompress_vm_i8m2(src, mask, vl)
#define __riscv_vcompress_vm_i8m4(src, mask, vl) __riscv_th_vcompress_vm_i8m4(src, mask, vl)
#define __riscv_vcompress_vm_i8m8(src, mask, vl) __riscv_th_vcompress_vm_i8m8(src, mask, vl)
#define __riscv_vcompress_vm_i16m1(src, mask, vl) __riscv_th_vcompress_vm_i16m1(src, mask, vl)
#define __riscv_vcompress_vm_i16m2(src, mask, vl) __riscv_th_vcompress_vm_i16m2(src, mask, vl)
#define __riscv_vcompress_vm_i16m4(src, mask, vl) __riscv_th_vcompress_vm_i16m4(src, mask, vl)
#define __riscv_vcompress_vm_i16m8(src, mask, vl) __riscv_th_vcompress_vm_i16m8(src, mask, vl)
#define __riscv_vcompress_vm_i32m1(src, mask, vl) __riscv_th_vcompress_vm_i32m1(src, mask, vl)
#define __riscv_vcompress_vm_i32m2(src, mask, vl) __riscv_th_vcompress_vm_i32m2(src, mask, vl)
#define __riscv_vcompress_vm_i32m4(src, mask, vl) __riscv_th_vcompress_vm_i32m4(src, mask, vl)
#define __riscv_vcompress_vm_i32m8(src, mask, vl) __riscv_th_vcompress_vm_i32m8(src, mask, vl)
#define __riscv_vcompress_vm_i64m1(src, mask, vl) __riscv_th_vcompress_vm_i64m1(src, mask, vl)
#define __riscv_vcompress_vm_i64m2(src, mask, vl) __riscv_th_vcompress_vm_i64m2(src, mask, vl)
#define __riscv_vcompress_vm_i64m4(src, mask, vl) __riscv_th_vcompress_vm_i64m4(src, mask, vl)
#define __riscv_vcompress_vm_i64m8(src, mask, vl) __riscv_th_vcompress_vm_i64m8(src, mask, vl)
#define __riscv_vcompress_vm_u8m1(src, mask, vl) __riscv_th_vcompress_vm_u8m1(src, mask, vl)
#define __riscv_vcompress_vm_u8m2(src, mask, vl) __riscv_th_vcompress_vm_u8m2(src, mask, vl)
#define __riscv_vcompress_vm_u8m4(src, mask, vl) __riscv_th_vcompress_vm_u8m4(src, mask, vl)
#define __riscv_vcompress_vm_u8m8(src, mask, vl) __riscv_th_vcompress_vm_u8m8(src, mask, vl)
#define __riscv_vcompress_vm_u16m1(src, mask, vl) __riscv_th_vcompress_vm_u16m1(src, mask, vl)
#define __riscv_vcompress_vm_u16m2(src, mask, vl) __riscv_th_vcompress_vm_u16m2(src, mask, vl)
#define __riscv_vcompress_vm_u16m4(src, mask, vl) __riscv_th_vcompress_vm_u16m4(src, mask, vl)
#define __riscv_vcompress_vm_u16m8(src, mask, vl) __riscv_th_vcompress_vm_u16m8(src, mask, vl)
#define __riscv_vcompress_vm_u32m1(src, mask, vl) __riscv_th_vcompress_vm_u32m1(src, mask, vl)
#define __riscv_vcompress_vm_u32m2(src, mask, vl) __riscv_th_vcompress_vm_u32m2(src, mask, vl)
#define __riscv_vcompress_vm_u32m4(src, mask, vl) __riscv_th_vcompress_vm_u32m4(src, mask, vl)
#define __riscv_vcompress_vm_u32m8(src, mask, vl) __riscv_th_vcompress_vm_u32m8(src, mask, vl)
#define __riscv_vcompress_vm_u64m1(src, mask, vl) __riscv_th_vcompress_vm_u64m1(src, mask, vl)
#define __riscv_vcompress_vm_u64m2(src, mask, vl) __riscv_th_vcompress_vm_u64m2(src, mask, vl)
#define __riscv_vcompress_vm_u64m4(src, mask, vl) __riscv_th_vcompress_vm_u64m4(src, mask, vl)
#define __riscv_vcompress_vm_u64m8(src, mask, vl) __riscv_th_vcompress_vm_u64m8(src, mask, vl)
#define __riscv_vrgather_vv_f16m1(op1, index, vl) __riscv_th_vrgather_vv_f16m1(op1, index, vl)
#define __riscv_vrgather_vx_f16m1(op1, index, vl) __riscv_th_vrgather_vx_f16m1(op1, index, vl)
#define __riscv_vrgather_vv_f16m2(op1, index, vl) __riscv_th_vrgather_vv_f16m2(op1, index, vl)
#define __riscv_vrgather_vx_f16m2(op1, index, vl) __riscv_th_vrgather_vx_f16m2(op1, index, vl)
#define __riscv_vrgather_vv_f16m4(op1, index, vl) __riscv_th_vrgather_vv_f16m4(op1, index, vl)
#define __riscv_vrgather_vx_f16m4(op1, index, vl) __riscv_th_vrgather_vx_f16m4(op1, index, vl)
#define __riscv_vrgather_vv_f16m8(op1, index, vl) __riscv_th_vrgather_vv_f16m8(op1, index, vl)
#define __riscv_vrgather_vx_f16m8(op1, index, vl) __riscv_th_vrgather_vx_f16m8(op1, index, vl)
#define __riscv_vrgather_vv_f32m1(op1, index, vl) __riscv_th_vrgather_vv_f32m1(op1, index, vl)
#define __riscv_vrgather_vx_f32m1(op1, index, vl) __riscv_th_vrgather_vx_f32m1(op1, index, vl)
#define __riscv_vrgather_vv_f32m2(op1, index, vl) __riscv_th_vrgather_vv_f32m2(op1, index, vl)
#define __riscv_vrgather_vx_f32m2(op1, index, vl) __riscv_th_vrgather_vx_f32m2(op1, index, vl)
#define __riscv_vrgather_vv_f32m4(op1, index, vl) __riscv_th_vrgather_vv_f32m4(op1, index, vl)
#define __riscv_vrgather_vx_f32m4(op1, index, vl) __riscv_th_vrgather_vx_f32m4(op1, index, vl)
#define __riscv_vrgather_vv_f32m8(op1, index, vl) __riscv_th_vrgather_vv_f32m8(op1, index, vl)
#define __riscv_vrgather_vx_f32m8(op1, index, vl) __riscv_th_vrgather_vx_f32m8(op1, index, vl)
#define __riscv_vrgather_vv_f64m1(op1, index, vl) __riscv_th_vrgather_vv_f64m1(op1, index, vl)
#define __riscv_vrgather_vx_f64m1(op1, index, vl) __riscv_th_vrgather_vx_f64m1(op1, index, vl)
#define __riscv_vrgather_vv_f64m2(op1, index, vl) __riscv_th_vrgather_vv_f64m2(op1, index, vl)
#define __riscv_vrgather_vx_f64m2(op1, index, vl) __riscv_th_vrgather_vx_f64m2(op1, index, vl)
#define __riscv_vrgather_vv_f64m4(op1, index, vl) __riscv_th_vrgather_vv_f64m4(op1, index, vl)
#define __riscv_vrgather_vx_f64m4(op1, index, vl) __riscv_th_vrgather_vx_f64m4(op1, index, vl)
#define __riscv_vrgather_vv_f64m8(op1, index, vl) __riscv_th_vrgather_vv_f64m8(op1, index, vl)
#define __riscv_vrgather_vx_f64m8(op1, index, vl) __riscv_th_vrgather_vx_f64m8(op1, index, vl)
#define __riscv_vrgather_vv_i8m1(op1, index, vl) __riscv_th_vrgather_vv_i8m1(op1, index, vl)
#define __riscv_vrgather_vx_i8m1(op1, index, vl) __riscv_th_vrgather_vx_i8m1(op1, index, vl)
#define __riscv_vrgather_vv_i8m2(op1, index, vl) __riscv_th_vrgather_vv_i8m2(op1, index, vl)
#define __riscv_vrgather_vx_i8m2(op1, index, vl) __riscv_th_vrgather_vx_i8m2(op1, index, vl)
#define __riscv_vrgather_vv_i8m4(op1, index, vl) __riscv_th_vrgather_vv_i8m4(op1, index, vl)
#define __riscv_vrgather_vx_i8m4(op1, index, vl) __riscv_th_vrgather_vx_i8m4(op1, index, vl)
#define __riscv_vrgather_vv_i8m8(op1, index, vl) __riscv_th_vrgather_vv_i8m8(op1, index, vl)
#define __riscv_vrgather_vx_i8m8(op1, index, vl) __riscv_th_vrgather_vx_i8m8(op1, index, vl)
#define __riscv_vrgather_vv_i16m1(op1, index, vl) __riscv_th_vrgather_vv_i16m1(op1, index, vl)
#define __riscv_vrgather_vx_i16m1(op1, index, vl) __riscv_th_vrgather_vx_i16m1(op1, index, vl)
#define __riscv_vrgather_vv_i16m2(op1, index, vl) __riscv_th_vrgather_vv_i16m2(op1, index, vl)
#define __riscv_vrgather_vx_i16m2(op1, index, vl) __riscv_th_vrgather_vx_i16m2(op1, index, vl)
#define __riscv_vrgather_vv_i16m4(op1, index, vl) __riscv_th_vrgather_vv_i16m4(op1, index, vl)
#define __riscv_vrgather_vx_i16m4(op1, index, vl) __riscv_th_vrgather_vx_i16m4(op1, index, vl)
#define __riscv_vrgather_vv_i16m8(op1, index, vl) __riscv_th_vrgather_vv_i16m8(op1, index, vl)
#define __riscv_vrgather_vx_i16m8(op1, index, vl) __riscv_th_vrgather_vx_i16m8(op1, index, vl)
#define __riscv_vrgather_vv_i32m1(op1, index, vl) __riscv_th_vrgather_vv_i32m1(op1, index, vl)
#define __riscv_vrgather_vx_i32m1(op1, index, vl) __riscv_th_vrgather_vx_i32m1(op1, index, vl)
#define __riscv_vrgather_vv_i32m2(op1, index, vl) __riscv_th_vrgather_vv_i32m2(op1, index, vl)
#define __riscv_vrgather_vx_i32m2(op1, index, vl) __riscv_th_vrgather_vx_i32m2(op1, index, vl)
#define __riscv_vrgather_vv_i32m4(op1, index, vl) __riscv_th_vrgather_vv_i32m4(op1, index, vl)
#define __riscv_vrgather_vx_i32m4(op1, index, vl) __riscv_th_vrgather_vx_i32m4(op1, index, vl)
#define __riscv_vrgather_vv_i32m8(op1, index, vl) __riscv_th_vrgather_vv_i32m8(op1, index, vl)
#define __riscv_vrgather_vx_i32m8(op1, index, vl) __riscv_th_vrgather_vx_i32m8(op1, index, vl)
#define __riscv_vrgather_vv_i64m1(op1, index, vl) __riscv_th_vrgather_vv_i64m1(op1, index, vl)
#define __riscv_vrgather_vx_i64m1(op1, index, vl) __riscv_th_vrgather_vx_i64m1(op1, index, vl)
#define __riscv_vrgather_vv_i64m2(op1, index, vl) __riscv_th_vrgather_vv_i64m2(op1, index, vl)
#define __riscv_vrgather_vx_i64m2(op1, index, vl) __riscv_th_vrgather_vx_i64m2(op1, index, vl)
#define __riscv_vrgather_vv_i64m4(op1, index, vl) __riscv_th_vrgather_vv_i64m4(op1, index, vl)
#define __riscv_vrgather_vx_i64m4(op1, index, vl) __riscv_th_vrgather_vx_i64m4(op1, index, vl)
#define __riscv_vrgather_vv_i64m8(op1, index, vl) __riscv_th_vrgather_vv_i64m8(op1, index, vl)
#define __riscv_vrgather_vx_i64m8(op1, index, vl) __riscv_th_vrgather_vx_i64m8(op1, index, vl)
#define __riscv_vrgather_vv_u8m1(op1, index, vl) __riscv_th_vrgather_vv_u8m1(op1, index, vl)
#define __riscv_vrgather_vx_u8m1(op1, index, vl) __riscv_th_vrgather_vx_u8m1(op1, index, vl)
#define __riscv_vrgather_vv_u8m2(op1, index, vl) __riscv_th_vrgather_vv_u8m2(op1, index, vl)
#define __riscv_vrgather_vx_u8m2(op1, index, vl) __riscv_th_vrgather_vx_u8m2(op1, index, vl)
#define __riscv_vrgather_vv_u8m4(op1, index, vl) __riscv_th_vrgather_vv_u8m4(op1, index, vl)
#define __riscv_vrgather_vx_u8m4(op1, index, vl) __riscv_th_vrgather_vx_u8m4(op1, index, vl)
#define __riscv_vrgather_vv_u8m8(op1, index, vl) __riscv_th_vrgather_vv_u8m8(op1, index, vl)
#define __riscv_vrgather_vx_u8m8(op1, index, vl) __riscv_th_vrgather_vx_u8m8(op1, index, vl)
#define __riscv_vrgather_vv_u16m1(op1, index, vl) __riscv_th_vrgather_vv_u16m1(op1, index, vl)
#define __riscv_vrgather_vx_u16m1(op1, index, vl) __riscv_th_vrgather_vx_u16m1(op1, index, vl)
#define __riscv_vrgather_vv_u16m2(op1, index, vl) __riscv_th_vrgather_vv_u16m2(op1, index, vl)
#define __riscv_vrgather_vx_u16m2(op1, index, vl) __riscv_th_vrgather_vx_u16m2(op1, index, vl)
#define __riscv_vrgather_vv_u16m4(op1, index, vl) __riscv_th_vrgather_vv_u16m4(op1, index, vl)
#define __riscv_vrgather_vx_u16m4(op1, index, vl) __riscv_th_vrgather_vx_u16m4(op1, index, vl)
#define __riscv_vrgather_vv_u16m8(op1, index, vl) __riscv_th_vrgather_vv_u16m8(op1, index, vl)
#define __riscv_vrgather_vx_u16m8(op1, index, vl) __riscv_th_vrgather_vx_u16m8(op1, index, vl)
#define __riscv_vrgather_vv_u32m1(op1, index, vl) __riscv_th_vrgather_vv_u32m1(op1, index, vl)
#define __riscv_vrgather_vx_u32m1(op1, index, vl) __riscv_th_vrgather_vx_u32m1(op1, index, vl)
#define __riscv_vrgather_vv_u32m2(op1, index, vl) __riscv_th_vrgather_vv_u32m2(op1, index, vl)
#define __riscv_vrgather_vx_u32m2(op1, index, vl) __riscv_th_vrgather_vx_u32m2(op1, index, vl)
#define __riscv_vrgather_vv_u32m4(op1, index, vl) __riscv_th_vrgather_vv_u32m4(op1, index, vl)
#define __riscv_vrgather_vx_u32m4(op1, index, vl) __riscv_th_vrgather_vx_u32m4(op1, index, vl)
#define __riscv_vrgather_vv_u32m8(op1, index, vl) __riscv_th_vrgather_vv_u32m8(op1, index, vl)
#define __riscv_vrgather_vx_u32m8(op1, index, vl) __riscv_th_vrgather_vx_u32m8(op1, index, vl)
#define __riscv_vrgather_vv_u64m1(op1, index, vl) __riscv_th_vrgather_vv_u64m1(op1, index, vl)
#define __riscv_vrgather_vx_u64m1(op1, index, vl) __riscv_th_vrgather_vx_u64m1(op1, index, vl)
#define __riscv_vrgather_vv_u64m2(op1, index, vl) __riscv_th_vrgather_vv_u64m2(op1, index, vl)
#define __riscv_vrgather_vx_u64m2(op1, index, vl) __riscv_th_vrgather_vx_u64m2(op1, index, vl)
#define __riscv_vrgather_vv_u64m4(op1, index, vl) __riscv_th_vrgather_vv_u64m4(op1, index, vl)
#define __riscv_vrgather_vx_u64m4(op1, index, vl) __riscv_th_vrgather_vx_u64m4(op1, index, vl)
#define __riscv_vrgather_vv_u64m8(op1, index, vl) __riscv_th_vrgather_vv_u64m8(op1, index, vl)
#define __riscv_vrgather_vx_u64m8(op1, index, vl) __riscv_th_vrgather_vx_u64m8(op1, index, vl)
#define __riscv_vrgather_vv_f16m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f16m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f16m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f16m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f16m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f16m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f16m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f16m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f16m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f16m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f16m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f16m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f16m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f16m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f16m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f16m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f32m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f32m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f32m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f32m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f32m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f32m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f32m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f32m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f32m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f32m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f32m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f32m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f32m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f32m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f32m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f32m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f64m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f64m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f64m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f64m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f64m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f64m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f64m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f64m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f64m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f64m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f64m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f64m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_f64m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_f64m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_f64m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_f64m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i8m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i8m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i8m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i8m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i8m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i8m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i8m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i8m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i8m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i8m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i8m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i8m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i8m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i8m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i8m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i8m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i16m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i16m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i16m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i16m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i16m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i16m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i16m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i16m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i16m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i16m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i16m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i16m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i16m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i16m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i16m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i16m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i32m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i32m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i32m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i32m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i32m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i32m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i32m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i32m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i32m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i32m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i32m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i32m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i32m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i32m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i32m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i32m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i64m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i64m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i64m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i64m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i64m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i64m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i64m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i64m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i64m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i64m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i64m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i64m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_i64m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_i64m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_i64m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_i64m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u8m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u8m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u8m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u8m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u8m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u8m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u8m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u8m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u8m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u8m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u8m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u8m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u8m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u8m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u8m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u8m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u16m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u16m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u16m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u16m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u16m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u16m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u16m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u16m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u16m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u16m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u16m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u16m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u16m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u16m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u16m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u16m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u32m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u32m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u32m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u32m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u32m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u32m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u32m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u32m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u32m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u32m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u32m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u32m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u32m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u32m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u32m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u32m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u64m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u64m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u64m1_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u64m1_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u64m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u64m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u64m2_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u64m2_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u64m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u64m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u64m4_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u64m4_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vv_u64m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vv_u64m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vrgather_vx_u64m8_m(mask, maskedoff, op1, index, vl) __riscv_th_vrgather_vx_u64m8_mu(mask, maskedoff, op1, index, vl)
#define __riscv_vslide1down_vx_i8m1(src, value, vl) __riscv_th_vslide1down_vx_i8m1(src, value, vl)
#define __riscv_vslide1down_vx_i8m2(src, value, vl) __riscv_th_vslide1down_vx_i8m2(src, value, vl)
#define __riscv_vslide1down_vx_i8m4(src, value, vl) __riscv_th_vslide1down_vx_i8m4(src, value, vl)
#define __riscv_vslide1down_vx_i8m8(src, value, vl) __riscv_th_vslide1down_vx_i8m8(src, value, vl)
#define __riscv_vslide1down_vx_i16m1(src, value, vl) __riscv_th_vslide1down_vx_i16m1(src, value, vl)
#define __riscv_vslide1down_vx_i16m2(src, value, vl) __riscv_th_vslide1down_vx_i16m2(src, value, vl)
#define __riscv_vslide1down_vx_i16m4(src, value, vl) __riscv_th_vslide1down_vx_i16m4(src, value, vl)
#define __riscv_vslide1down_vx_i16m8(src, value, vl) __riscv_th_vslide1down_vx_i16m8(src, value, vl)
#define __riscv_vslide1down_vx_i32m1(src, value, vl) __riscv_th_vslide1down_vx_i32m1(src, value, vl)
#define __riscv_vslide1down_vx_i32m2(src, value, vl) __riscv_th_vslide1down_vx_i32m2(src, value, vl)
#define __riscv_vslide1down_vx_i32m4(src, value, vl) __riscv_th_vslide1down_vx_i32m4(src, value, vl)
#define __riscv_vslide1down_vx_i32m8(src, value, vl) __riscv_th_vslide1down_vx_i32m8(src, value, vl)
#define __riscv_vslide1down_vx_i64m1(src, value, vl) __riscv_th_vslide1down_vx_i64m1(src, value, vl)
#define __riscv_vslide1down_vx_i64m2(src, value, vl) __riscv_th_vslide1down_vx_i64m2(src, value, vl)
#define __riscv_vslide1down_vx_i64m4(src, value, vl) __riscv_th_vslide1down_vx_i64m4(src, value, vl)
#define __riscv_vslide1down_vx_i64m8(src, value, vl) __riscv_th_vslide1down_vx_i64m8(src, value, vl)
#define __riscv_vslide1down_vx_u8m1(src, value, vl) __riscv_th_vslide1down_vx_u8m1(src, value, vl)
#define __riscv_vslide1down_vx_u8m2(src, value, vl) __riscv_th_vslide1down_vx_u8m2(src, value, vl)
#define __riscv_vslide1down_vx_u8m4(src, value, vl) __riscv_th_vslide1down_vx_u8m4(src, value, vl)
#define __riscv_vslide1down_vx_u8m8(src, value, vl) __riscv_th_vslide1down_vx_u8m8(src, value, vl)
#define __riscv_vslide1down_vx_u16m1(src, value, vl) __riscv_th_vslide1down_vx_u16m1(src, value, vl)
#define __riscv_vslide1down_vx_u16m2(src, value, vl) __riscv_th_vslide1down_vx_u16m2(src, value, vl)
#define __riscv_vslide1down_vx_u16m4(src, value, vl) __riscv_th_vslide1down_vx_u16m4(src, value, vl)
#define __riscv_vslide1down_vx_u16m8(src, value, vl) __riscv_th_vslide1down_vx_u16m8(src, value, vl)
#define __riscv_vslide1down_vx_u32m1(src, value, vl) __riscv_th_vslide1down_vx_u32m1(src, value, vl)
#define __riscv_vslide1down_vx_u32m2(src, value, vl) __riscv_th_vslide1down_vx_u32m2(src, value, vl)
#define __riscv_vslide1down_vx_u32m4(src, value, vl) __riscv_th_vslide1down_vx_u32m4(src, value, vl)
#define __riscv_vslide1down_vx_u32m8(src, value, vl) __riscv_th_vslide1down_vx_u32m8(src, value, vl)
#define __riscv_vslide1down_vx_u64m1(src, value, vl) __riscv_th_vslide1down_vx_u64m1(src, value, vl)
#define __riscv_vslide1down_vx_u64m2(src, value, vl) __riscv_th_vslide1down_vx_u64m2(src, value, vl)
#define __riscv_vslide1down_vx_u64m4(src, value, vl) __riscv_th_vslide1down_vx_u64m4(src, value, vl)
#define __riscv_vslide1down_vx_u64m8(src, value, vl) __riscv_th_vslide1down_vx_u64m8(src, value, vl)
#define __riscv_vslide1down_vx_i8m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i8m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i8m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i8m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i8m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i8m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i8m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i8m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i16m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i16m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i16m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i16m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i16m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i16m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i16m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i16m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i32m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i32m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i32m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i32m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i32m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i32m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i32m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i32m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i64m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i64m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i64m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i64m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i64m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i64m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_i64m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_i64m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u8m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u8m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u8m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u8m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u8m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u8m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u8m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u8m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u16m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u16m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u16m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u16m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u16m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u16m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u16m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u16m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u32m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u32m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u32m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u32m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u32m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u32m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u32m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u32m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u64m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u64m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u64m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u64m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u64m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u64m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1down_vx_u64m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1down_vx_u64m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i8m1(src, value, vl) __riscv_th_vslide1up_vx_i8m1(src, value, vl)
#define __riscv_vslide1up_vx_i8m2(src, value, vl) __riscv_th_vslide1up_vx_i8m2(src, value, vl)
#define __riscv_vslide1up_vx_i8m4(src, value, vl) __riscv_th_vslide1up_vx_i8m4(src, value, vl)
#define __riscv_vslide1up_vx_i8m8(src, value, vl) __riscv_th_vslide1up_vx_i8m8(src, value, vl)
#define __riscv_vslide1up_vx_i16m1(src, value, vl) __riscv_th_vslide1up_vx_i16m1(src, value, vl)
#define __riscv_vslide1up_vx_i16m2(src, value, vl) __riscv_th_vslide1up_vx_i16m2(src, value, vl)
#define __riscv_vslide1up_vx_i16m4(src, value, vl) __riscv_th_vslide1up_vx_i16m4(src, value, vl)
#define __riscv_vslide1up_vx_i16m8(src, value, vl) __riscv_th_vslide1up_vx_i16m8(src, value, vl)
#define __riscv_vslide1up_vx_i32m1(src, value, vl) __riscv_th_vslide1up_vx_i32m1(src, value, vl)
#define __riscv_vslide1up_vx_i32m2(src, value, vl) __riscv_th_vslide1up_vx_i32m2(src, value, vl)
#define __riscv_vslide1up_vx_i32m4(src, value, vl) __riscv_th_vslide1up_vx_i32m4(src, value, vl)
#define __riscv_vslide1up_vx_i32m8(src, value, vl) __riscv_th_vslide1up_vx_i32m8(src, value, vl)
#define __riscv_vslide1up_vx_i64m1(src, value, vl) __riscv_th_vslide1up_vx_i64m1(src, value, vl)
#define __riscv_vslide1up_vx_i64m2(src, value, vl) __riscv_th_vslide1up_vx_i64m2(src, value, vl)
#define __riscv_vslide1up_vx_i64m4(src, value, vl) __riscv_th_vslide1up_vx_i64m4(src, value, vl)
#define __riscv_vslide1up_vx_i64m8(src, value, vl) __riscv_th_vslide1up_vx_i64m8(src, value, vl)
#define __riscv_vslide1up_vx_u8m1(src, value, vl) __riscv_th_vslide1up_vx_u8m1(src, value, vl)
#define __riscv_vslide1up_vx_u8m2(src, value, vl) __riscv_th_vslide1up_vx_u8m2(src, value, vl)
#define __riscv_vslide1up_vx_u8m4(src, value, vl) __riscv_th_vslide1up_vx_u8m4(src, value, vl)
#define __riscv_vslide1up_vx_u8m8(src, value, vl) __riscv_th_vslide1up_vx_u8m8(src, value, vl)
#define __riscv_vslide1up_vx_u16m1(src, value, vl) __riscv_th_vslide1up_vx_u16m1(src, value, vl)
#define __riscv_vslide1up_vx_u16m2(src, value, vl) __riscv_th_vslide1up_vx_u16m2(src, value, vl)
#define __riscv_vslide1up_vx_u16m4(src, value, vl) __riscv_th_vslide1up_vx_u16m4(src, value, vl)
#define __riscv_vslide1up_vx_u16m8(src, value, vl) __riscv_th_vslide1up_vx_u16m8(src, value, vl)
#define __riscv_vslide1up_vx_u32m1(src, value, vl) __riscv_th_vslide1up_vx_u32m1(src, value, vl)
#define __riscv_vslide1up_vx_u32m2(src, value, vl) __riscv_th_vslide1up_vx_u32m2(src, value, vl)
#define __riscv_vslide1up_vx_u32m4(src, value, vl) __riscv_th_vslide1up_vx_u32m4(src, value, vl)
#define __riscv_vslide1up_vx_u32m8(src, value, vl) __riscv_th_vslide1up_vx_u32m8(src, value, vl)
#define __riscv_vslide1up_vx_u64m1(src, value, vl) __riscv_th_vslide1up_vx_u64m1(src, value, vl)
#define __riscv_vslide1up_vx_u64m2(src, value, vl) __riscv_th_vslide1up_vx_u64m2(src, value, vl)
#define __riscv_vslide1up_vx_u64m4(src, value, vl) __riscv_th_vslide1up_vx_u64m4(src, value, vl)
#define __riscv_vslide1up_vx_u64m8(src, value, vl) __riscv_th_vslide1up_vx_u64m8(src, value, vl)
#define __riscv_vslide1up_vx_i8m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i8m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i8m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i8m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i8m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i8m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i8m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i8m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i16m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i16m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i16m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i16m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i16m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i16m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i16m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i16m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i32m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i32m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i32m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i32m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i32m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i32m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i32m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i32m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i64m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i64m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i64m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i64m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i64m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i64m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_i64m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_i64m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u8m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u8m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u8m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u8m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u8m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u8m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u8m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u8m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u16m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u16m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u16m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u16m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u16m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u16m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u16m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u16m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u32m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u32m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u32m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u32m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u32m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u32m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u32m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u32m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u64m1_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u64m1_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u64m2_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u64m2_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u64m4_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u64m4_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslide1up_vx_u64m8_m(mask, maskedoff, src, value, vl) __riscv_th_vslide1up_vx_u64m8_mu(mask, maskedoff, src, value, vl)
#define __riscv_vslidedown_vx_f16m1(src, offset, vl) __riscv_th_vslidedown_vx_f16m1(src, offset, vl)
#define __riscv_vslidedown_vx_f16m2(src, offset, vl) __riscv_th_vslidedown_vx_f16m2(src, offset, vl)
#define __riscv_vslidedown_vx_f16m4(src, offset, vl) __riscv_th_vslidedown_vx_f16m4(src, offset, vl)
#define __riscv_vslidedown_vx_f16m8(src, offset, vl) __riscv_th_vslidedown_vx_f16m8(src, offset, vl)
#define __riscv_vslidedown_vx_f32m1(src, offset, vl) __riscv_th_vslidedown_vx_f32m1(src, offset, vl)
#define __riscv_vslidedown_vx_f32m2(src, offset, vl) __riscv_th_vslidedown_vx_f32m2(src, offset, vl)
#define __riscv_vslidedown_vx_f32m4(src, offset, vl) __riscv_th_vslidedown_vx_f32m4(src, offset, vl)
#define __riscv_vslidedown_vx_f32m8(src, offset, vl) __riscv_th_vslidedown_vx_f32m8(src, offset, vl)
#define __riscv_vslidedown_vx_f64m1(src, offset, vl) __riscv_th_vslidedown_vx_f64m1(src, offset, vl)
#define __riscv_vslidedown_vx_f64m2(src, offset, vl) __riscv_th_vslidedown_vx_f64m2(src, offset, vl)
#define __riscv_vslidedown_vx_f64m4(src, offset, vl) __riscv_th_vslidedown_vx_f64m4(src, offset, vl)
#define __riscv_vslidedown_vx_f64m8(src, offset, vl) __riscv_th_vslidedown_vx_f64m8(src, offset, vl)
#define __riscv_vslidedown_vx_i8m1(src, offset, vl) __riscv_th_vslidedown_vx_i8m1(src, offset, vl)
#define __riscv_vslidedown_vx_i8m2(src, offset, vl) __riscv_th_vslidedown_vx_i8m2(src, offset, vl)
#define __riscv_vslidedown_vx_i8m4(src, offset, vl) __riscv_th_vslidedown_vx_i8m4(src, offset, vl)
#define __riscv_vslidedown_vx_i8m8(src, offset, vl) __riscv_th_vslidedown_vx_i8m8(src, offset, vl)
#define __riscv_vslidedown_vx_i16m1(src, offset, vl) __riscv_th_vslidedown_vx_i16m1(src, offset, vl)
#define __riscv_vslidedown_vx_i16m2(src, offset, vl) __riscv_th_vslidedown_vx_i16m2(src, offset, vl)
#define __riscv_vslidedown_vx_i16m4(src, offset, vl) __riscv_th_vslidedown_vx_i16m4(src, offset, vl)
#define __riscv_vslidedown_vx_i16m8(src, offset, vl) __riscv_th_vslidedown_vx_i16m8(src, offset, vl)
#define __riscv_vslidedown_vx_i32m1(src, offset, vl) __riscv_th_vslidedown_vx_i32m1(src, offset, vl)
#define __riscv_vslidedown_vx_i32m2(src, offset, vl) __riscv_th_vslidedown_vx_i32m2(src, offset, vl)
#define __riscv_vslidedown_vx_i32m4(src, offset, vl) __riscv_th_vslidedown_vx_i32m4(src, offset, vl)
#define __riscv_vslidedown_vx_i32m8(src, offset, vl) __riscv_th_vslidedown_vx_i32m8(src, offset, vl)
#define __riscv_vslidedown_vx_i64m1(src, offset, vl) __riscv_th_vslidedown_vx_i64m1(src, offset, vl)
#define __riscv_vslidedown_vx_i64m2(src, offset, vl) __riscv_th_vslidedown_vx_i64m2(src, offset, vl)
#define __riscv_vslidedown_vx_i64m4(src, offset, vl) __riscv_th_vslidedown_vx_i64m4(src, offset, vl)
#define __riscv_vslidedown_vx_i64m8(src, offset, vl) __riscv_th_vslidedown_vx_i64m8(src, offset, vl)
#define __riscv_vslidedown_vx_u8m1(src, offset, vl) __riscv_th_vslidedown_vx_u8m1(src, offset, vl)
#define __riscv_vslidedown_vx_u8m2(src, offset, vl) __riscv_th_vslidedown_vx_u8m2(src, offset, vl)
#define __riscv_vslidedown_vx_u8m4(src, offset, vl) __riscv_th_vslidedown_vx_u8m4(src, offset, vl)
#define __riscv_vslidedown_vx_u8m8(src, offset, vl) __riscv_th_vslidedown_vx_u8m8(src, offset, vl)
#define __riscv_vslidedown_vx_u16m1(src, offset, vl) __riscv_th_vslidedown_vx_u16m1(src, offset, vl)
#define __riscv_vslidedown_vx_u16m2(src, offset, vl) __riscv_th_vslidedown_vx_u16m2(src, offset, vl)
#define __riscv_vslidedown_vx_u16m4(src, offset, vl) __riscv_th_vslidedown_vx_u16m4(src, offset, vl)
#define __riscv_vslidedown_vx_u16m8(src, offset, vl) __riscv_th_vslidedown_vx_u16m8(src, offset, vl)
#define __riscv_vslidedown_vx_u32m1(src, offset, vl) __riscv_th_vslidedown_vx_u32m1(src, offset, vl)
#define __riscv_vslidedown_vx_u32m2(src, offset, vl) __riscv_th_vslidedown_vx_u32m2(src, offset, vl)
#define __riscv_vslidedown_vx_u32m4(src, offset, vl) __riscv_th_vslidedown_vx_u32m4(src, offset, vl)
#define __riscv_vslidedown_vx_u32m8(src, offset, vl) __riscv_th_vslidedown_vx_u32m8(src, offset, vl)
#define __riscv_vslidedown_vx_u64m1(src, offset, vl) __riscv_th_vslidedown_vx_u64m1(src, offset, vl)
#define __riscv_vslidedown_vx_u64m2(src, offset, vl) __riscv_th_vslidedown_vx_u64m2(src, offset, vl)
#define __riscv_vslidedown_vx_u64m4(src, offset, vl) __riscv_th_vslidedown_vx_u64m4(src, offset, vl)
#define __riscv_vslidedown_vx_u64m8(src, offset, vl) __riscv_th_vslidedown_vx_u64m8(src, offset, vl)
#define __riscv_vslidedown_vx_f16m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f16m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f16m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f16m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f16m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f16m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f16m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f16m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f32m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f32m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f32m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f32m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f32m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f32m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f32m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f32m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f64m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f64m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f64m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f64m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f64m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f64m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_f64m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_f64m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i8m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i8m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i8m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i8m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i8m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i8m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i8m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i8m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i16m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i16m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i16m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i16m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i16m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i16m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i16m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i16m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i32m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i32m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i32m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i32m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i32m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i32m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i32m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i32m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i64m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i64m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i64m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i64m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i64m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i64m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_i64m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_i64m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u8m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u8m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u8m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u8m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u8m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u8m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u8m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u8m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u16m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u16m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u16m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u16m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u16m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u16m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u16m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u16m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u32m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u32m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u32m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u32m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u32m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u32m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u32m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u32m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u64m1_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u64m1_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u64m2_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u64m2_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u64m4_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u64m4_mu(mask, dest, src, offset, vl)
#define __riscv_vslidedown_vx_u64m8_m(mask, dest, src, offset, vl) __riscv_th_vslidedown_vx_u64m8_mu(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m1(dest, src, offset, vl) __riscv_th_vslideup_vx_f16m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m2(dest, src, offset, vl) __riscv_th_vslideup_vx_f16m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m4(dest, src, offset, vl) __riscv_th_vslideup_vx_f16m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m8(dest, src, offset, vl) __riscv_th_vslideup_vx_f16m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m1(dest, src, offset, vl) __riscv_th_vslideup_vx_f32m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m2(dest, src, offset, vl) __riscv_th_vslideup_vx_f32m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m4(dest, src, offset, vl) __riscv_th_vslideup_vx_f32m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m8(dest, src, offset, vl) __riscv_th_vslideup_vx_f32m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m1(dest, src, offset, vl) __riscv_th_vslideup_vx_f64m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m2(dest, src, offset, vl) __riscv_th_vslideup_vx_f64m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m4(dest, src, offset, vl) __riscv_th_vslideup_vx_f64m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m8(dest, src, offset, vl) __riscv_th_vslideup_vx_f64m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m1(dest, src, offset, vl) __riscv_th_vslideup_vx_i8m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m2(dest, src, offset, vl) __riscv_th_vslideup_vx_i8m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m4(dest, src, offset, vl) __riscv_th_vslideup_vx_i8m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m8(dest, src, offset, vl) __riscv_th_vslideup_vx_i8m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m1(dest, src, offset, vl) __riscv_th_vslideup_vx_i16m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m2(dest, src, offset, vl) __riscv_th_vslideup_vx_i16m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m4(dest, src, offset, vl) __riscv_th_vslideup_vx_i16m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m8(dest, src, offset, vl) __riscv_th_vslideup_vx_i16m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m1(dest, src, offset, vl) __riscv_th_vslideup_vx_i32m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m2(dest, src, offset, vl) __riscv_th_vslideup_vx_i32m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m4(dest, src, offset, vl) __riscv_th_vslideup_vx_i32m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m8(dest, src, offset, vl) __riscv_th_vslideup_vx_i32m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m1(dest, src, offset, vl) __riscv_th_vslideup_vx_i64m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m2(dest, src, offset, vl) __riscv_th_vslideup_vx_i64m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m4(dest, src, offset, vl) __riscv_th_vslideup_vx_i64m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m8(dest, src, offset, vl) __riscv_th_vslideup_vx_i64m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m1(dest, src, offset, vl) __riscv_th_vslideup_vx_u8m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m2(dest, src, offset, vl) __riscv_th_vslideup_vx_u8m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m4(dest, src, offset, vl) __riscv_th_vslideup_vx_u8m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m8(dest, src, offset, vl) __riscv_th_vslideup_vx_u8m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m1(dest, src, offset, vl) __riscv_th_vslideup_vx_u16m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m2(dest, src, offset, vl) __riscv_th_vslideup_vx_u16m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m4(dest, src, offset, vl) __riscv_th_vslideup_vx_u16m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m8(dest, src, offset, vl) __riscv_th_vslideup_vx_u16m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m1(dest, src, offset, vl) __riscv_th_vslideup_vx_u32m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m2(dest, src, offset, vl) __riscv_th_vslideup_vx_u32m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m4(dest, src, offset, vl) __riscv_th_vslideup_vx_u32m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m8(dest, src, offset, vl) __riscv_th_vslideup_vx_u32m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m1(dest, src, offset, vl) __riscv_th_vslideup_vx_u64m1(dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m2(dest, src, offset, vl) __riscv_th_vslideup_vx_u64m2(dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m4(dest, src, offset, vl) __riscv_th_vslideup_vx_u64m4(dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m8(dest, src, offset, vl) __riscv_th_vslideup_vx_u64m8(dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f16m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f16m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f16m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f16m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f16m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f32m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f32m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f32m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f32m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f32m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f64m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f64m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f64m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_f64m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_f64m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i8m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i8m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i8m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i8m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i8m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i16m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i16m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i16m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i16m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i16m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i32m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i32m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i32m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i32m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i32m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i64m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i64m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i64m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_i64m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_i64m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u8m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u8m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u8m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u8m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u8m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u16m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u16m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u16m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u16m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u16m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u32m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u32m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u32m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u32m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u32m8_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m1_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u64m1_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m2_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u64m2_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m4_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u64m4_m(mask, dest, src, offset, vl)
#define __riscv_vslideup_vx_u64m8_m(mask, dest, src, offset, vl) __riscv_th_vslideup_vx_u64m8_m(mask, dest, src, offset, vl)

}] in
def th_vector_permutation_wrapper_macros: RVVHeader;

let HeaderCode = [{
// Vector utility functions
#define __riscv_vundefined_f16m1() __riscv_th_vundefined_f16m1()
#define __riscv_vundefined_f16m2() __riscv_th_vundefined_f16m2()
#define __riscv_vundefined_f16m4() __riscv_th_vundefined_f16m4()
#define __riscv_vundefined_f16m8() __riscv_th_vundefined_f16m8()
#define __riscv_vundefined_f32m1() __riscv_th_vundefined_f32m1()
#define __riscv_vundefined_f32m2() __riscv_th_vundefined_f32m2()
#define __riscv_vundefined_f32m4() __riscv_th_vundefined_f32m4()
#define __riscv_vundefined_f32m8() __riscv_th_vundefined_f32m8()
#define __riscv_vundefined_f64m1() __riscv_th_vundefined_f64m1()
#define __riscv_vundefined_f64m2() __riscv_th_vundefined_f64m2()
#define __riscv_vundefined_f64m4() __riscv_th_vundefined_f64m4()
#define __riscv_vundefined_f64m8() __riscv_th_vundefined_f64m8()
#define __riscv_vundefined_i8m1() __riscv_th_vundefined_i8m1()
#define __riscv_vundefined_i8m2() __riscv_th_vundefined_i8m2()
#define __riscv_vundefined_i8m4() __riscv_th_vundefined_i8m4()
#define __riscv_vundefined_i8m8() __riscv_th_vundefined_i8m8()
#define __riscv_vundefined_i16m1() __riscv_th_vundefined_i16m1()
#define __riscv_vundefined_i16m2() __riscv_th_vundefined_i16m2()
#define __riscv_vundefined_i16m4() __riscv_th_vundefined_i16m4()
#define __riscv_vundefined_i16m8() __riscv_th_vundefined_i16m8()
#define __riscv_vundefined_i32m1() __riscv_th_vundefined_i32m1()
#define __riscv_vundefined_i32m2() __riscv_th_vundefined_i32m2()
#define __riscv_vundefined_i32m4() __riscv_th_vundefined_i32m4()
#define __riscv_vundefined_i32m8() __riscv_th_vundefined_i32m8()
#define __riscv_vundefined_i64m1() __riscv_th_vundefined_i64m1()
#define __riscv_vundefined_i64m2() __riscv_th_vundefined_i64m2()
#define __riscv_vundefined_i64m4() __riscv_th_vundefined_i64m4()
#define __riscv_vundefined_i64m8() __riscv_th_vundefined_i64m8()
#define __riscv_vundefined_u8m1() __riscv_th_vundefined_u8m1()
#define __riscv_vundefined_u8m2() __riscv_th_vundefined_u8m2()
#define __riscv_vundefined_u8m4() __riscv_th_vundefined_u8m4()
#define __riscv_vundefined_u8m8() __riscv_th_vundefined_u8m8()
#define __riscv_vundefined_u16m1() __riscv_th_vundefined_u16m1()
#define __riscv_vundefined_u16m2() __riscv_th_vundefined_u16m2()
#define __riscv_vundefined_u16m4() __riscv_th_vundefined_u16m4()
#define __riscv_vundefined_u16m8() __riscv_th_vundefined_u16m8()
#define __riscv_vundefined_u32m1() __riscv_th_vundefined_u32m1()
#define __riscv_vundefined_u32m2() __riscv_th_vundefined_u32m2()
#define __riscv_vundefined_u32m4() __riscv_th_vundefined_u32m4()
#define __riscv_vundefined_u32m8() __riscv_th_vundefined_u32m8()
#define __riscv_vundefined_u64m1() __riscv_th_vundefined_u64m1()
#define __riscv_vundefined_u64m2() __riscv_th_vundefined_u64m2()
#define __riscv_vundefined_u64m4() __riscv_th_vundefined_u64m4()
#define __riscv_vundefined_u64m8() __riscv_th_vundefined_u64m8()

#define __riscv_vreinterpret_v_i8m1_u8m1(src) __riscv_th_vreinterpret_v_i8m1_u8m1(src)
#define __riscv_vreinterpret_v_i8m2_u8m2(src) __riscv_th_vreinterpret_v_i8m2_u8m2(src)
#define __riscv_vreinterpret_v_i8m4_u8m4(src) __riscv_th_vreinterpret_v_i8m4_u8m4(src)
#define __riscv_vreinterpret_v_i8m8_u8m8(src) __riscv_th_vreinterpret_v_i8m8_u8m8(src)
#define __riscv_vreinterpret_v_u8m1_i8m1(src) __riscv_th_vreinterpret_v_u8m1_i8m1(src)
#define __riscv_vreinterpret_v_u8m2_i8m2(src) __riscv_th_vreinterpret_v_u8m2_i8m2(src)
#define __riscv_vreinterpret_v_u8m4_i8m4(src) __riscv_th_vreinterpret_v_u8m4_i8m4(src)
#define __riscv_vreinterpret_v_u8m8_i8m8(src) __riscv_th_vreinterpret_v_u8m8_i8m8(src)
#define __riscv_vreinterpret_v_i16m1_f16m1(src) __riscv_th_vreinterpret_v_i16m1_f16m1(src)
#define __riscv_vreinterpret_v_i16m2_f16m2(src) __riscv_th_vreinterpret_v_i16m2_f16m2(src)
#define __riscv_vreinterpret_v_i16m4_f16m4(src) __riscv_th_vreinterpret_v_i16m4_f16m4(src)
#define __riscv_vreinterpret_v_i16m8_f16m8(src) __riscv_th_vreinterpret_v_i16m8_f16m8(src)
#define __riscv_vreinterpret_v_u16m1_f16m1(src) __riscv_th_vreinterpret_v_u16m1_f16m1(src)
#define __riscv_vreinterpret_v_u16m2_f16m2(src) __riscv_th_vreinterpret_v_u16m2_f16m2(src)
#define __riscv_vreinterpret_v_u16m4_f16m4(src) __riscv_th_vreinterpret_v_u16m4_f16m4(src)
#define __riscv_vreinterpret_v_u16m8_f16m8(src) __riscv_th_vreinterpret_v_u16m8_f16m8(src)
#define __riscv_vreinterpret_v_i16m1_u16m1(src) __riscv_th_vreinterpret_v_i16m1_u16m1(src)
#define __riscv_vreinterpret_v_i16m2_u16m2(src) __riscv_th_vreinterpret_v_i16m2_u16m2(src)
#define __riscv_vreinterpret_v_i16m4_u16m4(src) __riscv_th_vreinterpret_v_i16m4_u16m4(src)
#define __riscv_vreinterpret_v_i16m8_u16m8(src) __riscv_th_vreinterpret_v_i16m8_u16m8(src)
#define __riscv_vreinterpret_v_u16m1_i16m1(src) __riscv_th_vreinterpret_v_u16m1_i16m1(src)
#define __riscv_vreinterpret_v_u16m2_i16m2(src) __riscv_th_vreinterpret_v_u16m2_i16m2(src)
#define __riscv_vreinterpret_v_u16m4_i16m4(src) __riscv_th_vreinterpret_v_u16m4_i16m4(src)
#define __riscv_vreinterpret_v_u16m8_i16m8(src) __riscv_th_vreinterpret_v_u16m8_i16m8(src)
#define __riscv_vreinterpret_v_f16m1_i16m1(src) __riscv_th_vreinterpret_v_f16m1_i16m1(src)
#define __riscv_vreinterpret_v_f16m2_i16m2(src) __riscv_th_vreinterpret_v_f16m2_i16m2(src)
#define __riscv_vreinterpret_v_f16m4_i16m4(src) __riscv_th_vreinterpret_v_f16m4_i16m4(src)
#define __riscv_vreinterpret_v_f16m8_i16m8(src) __riscv_th_vreinterpret_v_f16m8_i16m8(src)
#define __riscv_vreinterpret_v_f16m1_u16m1(src) __riscv_th_vreinterpret_v_f16m1_u16m1(src)
#define __riscv_vreinterpret_v_f16m2_u16m2(src) __riscv_th_vreinterpret_v_f16m2_u16m2(src)
#define __riscv_vreinterpret_v_f16m4_u16m4(src) __riscv_th_vreinterpret_v_f16m4_u16m4(src)
#define __riscv_vreinterpret_v_f16m8_u16m8(src) __riscv_th_vreinterpret_v_f16m8_u16m8(src)
#define __riscv_vreinterpret_v_i32m1_f32m1(src) __riscv_th_vreinterpret_v_i32m1_f32m1(src)
#define __riscv_vreinterpret_v_i32m2_f32m2(src) __riscv_th_vreinterpret_v_i32m2_f32m2(src)
#define __riscv_vreinterpret_v_i32m4_f32m4(src) __riscv_th_vreinterpret_v_i32m4_f32m4(src)
#define __riscv_vreinterpret_v_i32m8_f32m8(src) __riscv_th_vreinterpret_v_i32m8_f32m8(src)
#define __riscv_vreinterpret_v_u32m1_f32m1(src) __riscv_th_vreinterpret_v_u32m1_f32m1(src)
#define __riscv_vreinterpret_v_u32m2_f32m2(src) __riscv_th_vreinterpret_v_u32m2_f32m2(src)
#define __riscv_vreinterpret_v_u32m4_f32m4(src) __riscv_th_vreinterpret_v_u32m4_f32m4(src)
#define __riscv_vreinterpret_v_u32m8_f32m8(src) __riscv_th_vreinterpret_v_u32m8_f32m8(src)
#define __riscv_vreinterpret_v_i32m1_u32m1(src) __riscv_th_vreinterpret_v_i32m1_u32m1(src)
#define __riscv_vreinterpret_v_i32m2_u32m2(src) __riscv_th_vreinterpret_v_i32m2_u32m2(src)
#define __riscv_vreinterpret_v_i32m4_u32m4(src) __riscv_th_vreinterpret_v_i32m4_u32m4(src)
#define __riscv_vreinterpret_v_i32m8_u32m8(src) __riscv_th_vreinterpret_v_i32m8_u32m8(src)
#define __riscv_vreinterpret_v_u32m1_i32m1(src) __riscv_th_vreinterpret_v_u32m1_i32m1(src)
#define __riscv_vreinterpret_v_u32m2_i32m2(src) __riscv_th_vreinterpret_v_u32m2_i32m2(src)
#define __riscv_vreinterpret_v_u32m4_i32m4(src) __riscv_th_vreinterpret_v_u32m4_i32m4(src)
#define __riscv_vreinterpret_v_u32m8_i32m8(src) __riscv_th_vreinterpret_v_u32m8_i32m8(src)
#define __riscv_vreinterpret_v_f32m1_i32m1(src) __riscv_th_vreinterpret_v_f32m1_i32m1(src)
#define __riscv_vreinterpret_v_f32m2_i32m2(src) __riscv_th_vreinterpret_v_f32m2_i32m2(src)
#define __riscv_vreinterpret_v_f32m4_i32m4(src) __riscv_th_vreinterpret_v_f32m4_i32m4(src)
#define __riscv_vreinterpret_v_f32m8_i32m8(src) __riscv_th_vreinterpret_v_f32m8_i32m8(src)
#define __riscv_vreinterpret_v_f32m1_u32m1(src) __riscv_th_vreinterpret_v_f32m1_u32m1(src)
#define __riscv_vreinterpret_v_f32m2_u32m2(src) __riscv_th_vreinterpret_v_f32m2_u32m2(src)
#define __riscv_vreinterpret_v_f32m4_u32m4(src) __riscv_th_vreinterpret_v_f32m4_u32m4(src)
#define __riscv_vreinterpret_v_f32m8_u32m8(src) __riscv_th_vreinterpret_v_f32m8_u32m8(src)
#define __riscv_vreinterpret_v_i64m1_f64m1(src) __riscv_th_vreinterpret_v_i64m1_f64m1(src)
#define __riscv_vreinterpret_v_i64m2_f64m2(src) __riscv_th_vreinterpret_v_i64m2_f64m2(src)
#define __riscv_vreinterpret_v_i64m4_f64m4(src) __riscv_th_vreinterpret_v_i64m4_f64m4(src)
#define __riscv_vreinterpret_v_i64m8_f64m8(src) __riscv_th_vreinterpret_v_i64m8_f64m8(src)
#define __riscv_vreinterpret_v_u64m1_f64m1(src) __riscv_th_vreinterpret_v_u64m1_f64m1(src)
#define __riscv_vreinterpret_v_u64m2_f64m2(src) __riscv_th_vreinterpret_v_u64m2_f64m2(src)
#define __riscv_vreinterpret_v_u64m4_f64m4(src) __riscv_th_vreinterpret_v_u64m4_f64m4(src)
#define __riscv_vreinterpret_v_u64m8_f64m8(src) __riscv_th_vreinterpret_v_u64m8_f64m8(src)
#define __riscv_vreinterpret_v_i64m1_u64m1(src) __riscv_th_vreinterpret_v_i64m1_u64m1(src)
#define __riscv_vreinterpret_v_i64m2_u64m2(src) __riscv_th_vreinterpret_v_i64m2_u64m2(src)
#define __riscv_vreinterpret_v_i64m4_u64m4(src) __riscv_th_vreinterpret_v_i64m4_u64m4(src)
#define __riscv_vreinterpret_v_i64m8_u64m8(src) __riscv_th_vreinterpret_v_i64m8_u64m8(src)
#define __riscv_vreinterpret_v_u64m1_i64m1(src) __riscv_th_vreinterpret_v_u64m1_i64m1(src)
#define __riscv_vreinterpret_v_u64m2_i64m2(src) __riscv_th_vreinterpret_v_u64m2_i64m2(src)
#define __riscv_vreinterpret_v_u64m4_i64m4(src) __riscv_th_vreinterpret_v_u64m4_i64m4(src)
#define __riscv_vreinterpret_v_u64m8_i64m8(src) __riscv_th_vreinterpret_v_u64m8_i64m8(src)
#define __riscv_vreinterpret_v_f64m1_i64m1(src) __riscv_th_vreinterpret_v_f64m1_i64m1(src)
#define __riscv_vreinterpret_v_f64m2_i64m2(src) __riscv_th_vreinterpret_v_f64m2_i64m2(src)
#define __riscv_vreinterpret_v_f64m4_i64m4(src) __riscv_th_vreinterpret_v_f64m4_i64m4(src)
#define __riscv_vreinterpret_v_f64m8_i64m8(src) __riscv_th_vreinterpret_v_f64m8_i64m8(src)
#define __riscv_vreinterpret_v_f64m1_u64m1(src) __riscv_th_vreinterpret_v_f64m1_u64m1(src)
#define __riscv_vreinterpret_v_f64m2_u64m2(src) __riscv_th_vreinterpret_v_f64m2_u64m2(src)
#define __riscv_vreinterpret_v_f64m4_u64m4(src) __riscv_th_vreinterpret_v_f64m4_u64m4(src)
#define __riscv_vreinterpret_v_f64m8_u64m8(src) __riscv_th_vreinterpret_v_f64m8_u64m8(src)

#define __riscv_vreinterpret_v_i8m1_i16m1(src) __riscv_th_vreinterpret_v_i8m1_i16m1(src)
#define __riscv_vreinterpret_v_i8m2_i16m2(src) __riscv_th_vreinterpret_v_i8m2_i16m2(src)
#define __riscv_vreinterpret_v_i8m4_i16m4(src) __riscv_th_vreinterpret_v_i8m4_i16m4(src)
#define __riscv_vreinterpret_v_i8m8_i16m8(src) __riscv_th_vreinterpret_v_i8m8_i16m8(src)
#define __riscv_vreinterpret_v_u8m1_u16m1(src) __riscv_th_vreinterpret_v_u8m1_u16m1(src)
#define __riscv_vreinterpret_v_u8m2_u16m2(src) __riscv_th_vreinterpret_v_u8m2_u16m2(src)
#define __riscv_vreinterpret_v_u8m4_u16m4(src) __riscv_th_vreinterpret_v_u8m4_u16m4(src)
#define __riscv_vreinterpret_v_u8m8_u16m8(src) __riscv_th_vreinterpret_v_u8m8_u16m8(src)
#define __riscv_vreinterpret_v_i8m1_i32m1(src) __riscv_th_vreinterpret_v_i8m1_i32m1(src)
#define __riscv_vreinterpret_v_i8m2_i32m2(src) __riscv_th_vreinterpret_v_i8m2_i32m2(src)
#define __riscv_vreinterpret_v_i8m4_i32m4(src) __riscv_th_vreinterpret_v_i8m4_i32m4(src)
#define __riscv_vreinterpret_v_i8m8_i32m8(src) __riscv_th_vreinterpret_v_i8m8_i32m8(src)
#define __riscv_vreinterpret_v_u8m1_u32m1(src) __riscv_th_vreinterpret_v_u8m1_u32m1(src)
#define __riscv_vreinterpret_v_u8m2_u32m2(src) __riscv_th_vreinterpret_v_u8m2_u32m2(src)
#define __riscv_vreinterpret_v_u8m4_u32m4(src) __riscv_th_vreinterpret_v_u8m4_u32m4(src)
#define __riscv_vreinterpret_v_u8m8_u32m8(src) __riscv_th_vreinterpret_v_u8m8_u32m8(src)
#define __riscv_vreinterpret_v_i8m1_i64m1(src) __riscv_th_vreinterpret_v_i8m1_i64m1(src)
#define __riscv_vreinterpret_v_i8m2_i64m2(src) __riscv_th_vreinterpret_v_i8m2_i64m2(src)
#define __riscv_vreinterpret_v_i8m4_i64m4(src) __riscv_th_vreinterpret_v_i8m4_i64m4(src)
#define __riscv_vreinterpret_v_i8m8_i64m8(src) __riscv_th_vreinterpret_v_i8m8_i64m8(src)
#define __riscv_vreinterpret_v_u8m1_u64m1(src) __riscv_th_vreinterpret_v_u8m1_u64m1(src)
#define __riscv_vreinterpret_v_u8m2_u64m2(src) __riscv_th_vreinterpret_v_u8m2_u64m2(src)
#define __riscv_vreinterpret_v_u8m4_u64m4(src) __riscv_th_vreinterpret_v_u8m4_u64m4(src)
#define __riscv_vreinterpret_v_u8m8_u64m8(src) __riscv_th_vreinterpret_v_u8m8_u64m8(src)
#define __riscv_vreinterpret_v_i16m1_i8m1(src) __riscv_th_vreinterpret_v_i16m1_i8m1(src)
#define __riscv_vreinterpret_v_i16m2_i8m2(src) __riscv_th_vreinterpret_v_i16m2_i8m2(src)
#define __riscv_vreinterpret_v_i16m4_i8m4(src) __riscv_th_vreinterpret_v_i16m4_i8m4(src)
#define __riscv_vreinterpret_v_i16m8_i8m8(src) __riscv_th_vreinterpret_v_i16m8_i8m8(src)
#define __riscv_vreinterpret_v_u16m1_u8m1(src) __riscv_th_vreinterpret_v_u16m1_u8m1(src)
#define __riscv_vreinterpret_v_u16m2_u8m2(src) __riscv_th_vreinterpret_v_u16m2_u8m2(src)
#define __riscv_vreinterpret_v_u16m4_u8m4(src) __riscv_th_vreinterpret_v_u16m4_u8m4(src)
#define __riscv_vreinterpret_v_u16m8_u8m8(src) __riscv_th_vreinterpret_v_u16m8_u8m8(src)
#define __riscv_vreinterpret_v_i16m1_i32m1(src) __riscv_th_vreinterpret_v_i16m1_i32m1(src)
#define __riscv_vreinterpret_v_i16m2_i32m2(src) __riscv_th_vreinterpret_v_i16m2_i32m2(src)
#define __riscv_vreinterpret_v_i16m4_i32m4(src) __riscv_th_vreinterpret_v_i16m4_i32m4(src)
#define __riscv_vreinterpret_v_i16m8_i32m8(src) __riscv_th_vreinterpret_v_i16m8_i32m8(src)
#define __riscv_vreinterpret_v_u16m1_u32m1(src) __riscv_th_vreinterpret_v_u16m1_u32m1(src)
#define __riscv_vreinterpret_v_u16m2_u32m2(src) __riscv_th_vreinterpret_v_u16m2_u32m2(src)
#define __riscv_vreinterpret_v_u16m4_u32m4(src) __riscv_th_vreinterpret_v_u16m4_u32m4(src)
#define __riscv_vreinterpret_v_u16m8_u32m8(src) __riscv_th_vreinterpret_v_u16m8_u32m8(src)
#define __riscv_vreinterpret_v_i16m1_i64m1(src) __riscv_th_vreinterpret_v_i16m1_i64m1(src)
#define __riscv_vreinterpret_v_i16m2_i64m2(src) __riscv_th_vreinterpret_v_i16m2_i64m2(src)
#define __riscv_vreinterpret_v_i16m4_i64m4(src) __riscv_th_vreinterpret_v_i16m4_i64m4(src)
#define __riscv_vreinterpret_v_i16m8_i64m8(src) __riscv_th_vreinterpret_v_i16m8_i64m8(src)
#define __riscv_vreinterpret_v_u16m1_u64m1(src) __riscv_th_vreinterpret_v_u16m1_u64m1(src)
#define __riscv_vreinterpret_v_u16m2_u64m2(src) __riscv_th_vreinterpret_v_u16m2_u64m2(src)
#define __riscv_vreinterpret_v_u16m4_u64m4(src) __riscv_th_vreinterpret_v_u16m4_u64m4(src)
#define __riscv_vreinterpret_v_u16m8_u64m8(src) __riscv_th_vreinterpret_v_u16m8_u64m8(src)
#define __riscv_vreinterpret_v_i32m1_i8m1(src) __riscv_th_vreinterpret_v_i32m1_i8m1(src)
#define __riscv_vreinterpret_v_i32m2_i8m2(src) __riscv_th_vreinterpret_v_i32m2_i8m2(src)
#define __riscv_vreinterpret_v_i32m4_i8m4(src) __riscv_th_vreinterpret_v_i32m4_i8m4(src)
#define __riscv_vreinterpret_v_i32m8_i8m8(src) __riscv_th_vreinterpret_v_i32m8_i8m8(src)
#define __riscv_vreinterpret_v_u32m1_u8m1(src) __riscv_th_vreinterpret_v_u32m1_u8m1(src)
#define __riscv_vreinterpret_v_u32m2_u8m2(src) __riscv_th_vreinterpret_v_u32m2_u8m2(src)
#define __riscv_vreinterpret_v_u32m4_u8m4(src) __riscv_th_vreinterpret_v_u32m4_u8m4(src)
#define __riscv_vreinterpret_v_u32m8_u8m8(src) __riscv_th_vreinterpret_v_u32m8_u8m8(src)
#define __riscv_vreinterpret_v_i32m1_i16m1(src) __riscv_th_vreinterpret_v_i32m1_i16m1(src)
#define __riscv_vreinterpret_v_i32m2_i16m2(src) __riscv_th_vreinterpret_v_i32m2_i16m2(src)
#define __riscv_vreinterpret_v_i32m4_i16m4(src) __riscv_th_vreinterpret_v_i32m4_i16m4(src)
#define __riscv_vreinterpret_v_i32m8_i16m8(src) __riscv_th_vreinterpret_v_i32m8_i16m8(src)
#define __riscv_vreinterpret_v_u32m1_u16m1(src) __riscv_th_vreinterpret_v_u32m1_u16m1(src)
#define __riscv_vreinterpret_v_u32m2_u16m2(src) __riscv_th_vreinterpret_v_u32m2_u16m2(src)
#define __riscv_vreinterpret_v_u32m4_u16m4(src) __riscv_th_vreinterpret_v_u32m4_u16m4(src)
#define __riscv_vreinterpret_v_u32m8_u16m8(src) __riscv_th_vreinterpret_v_u32m8_u16m8(src)
#define __riscv_vreinterpret_v_i32m1_i64m1(src) __riscv_th_vreinterpret_v_i32m1_i64m1(src)
#define __riscv_vreinterpret_v_i32m2_i64m2(src) __riscv_th_vreinterpret_v_i32m2_i64m2(src)
#define __riscv_vreinterpret_v_i32m4_i64m4(src) __riscv_th_vreinterpret_v_i32m4_i64m4(src)
#define __riscv_vreinterpret_v_i32m8_i64m8(src) __riscv_th_vreinterpret_v_i32m8_i64m8(src)
#define __riscv_vreinterpret_v_u32m1_u64m1(src) __riscv_th_vreinterpret_v_u32m1_u64m1(src)
#define __riscv_vreinterpret_v_u32m2_u64m2(src) __riscv_th_vreinterpret_v_u32m2_u64m2(src)
#define __riscv_vreinterpret_v_u32m4_u64m4(src) __riscv_th_vreinterpret_v_u32m4_u64m4(src)
#define __riscv_vreinterpret_v_u32m8_u64m8(src) __riscv_th_vreinterpret_v_u32m8_u64m8(src)
#define __riscv_vreinterpret_v_i64m1_i8m1(src) __riscv_th_vreinterpret_v_i64m1_i8m1(src)
#define __riscv_vreinterpret_v_i64m2_i8m2(src) __riscv_th_vreinterpret_v_i64m2_i8m2(src)
#define __riscv_vreinterpret_v_i64m4_i8m4(src) __riscv_th_vreinterpret_v_i64m4_i8m4(src)
#define __riscv_vreinterpret_v_i64m8_i8m8(src) __riscv_th_vreinterpret_v_i64m8_i8m8(src)
#define __riscv_vreinterpret_v_u64m1_u8m1(src) __riscv_th_vreinterpret_v_u64m1_u8m1(src)
#define __riscv_vreinterpret_v_u64m2_u8m2(src) __riscv_th_vreinterpret_v_u64m2_u8m2(src)
#define __riscv_vreinterpret_v_u64m4_u8m4(src) __riscv_th_vreinterpret_v_u64m4_u8m4(src)
#define __riscv_vreinterpret_v_u64m8_u8m8(src) __riscv_th_vreinterpret_v_u64m8_u8m8(src)
#define __riscv_vreinterpret_v_i64m1_i16m1(src) __riscv_th_vreinterpret_v_i64m1_i16m1(src)
#define __riscv_vreinterpret_v_i64m2_i16m2(src) __riscv_th_vreinterpret_v_i64m2_i16m2(src)
#define __riscv_vreinterpret_v_i64m4_i16m4(src) __riscv_th_vreinterpret_v_i64m4_i16m4(src)
#define __riscv_vreinterpret_v_i64m8_i16m8(src) __riscv_th_vreinterpret_v_i64m8_i16m8(src)
#define __riscv_vreinterpret_v_u64m1_u16m1(src) __riscv_th_vreinterpret_v_u64m1_u16m1(src)
#define __riscv_vreinterpret_v_u64m2_u16m2(src) __riscv_th_vreinterpret_v_u64m2_u16m2(src)
#define __riscv_vreinterpret_v_u64m4_u16m4(src) __riscv_th_vreinterpret_v_u64m4_u16m4(src)
#define __riscv_vreinterpret_v_u64m8_u16m8(src) __riscv_th_vreinterpret_v_u64m8_u16m8(src)
#define __riscv_vreinterpret_v_i64m1_i32m1(src) __riscv_th_vreinterpret_v_i64m1_i32m1(src)
#define __riscv_vreinterpret_v_i64m2_i32m2(src) __riscv_th_vreinterpret_v_i64m2_i32m2(src)
#define __riscv_vreinterpret_v_i64m4_i32m4(src) __riscv_th_vreinterpret_v_i64m4_i32m4(src)
#define __riscv_vreinterpret_v_i64m8_i32m8(src) __riscv_th_vreinterpret_v_i64m8_i32m8(src)
#define __riscv_vreinterpret_v_u64m1_u32m1(src) __riscv_th_vreinterpret_v_u64m1_u32m1(src)
#define __riscv_vreinterpret_v_u64m2_u32m2(src) __riscv_th_vreinterpret_v_u64m2_u32m2(src)
#define __riscv_vreinterpret_v_u64m4_u32m4(src) __riscv_th_vreinterpret_v_u64m4_u32m4(src)
#define __riscv_vreinterpret_v_u64m8_u32m8(src) __riscv_th_vreinterpret_v_u64m8_u32m8(src)

#define __riscv_vreinterpret_v_i8m1_b64(src) __riscv_th_vreinterpret_v_i8m1_b64(src)
#define __riscv_vreinterpret_v_b64_i8m1(src) __riscv_th_vreinterpret_v_b64_i8m1(src)
#define __riscv_vreinterpret_v_i8m1_b32(src) __riscv_th_vreinterpret_v_i8m1_b32(src)
#define __riscv_vreinterpret_v_b32_i8m1(src) __riscv_th_vreinterpret_v_b32_i8m1(src)
#define __riscv_vreinterpret_v_i8m1_b16(src) __riscv_th_vreinterpret_v_i8m1_b16(src)
#define __riscv_vreinterpret_v_b16_i8m1(src) __riscv_th_vreinterpret_v_b16_i8m1(src)
#define __riscv_vreinterpret_v_i8m1_b8(src) __riscv_th_vreinterpret_v_i8m1_b8(src)
#define __riscv_vreinterpret_v_b8_i8m1(src) __riscv_th_vreinterpret_v_b8_i8m1(src)
#define __riscv_vreinterpret_v_i8m1_b4(src) __riscv_th_vreinterpret_v_i8m1_b4(src)
#define __riscv_vreinterpret_v_b4_i8m1(src) __riscv_th_vreinterpret_v_b4_i8m1(src)
#define __riscv_vreinterpret_v_i8m1_b2(src) __riscv_th_vreinterpret_v_i8m1_b2(src)
#define __riscv_vreinterpret_v_b2_i8m1(src) __riscv_th_vreinterpret_v_b2_i8m1(src)
#define __riscv_vreinterpret_v_i8m1_b1(src) __riscv_th_vreinterpret_v_i8m1_b1(src)
#define __riscv_vreinterpret_v_b1_i8m1(src) __riscv_th_vreinterpret_v_b1_i8m1(src)
#define __riscv_vreinterpret_v_u8m1_b64(src) __riscv_th_vreinterpret_v_u8m1_b64(src)
#define __riscv_vreinterpret_v_b64_u8m1(src) __riscv_th_vreinterpret_v_b64_u8m1(src)
#define __riscv_vreinterpret_v_u8m1_b32(src) __riscv_th_vreinterpret_v_u8m1_b32(src)
#define __riscv_vreinterpret_v_b32_u8m1(src) __riscv_th_vreinterpret_v_b32_u8m1(src)
#define __riscv_vreinterpret_v_u8m1_b16(src) __riscv_th_vreinterpret_v_u8m1_b16(src)
#define __riscv_vreinterpret_v_b16_u8m1(src) __riscv_th_vreinterpret_v_b16_u8m1(src)
#define __riscv_vreinterpret_v_u8m1_b8(src) __riscv_th_vreinterpret_v_u8m1_b8(src)
#define __riscv_vreinterpret_v_b8_u8m1(src) __riscv_th_vreinterpret_v_b8_u8m1(src)
#define __riscv_vreinterpret_v_u8m1_b4(src) __riscv_th_vreinterpret_v_u8m1_b4(src)
#define __riscv_vreinterpret_v_b4_u8m1(src) __riscv_th_vreinterpret_v_b4_u8m1(src)
#define __riscv_vreinterpret_v_u8m1_b2(src) __riscv_th_vreinterpret_v_u8m1_b2(src)
#define __riscv_vreinterpret_v_b2_u8m1(src) __riscv_th_vreinterpret_v_b2_u8m1(src)
#define __riscv_vreinterpret_v_u8m1_b1(src) __riscv_th_vreinterpret_v_u8m1_b1(src)
#define __riscv_vreinterpret_v_b1_u8m1(src) __riscv_th_vreinterpret_v_b1_u8m1(src)
#define __riscv_vreinterpret_v_i16m1_b64(src) __riscv_th_vreinterpret_v_i16m1_b64(src)
#define __riscv_vreinterpret_v_b64_i16m1(src) __riscv_th_vreinterpret_v_b64_i16m1(src)
#define __riscv_vreinterpret_v_i16m1_b32(src) __riscv_th_vreinterpret_v_i16m1_b32(src)
#define __riscv_vreinterpret_v_b32_i16m1(src) __riscv_th_vreinterpret_v_b32_i16m1(src)
#define __riscv_vreinterpret_v_i16m1_b16(src) __riscv_th_vreinterpret_v_i16m1_b16(src)
#define __riscv_vreinterpret_v_b16_i16m1(src) __riscv_th_vreinterpret_v_b16_i16m1(src)
#define __riscv_vreinterpret_v_i16m1_b8(src) __riscv_th_vreinterpret_v_i16m1_b8(src)
#define __riscv_vreinterpret_v_b8_i16m1(src) __riscv_th_vreinterpret_v_b8_i16m1(src)
#define __riscv_vreinterpret_v_i16m1_b4(src) __riscv_th_vreinterpret_v_i16m1_b4(src)
#define __riscv_vreinterpret_v_b4_i16m1(src) __riscv_th_vreinterpret_v_b4_i16m1(src)
#define __riscv_vreinterpret_v_i16m1_b2(src) __riscv_th_vreinterpret_v_i16m1_b2(src)
#define __riscv_vreinterpret_v_b2_i16m1(src) __riscv_th_vreinterpret_v_b2_i16m1(src)
#define __riscv_vreinterpret_v_u16m1_b64(src) __riscv_th_vreinterpret_v_u16m1_b64(src)
#define __riscv_vreinterpret_v_b64_u16m1(src) __riscv_th_vreinterpret_v_b64_u16m1(src)
#define __riscv_vreinterpret_v_u16m1_b32(src) __riscv_th_vreinterpret_v_u16m1_b32(src)
#define __riscv_vreinterpret_v_b32_u16m1(src) __riscv_th_vreinterpret_v_b32_u16m1(src)
#define __riscv_vreinterpret_v_u16m1_b16(src) __riscv_th_vreinterpret_v_u16m1_b16(src)
#define __riscv_vreinterpret_v_b16_u16m1(src) __riscv_th_vreinterpret_v_b16_u16m1(src)
#define __riscv_vreinterpret_v_u16m1_b8(src) __riscv_th_vreinterpret_v_u16m1_b8(src)
#define __riscv_vreinterpret_v_b8_u16m1(src) __riscv_th_vreinterpret_v_b8_u16m1(src)
#define __riscv_vreinterpret_v_u16m1_b4(src) __riscv_th_vreinterpret_v_u16m1_b4(src)
#define __riscv_vreinterpret_v_b4_u16m1(src) __riscv_th_vreinterpret_v_b4_u16m1(src)
#define __riscv_vreinterpret_v_u16m1_b2(src) __riscv_th_vreinterpret_v_u16m1_b2(src)
#define __riscv_vreinterpret_v_b2_u16m1(src) __riscv_th_vreinterpret_v_b2_u16m1(src)
#define __riscv_vreinterpret_v_i32m1_b64(src) __riscv_th_vreinterpret_v_i32m1_b64(src)
#define __riscv_vreinterpret_v_b64_i32m1(src) __riscv_th_vreinterpret_v_b64_i32m1(src)
#define __riscv_vreinterpret_v_i32m1_b32(src) __riscv_th_vreinterpret_v_i32m1_b32(src)
#define __riscv_vreinterpret_v_b32_i32m1(src) __riscv_th_vreinterpret_v_b32_i32m1(src)
#define __riscv_vreinterpret_v_i32m1_b16(src) __riscv_th_vreinterpret_v_i32m1_b16(src)
#define __riscv_vreinterpret_v_b16_i32m1(src) __riscv_th_vreinterpret_v_b16_i32m1(src)
#define __riscv_vreinterpret_v_i32m1_b8(src) __riscv_th_vreinterpret_v_i32m1_b8(src)
#define __riscv_vreinterpret_v_b8_i32m1(src) __riscv_th_vreinterpret_v_b8_i32m1(src)
#define __riscv_vreinterpret_v_i32m1_b4(src) __riscv_th_vreinterpret_v_i32m1_b4(src)
#define __riscv_vreinterpret_v_b4_i32m1(src) __riscv_th_vreinterpret_v_b4_i32m1(src)
#define __riscv_vreinterpret_v_u32m1_b64(src) __riscv_th_vreinterpret_v_u32m1_b64(src)
#define __riscv_vreinterpret_v_b64_u32m1(src) __riscv_th_vreinterpret_v_b64_u32m1(src)
#define __riscv_vreinterpret_v_u32m1_b32(src) __riscv_th_vreinterpret_v_u32m1_b32(src)
#define __riscv_vreinterpret_v_b32_u32m1(src) __riscv_th_vreinterpret_v_b32_u32m1(src)
#define __riscv_vreinterpret_v_u32m1_b16(src) __riscv_th_vreinterpret_v_u32m1_b16(src)
#define __riscv_vreinterpret_v_b16_u32m1(src) __riscv_th_vreinterpret_v_b16_u32m1(src)
#define __riscv_vreinterpret_v_u32m1_b8(src) __riscv_th_vreinterpret_v_u32m1_b8(src)
#define __riscv_vreinterpret_v_b8_u32m1(src) __riscv_th_vreinterpret_v_b8_u32m1(src)
#define __riscv_vreinterpret_v_u32m1_b4(src) __riscv_th_vreinterpret_v_u32m1_b4(src)
#define __riscv_vreinterpret_v_b4_u32m1(src) __riscv_th_vreinterpret_v_b4_u32m1(src)
#define __riscv_vreinterpret_v_i64m1_b64(src) __riscv_th_vreinterpret_v_i64m1_b64(src)
#define __riscv_vreinterpret_v_b64_i64m1(src) __riscv_th_vreinterpret_v_b64_i64m1(src)
#define __riscv_vreinterpret_v_i64m1_b32(src) __riscv_th_vreinterpret_v_i64m1_b32(src)
#define __riscv_vreinterpret_v_b32_i64m1(src) __riscv_th_vreinterpret_v_b32_i64m1(src)
#define __riscv_vreinterpret_v_i64m1_b16(src) __riscv_th_vreinterpret_v_i64m1_b16(src)
#define __riscv_vreinterpret_v_b16_i64m1(src) __riscv_th_vreinterpret_v_b16_i64m1(src)
#define __riscv_vreinterpret_v_i64m1_b8(src) __riscv_th_vreinterpret_v_i64m1_b8(src)
#define __riscv_vreinterpret_v_b8_i64m1(src) __riscv_th_vreinterpret_v_b8_i64m1(src)
#define __riscv_vreinterpret_v_u64m1_b64(src) __riscv_th_vreinterpret_v_u64m1_b64(src)
#define __riscv_vreinterpret_v_b64_u64m1(src) __riscv_th_vreinterpret_v_b64_u64m1(src)
#define __riscv_vreinterpret_v_u64m1_b32(src) __riscv_th_vreinterpret_v_u64m1_b32(src)
#define __riscv_vreinterpret_v_b32_u64m1(src) __riscv_th_vreinterpret_v_b32_u64m1(src)
#define __riscv_vreinterpret_v_u64m1_b16(src) __riscv_th_vreinterpret_v_u64m1_b16(src)
#define __riscv_vreinterpret_v_b16_u64m1(src) __riscv_th_vreinterpret_v_b16_u64m1(src)
#define __riscv_vreinterpret_v_u64m1_b8(src) __riscv_th_vreinterpret_v_u64m1_b8(src)
#define __riscv_vreinterpret_v_b8_u64m1(src) __riscv_th_vreinterpret_v_b8_u64m1(src)

#define __riscv_vlmul_ext_v_f16m1_f16m2(op1) __riscv_th_vlmul_ext_v_f16m1_f16m2(op1)
#define __riscv_vlmul_ext_v_f16m1_f16m4(op1) __riscv_th_vlmul_ext_v_f16m1_f16m4(op1)
#define __riscv_vlmul_ext_v_f16m1_f16m8(op1) __riscv_th_vlmul_ext_v_f16m1_f16m8(op1)
#define __riscv_vlmul_ext_v_f16m2_f16m4(op1) __riscv_th_vlmul_ext_v_f16m2_f16m4(op1)
#define __riscv_vlmul_ext_v_f16m2_f16m8(op1) __riscv_th_vlmul_ext_v_f16m2_f16m8(op1)
#define __riscv_vlmul_ext_v_f16m4_f16m8(op1) __riscv_th_vlmul_ext_v_f16m4_f16m8(op1)
#define __riscv_vlmul_ext_v_f32m1_f32m2(op1) __riscv_th_vlmul_ext_v_f32m1_f32m2(op1)
#define __riscv_vlmul_ext_v_f32m1_f32m4(op1) __riscv_th_vlmul_ext_v_f32m1_f32m4(op1)
#define __riscv_vlmul_ext_v_f32m1_f32m8(op1) __riscv_th_vlmul_ext_v_f32m1_f32m8(op1)
#define __riscv_vlmul_ext_v_f32m2_f32m4(op1) __riscv_th_vlmul_ext_v_f32m2_f32m4(op1)
#define __riscv_vlmul_ext_v_f32m2_f32m8(op1) __riscv_th_vlmul_ext_v_f32m2_f32m8(op1)
#define __riscv_vlmul_ext_v_f32m4_f32m8(op1) __riscv_th_vlmul_ext_v_f32m4_f32m8(op1)
#define __riscv_vlmul_ext_v_f64m1_f64m2(op1) __riscv_th_vlmul_ext_v_f64m1_f64m2(op1)
#define __riscv_vlmul_ext_v_f64m1_f64m4(op1) __riscv_th_vlmul_ext_v_f64m1_f64m4(op1)
#define __riscv_vlmul_ext_v_f64m1_f64m8(op1) __riscv_th_vlmul_ext_v_f64m1_f64m8(op1)
#define __riscv_vlmul_ext_v_f64m2_f64m4(op1) __riscv_th_vlmul_ext_v_f64m2_f64m4(op1)
#define __riscv_vlmul_ext_v_f64m2_f64m8(op1) __riscv_th_vlmul_ext_v_f64m2_f64m8(op1)
#define __riscv_vlmul_ext_v_f64m4_f64m8(op1) __riscv_th_vlmul_ext_v_f64m4_f64m8(op1)
#define __riscv_vlmul_ext_v_i8m1_i8m2(op1) __riscv_th_vlmul_ext_v_i8m1_i8m2(op1)
#define __riscv_vlmul_ext_v_i8m1_i8m4(op1) __riscv_th_vlmul_ext_v_i8m1_i8m4(op1)
#define __riscv_vlmul_ext_v_i8m1_i8m8(op1) __riscv_th_vlmul_ext_v_i8m1_i8m8(op1)
#define __riscv_vlmul_ext_v_i8m2_i8m4(op1) __riscv_th_vlmul_ext_v_i8m2_i8m4(op1)
#define __riscv_vlmul_ext_v_i8m2_i8m8(op1) __riscv_th_vlmul_ext_v_i8m2_i8m8(op1)
#define __riscv_vlmul_ext_v_i8m4_i8m8(op1) __riscv_th_vlmul_ext_v_i8m4_i8m8(op1)
#define __riscv_vlmul_ext_v_i16m1_i16m2(op1) __riscv_th_vlmul_ext_v_i16m1_i16m2(op1)
#define __riscv_vlmul_ext_v_i16m1_i16m4(op1) __riscv_th_vlmul_ext_v_i16m1_i16m4(op1)
#define __riscv_vlmul_ext_v_i16m1_i16m8(op1) __riscv_th_vlmul_ext_v_i16m1_i16m8(op1)
#define __riscv_vlmul_ext_v_i16m2_i16m4(op1) __riscv_th_vlmul_ext_v_i16m2_i16m4(op1)
#define __riscv_vlmul_ext_v_i16m2_i16m8(op1) __riscv_th_vlmul_ext_v_i16m2_i16m8(op1)
#define __riscv_vlmul_ext_v_i16m4_i16m8(op1) __riscv_th_vlmul_ext_v_i16m4_i16m8(op1)
#define __riscv_vlmul_ext_v_i32m1_i32m2(op1) __riscv_th_vlmul_ext_v_i32m1_i32m2(op1)
#define __riscv_vlmul_ext_v_i32m1_i32m4(op1) __riscv_th_vlmul_ext_v_i32m1_i32m4(op1)
#define __riscv_vlmul_ext_v_i32m1_i32m8(op1) __riscv_th_vlmul_ext_v_i32m1_i32m8(op1)
#define __riscv_vlmul_ext_v_i32m2_i32m4(op1) __riscv_th_vlmul_ext_v_i32m2_i32m4(op1)
#define __riscv_vlmul_ext_v_i32m2_i32m8(op1) __riscv_th_vlmul_ext_v_i32m2_i32m8(op1)
#define __riscv_vlmul_ext_v_i32m4_i32m8(op1) __riscv_th_vlmul_ext_v_i32m4_i32m8(op1)
#define __riscv_vlmul_ext_v_i64m1_i64m2(op1) __riscv_th_vlmul_ext_v_i64m1_i64m2(op1)
#define __riscv_vlmul_ext_v_i64m1_i64m4(op1) __riscv_th_vlmul_ext_v_i64m1_i64m4(op1)
#define __riscv_vlmul_ext_v_i64m1_i64m8(op1) __riscv_th_vlmul_ext_v_i64m1_i64m8(op1)
#define __riscv_vlmul_ext_v_i64m2_i64m4(op1) __riscv_th_vlmul_ext_v_i64m2_i64m4(op1)
#define __riscv_vlmul_ext_v_i64m2_i64m8(op1) __riscv_th_vlmul_ext_v_i64m2_i64m8(op1)
#define __riscv_vlmul_ext_v_i64m4_i64m8(op1) __riscv_th_vlmul_ext_v_i64m4_i64m8(op1)
#define __riscv_vlmul_ext_v_u8m1_u8m2(op1) __riscv_th_vlmul_ext_v_u8m1_u8m2(op1)
#define __riscv_vlmul_ext_v_u8m1_u8m4(op1) __riscv_th_vlmul_ext_v_u8m1_u8m4(op1)
#define __riscv_vlmul_ext_v_u8m1_u8m8(op1) __riscv_th_vlmul_ext_v_u8m1_u8m8(op1)
#define __riscv_vlmul_ext_v_u8m2_u8m4(op1) __riscv_th_vlmul_ext_v_u8m2_u8m4(op1)
#define __riscv_vlmul_ext_v_u8m2_u8m8(op1) __riscv_th_vlmul_ext_v_u8m2_u8m8(op1)
#define __riscv_vlmul_ext_v_u8m4_u8m8(op1) __riscv_th_vlmul_ext_v_u8m4_u8m8(op1)
#define __riscv_vlmul_ext_v_u16m1_u16m2(op1) __riscv_th_vlmul_ext_v_u16m1_u16m2(op1)
#define __riscv_vlmul_ext_v_u16m1_u16m4(op1) __riscv_th_vlmul_ext_v_u16m1_u16m4(op1)
#define __riscv_vlmul_ext_v_u16m1_u16m8(op1) __riscv_th_vlmul_ext_v_u16m1_u16m8(op1)
#define __riscv_vlmul_ext_v_u16m2_u16m4(op1) __riscv_th_vlmul_ext_v_u16m2_u16m4(op1)
#define __riscv_vlmul_ext_v_u16m2_u16m8(op1) __riscv_th_vlmul_ext_v_u16m2_u16m8(op1)
#define __riscv_vlmul_ext_v_u16m4_u16m8(op1) __riscv_th_vlmul_ext_v_u16m4_u16m8(op1)
#define __riscv_vlmul_ext_v_u32m1_u32m2(op1) __riscv_th_vlmul_ext_v_u32m1_u32m2(op1)
#define __riscv_vlmul_ext_v_u32m1_u32m4(op1) __riscv_th_vlmul_ext_v_u32m1_u32m4(op1)
#define __riscv_vlmul_ext_v_u32m1_u32m8(op1) __riscv_th_vlmul_ext_v_u32m1_u32m8(op1)
#define __riscv_vlmul_ext_v_u32m2_u32m4(op1) __riscv_th_vlmul_ext_v_u32m2_u32m4(op1)
#define __riscv_vlmul_ext_v_u32m2_u32m8(op1) __riscv_th_vlmul_ext_v_u32m2_u32m8(op1)
#define __riscv_vlmul_ext_v_u32m4_u32m8(op1) __riscv_th_vlmul_ext_v_u32m4_u32m8(op1)
#define __riscv_vlmul_ext_v_u64m1_u64m2(op1) __riscv_th_vlmul_ext_v_u64m1_u64m2(op1)
#define __riscv_vlmul_ext_v_u64m1_u64m4(op1) __riscv_th_vlmul_ext_v_u64m1_u64m4(op1)
#define __riscv_vlmul_ext_v_u64m1_u64m8(op1) __riscv_th_vlmul_ext_v_u64m1_u64m8(op1)
#define __riscv_vlmul_ext_v_u64m2_u64m4(op1) __riscv_th_vlmul_ext_v_u64m2_u64m4(op1)
#define __riscv_vlmul_ext_v_u64m2_u64m8(op1) __riscv_th_vlmul_ext_v_u64m2_u64m8(op1)
#define __riscv_vlmul_ext_v_u64m4_u64m8(op1) __riscv_th_vlmul_ext_v_u64m4_u64m8(op1)
#define __riscv_vlmul_trunc_v_f16m2_f16m1(op1) __riscv_th_vlmul_trunc_v_f16m2_f16m1(op1)
#define __riscv_vlmul_trunc_v_f16m4_f16m1(op1) __riscv_th_vlmul_trunc_v_f16m4_f16m1(op1)
#define __riscv_vlmul_trunc_v_f16m4_f16m2(op1) __riscv_th_vlmul_trunc_v_f16m4_f16m2(op1)
#define __riscv_vlmul_trunc_v_f16m8_f16m1(op1) __riscv_th_vlmul_trunc_v_f16m8_f16m1(op1)
#define __riscv_vlmul_trunc_v_f16m8_f16m2(op1) __riscv_th_vlmul_trunc_v_f16m8_f16m2(op1)
#define __riscv_vlmul_trunc_v_f16m8_f16m4(op1) __riscv_th_vlmul_trunc_v_f16m8_f16m4(op1)
#define __riscv_vlmul_trunc_v_f32m2_f32m1(op1) __riscv_th_vlmul_trunc_v_f32m2_f32m1(op1)
#define __riscv_vlmul_trunc_v_f32m4_f32m1(op1) __riscv_th_vlmul_trunc_v_f32m4_f32m1(op1)
#define __riscv_vlmul_trunc_v_f32m4_f32m2(op1) __riscv_th_vlmul_trunc_v_f32m4_f32m2(op1)
#define __riscv_vlmul_trunc_v_f32m8_f32m1(op1) __riscv_th_vlmul_trunc_v_f32m8_f32m1(op1)
#define __riscv_vlmul_trunc_v_f32m8_f32m2(op1) __riscv_th_vlmul_trunc_v_f32m8_f32m2(op1)
#define __riscv_vlmul_trunc_v_f32m8_f32m4(op1) __riscv_th_vlmul_trunc_v_f32m8_f32m4(op1)
#define __riscv_vlmul_trunc_v_f64m2_f64m1(op1) __riscv_th_vlmul_trunc_v_f64m2_f64m1(op1)
#define __riscv_vlmul_trunc_v_f64m4_f64m1(op1) __riscv_th_vlmul_trunc_v_f64m4_f64m1(op1)
#define __riscv_vlmul_trunc_v_f64m4_f64m2(op1) __riscv_th_vlmul_trunc_v_f64m4_f64m2(op1)
#define __riscv_vlmul_trunc_v_f64m8_f64m1(op1) __riscv_th_vlmul_trunc_v_f64m8_f64m1(op1)
#define __riscv_vlmul_trunc_v_f64m8_f64m2(op1) __riscv_th_vlmul_trunc_v_f64m8_f64m2(op1)
#define __riscv_vlmul_trunc_v_f64m8_f64m4(op1) __riscv_th_vlmul_trunc_v_f64m8_f64m4(op1)
#define __riscv_vlmul_trunc_v_i8m2_i8m1(op1) __riscv_th_vlmul_trunc_v_i8m2_i8m1(op1)
#define __riscv_vlmul_trunc_v_i8m4_i8m1(op1) __riscv_th_vlmul_trunc_v_i8m4_i8m1(op1)
#define __riscv_vlmul_trunc_v_i8m4_i8m2(op1) __riscv_th_vlmul_trunc_v_i8m4_i8m2(op1)
#define __riscv_vlmul_trunc_v_i8m8_i8m1(op1) __riscv_th_vlmul_trunc_v_i8m8_i8m1(op1)
#define __riscv_vlmul_trunc_v_i8m8_i8m2(op1) __riscv_th_vlmul_trunc_v_i8m8_i8m2(op1)
#define __riscv_vlmul_trunc_v_i8m8_i8m4(op1) __riscv_th_vlmul_trunc_v_i8m8_i8m4(op1)
#define __riscv_vlmul_trunc_v_i16m2_i16m1(op1) __riscv_th_vlmul_trunc_v_i16m2_i16m1(op1)
#define __riscv_vlmul_trunc_v_i16m4_i16m1(op1) __riscv_th_vlmul_trunc_v_i16m4_i16m1(op1)
#define __riscv_vlmul_trunc_v_i16m4_i16m2(op1) __riscv_th_vlmul_trunc_v_i16m4_i16m2(op1)
#define __riscv_vlmul_trunc_v_i16m8_i16m1(op1) __riscv_th_vlmul_trunc_v_i16m8_i16m1(op1)
#define __riscv_vlmul_trunc_v_i16m8_i16m2(op1) __riscv_th_vlmul_trunc_v_i16m8_i16m2(op1)
#define __riscv_vlmul_trunc_v_i16m8_i16m4(op1) __riscv_th_vlmul_trunc_v_i16m8_i16m4(op1)
#define __riscv_vlmul_trunc_v_i32m2_i32m1(op1) __riscv_th_vlmul_trunc_v_i32m2_i32m1(op1)
#define __riscv_vlmul_trunc_v_i32m4_i32m1(op1) __riscv_th_vlmul_trunc_v_i32m4_i32m1(op1)
#define __riscv_vlmul_trunc_v_i32m4_i32m2(op1) __riscv_th_vlmul_trunc_v_i32m4_i32m2(op1)
#define __riscv_vlmul_trunc_v_i32m8_i32m1(op1) __riscv_th_vlmul_trunc_v_i32m8_i32m1(op1)
#define __riscv_vlmul_trunc_v_i32m8_i32m2(op1) __riscv_th_vlmul_trunc_v_i32m8_i32m2(op1)
#define __riscv_vlmul_trunc_v_i32m8_i32m4(op1) __riscv_th_vlmul_trunc_v_i32m8_i32m4(op1)
#define __riscv_vlmul_trunc_v_i64m2_i64m1(op1) __riscv_th_vlmul_trunc_v_i64m2_i64m1(op1)
#define __riscv_vlmul_trunc_v_i64m4_i64m1(op1) __riscv_th_vlmul_trunc_v_i64m4_i64m1(op1)
#define __riscv_vlmul_trunc_v_i64m4_i64m2(op1) __riscv_th_vlmul_trunc_v_i64m4_i64m2(op1)
#define __riscv_vlmul_trunc_v_i64m8_i64m1(op1) __riscv_th_vlmul_trunc_v_i64m8_i64m1(op1)
#define __riscv_vlmul_trunc_v_i64m8_i64m2(op1) __riscv_th_vlmul_trunc_v_i64m8_i64m2(op1)
#define __riscv_vlmul_trunc_v_i64m8_i64m4(op1) __riscv_th_vlmul_trunc_v_i64m8_i64m4(op1)
#define __riscv_vlmul_trunc_v_u8m2_u8m1(op1) __riscv_th_vlmul_trunc_v_u8m2_u8m1(op1)
#define __riscv_vlmul_trunc_v_u8m4_u8m1(op1) __riscv_th_vlmul_trunc_v_u8m4_u8m1(op1)
#define __riscv_vlmul_trunc_v_u8m4_u8m2(op1) __riscv_th_vlmul_trunc_v_u8m4_u8m2(op1)
#define __riscv_vlmul_trunc_v_u8m8_u8m1(op1) __riscv_th_vlmul_trunc_v_u8m8_u8m1(op1)
#define __riscv_vlmul_trunc_v_u8m8_u8m2(op1) __riscv_th_vlmul_trunc_v_u8m8_u8m2(op1)
#define __riscv_vlmul_trunc_v_u8m8_u8m4(op1) __riscv_th_vlmul_trunc_v_u8m8_u8m4(op1)
#define __riscv_vlmul_trunc_v_u16m2_u16m1(op1) __riscv_th_vlmul_trunc_v_u16m2_u16m1(op1)
#define __riscv_vlmul_trunc_v_u16m4_u16m1(op1) __riscv_th_vlmul_trunc_v_u16m4_u16m1(op1)
#define __riscv_vlmul_trunc_v_u16m4_u16m2(op1) __riscv_th_vlmul_trunc_v_u16m4_u16m2(op1)
#define __riscv_vlmul_trunc_v_u16m8_u16m1(op1) __riscv_th_vlmul_trunc_v_u16m8_u16m1(op1)
#define __riscv_vlmul_trunc_v_u16m8_u16m2(op1) __riscv_th_vlmul_trunc_v_u16m8_u16m2(op1)
#define __riscv_vlmul_trunc_v_u16m8_u16m4(op1) __riscv_th_vlmul_trunc_v_u16m8_u16m4(op1)
#define __riscv_vlmul_trunc_v_u32m2_u32m1(op1) __riscv_th_vlmul_trunc_v_u32m2_u32m1(op1)
#define __riscv_vlmul_trunc_v_u32m4_u32m1(op1) __riscv_th_vlmul_trunc_v_u32m4_u32m1(op1)
#define __riscv_vlmul_trunc_v_u32m4_u32m2(op1) __riscv_th_vlmul_trunc_v_u32m4_u32m2(op1)
#define __riscv_vlmul_trunc_v_u32m8_u32m1(op1) __riscv_th_vlmul_trunc_v_u32m8_u32m1(op1)
#define __riscv_vlmul_trunc_v_u32m8_u32m2(op1) __riscv_th_vlmul_trunc_v_u32m8_u32m2(op1)
#define __riscv_vlmul_trunc_v_u32m8_u32m4(op1) __riscv_th_vlmul_trunc_v_u32m8_u32m4(op1)
#define __riscv_vlmul_trunc_v_u64m2_u64m1(op1) __riscv_th_vlmul_trunc_v_u64m2_u64m1(op1)
#define __riscv_vlmul_trunc_v_u64m4_u64m1(op1) __riscv_th_vlmul_trunc_v_u64m4_u64m1(op1)
#define __riscv_vlmul_trunc_v_u64m4_u64m2(op1) __riscv_th_vlmul_trunc_v_u64m4_u64m2(op1)
#define __riscv_vlmul_trunc_v_u64m8_u64m1(op1) __riscv_th_vlmul_trunc_v_u64m8_u64m1(op1)
#define __riscv_vlmul_trunc_v_u64m8_u64m2(op1) __riscv_th_vlmul_trunc_v_u64m8_u64m2(op1)
#define __riscv_vlmul_trunc_v_u64m8_u64m4(op1) __riscv_th_vlmul_trunc_v_u64m8_u64m4(op1)

#define __riscv_vget_v_f16m2_f16m1(src, index) __riscv_th_vget_v_f16m2_f16m1(src, index)
#define __riscv_vget_v_f16m4_f16m1(src, index) __riscv_th_vget_v_f16m4_f16m1(src, index)
#define __riscv_vget_v_f16m8_f16m1(src, index) __riscv_th_vget_v_f16m8_f16m1(src, index)
#define __riscv_vget_v_f16m4_f16m2(src, index) __riscv_th_vget_v_f16m4_f16m2(src, index)
#define __riscv_vget_v_f16m8_f16m2(src, index) __riscv_th_vget_v_f16m8_f16m2(src, index)
#define __riscv_vget_v_f16m8_f16m4(src, index) __riscv_th_vget_v_f16m8_f16m4(src, index)
#define __riscv_vget_v_f32m2_f32m1(src, index) __riscv_th_vget_v_f32m2_f32m1(src, index)
#define __riscv_vget_v_f32m4_f32m1(src, index) __riscv_th_vget_v_f32m4_f32m1(src, index)
#define __riscv_vget_v_f32m8_f32m1(src, index) __riscv_th_vget_v_f32m8_f32m1(src, index)
#define __riscv_vget_v_f32m4_f32m2(src, index) __riscv_th_vget_v_f32m4_f32m2(src, index)
#define __riscv_vget_v_f32m8_f32m2(src, index) __riscv_th_vget_v_f32m8_f32m2(src, index)
#define __riscv_vget_v_f32m8_f32m4(src, index) __riscv_th_vget_v_f32m8_f32m4(src, index)
#define __riscv_vget_v_f64m2_f64m1(src, index) __riscv_th_vget_v_f64m2_f64m1(src, index)
#define __riscv_vget_v_f64m4_f64m1(src, index) __riscv_th_vget_v_f64m4_f64m1(src, index)
#define __riscv_vget_v_f64m8_f64m1(src, index) __riscv_th_vget_v_f64m8_f64m1(src, index)
#define __riscv_vget_v_f64m4_f64m2(src, index) __riscv_th_vget_v_f64m4_f64m2(src, index)
#define __riscv_vget_v_f64m8_f64m2(src, index) __riscv_th_vget_v_f64m8_f64m2(src, index)
#define __riscv_vget_v_f64m8_f64m4(src, index) __riscv_th_vget_v_f64m8_f64m4(src, index)
#define __riscv_vget_v_i8m2_i8m1(src, index) __riscv_th_vget_v_i8m2_i8m1(src, index)
#define __riscv_vget_v_i8m4_i8m1(src, index) __riscv_th_vget_v_i8m4_i8m1(src, index)
#define __riscv_vget_v_i8m8_i8m1(src, index) __riscv_th_vget_v_i8m8_i8m1(src, index)
#define __riscv_vget_v_i8m4_i8m2(src, index) __riscv_th_vget_v_i8m4_i8m2(src, index)
#define __riscv_vget_v_i8m8_i8m2(src, index) __riscv_th_vget_v_i8m8_i8m2(src, index)
#define __riscv_vget_v_i8m8_i8m4(src, index) __riscv_th_vget_v_i8m8_i8m4(src, index)
#define __riscv_vget_v_i16m2_i16m1(src, index) __riscv_th_vget_v_i16m2_i16m1(src, index)
#define __riscv_vget_v_i16m4_i16m1(src, index) __riscv_th_vget_v_i16m4_i16m1(src, index)
#define __riscv_vget_v_i16m8_i16m1(src, index) __riscv_th_vget_v_i16m8_i16m1(src, index)
#define __riscv_vget_v_i16m4_i16m2(src, index) __riscv_th_vget_v_i16m4_i16m2(src, index)
#define __riscv_vget_v_i16m8_i16m2(src, index) __riscv_th_vget_v_i16m8_i16m2(src, index)
#define __riscv_vget_v_i16m8_i16m4(src, index) __riscv_th_vget_v_i16m8_i16m4(src, index)
#define __riscv_vget_v_i32m2_i32m1(src, index) __riscv_th_vget_v_i32m2_i32m1(src, index)
#define __riscv_vget_v_i32m4_i32m1(src, index) __riscv_th_vget_v_i32m4_i32m1(src, index)
#define __riscv_vget_v_i32m8_i32m1(src, index) __riscv_th_vget_v_i32m8_i32m1(src, index)
#define __riscv_vget_v_i32m4_i32m2(src, index) __riscv_th_vget_v_i32m4_i32m2(src, index)
#define __riscv_vget_v_i32m8_i32m2(src, index) __riscv_th_vget_v_i32m8_i32m2(src, index)
#define __riscv_vget_v_i32m8_i32m4(src, index) __riscv_th_vget_v_i32m8_i32m4(src, index)
#define __riscv_vget_v_i64m2_i64m1(src, index) __riscv_th_vget_v_i64m2_i64m1(src, index)
#define __riscv_vget_v_i64m4_i64m1(src, index) __riscv_th_vget_v_i64m4_i64m1(src, index)
#define __riscv_vget_v_i64m8_i64m1(src, index) __riscv_th_vget_v_i64m8_i64m1(src, index)
#define __riscv_vget_v_i64m4_i64m2(src, index) __riscv_th_vget_v_i64m4_i64m2(src, index)
#define __riscv_vget_v_i64m8_i64m2(src, index) __riscv_th_vget_v_i64m8_i64m2(src, index)
#define __riscv_vget_v_i64m8_i64m4(src, index) __riscv_th_vget_v_i64m8_i64m4(src, index)
#define __riscv_vget_v_u8m2_u8m1(src, index) __riscv_th_vget_v_u8m2_u8m1(src, index)
#define __riscv_vget_v_u8m4_u8m1(src, index) __riscv_th_vget_v_u8m4_u8m1(src, index)
#define __riscv_vget_v_u8m8_u8m1(src, index) __riscv_th_vget_v_u8m8_u8m1(src, index)
#define __riscv_vget_v_u8m4_u8m2(src, index) __riscv_th_vget_v_u8m4_u8m2(src, index)
#define __riscv_vget_v_u8m8_u8m2(src, index) __riscv_th_vget_v_u8m8_u8m2(src, index)
#define __riscv_vget_v_u8m8_u8m4(src, index) __riscv_th_vget_v_u8m8_u8m4(src, index)
#define __riscv_vget_v_u16m2_u16m1(src, index) __riscv_th_vget_v_u16m2_u16m1(src, index)
#define __riscv_vget_v_u16m4_u16m1(src, index) __riscv_th_vget_v_u16m4_u16m1(src, index)
#define __riscv_vget_v_u16m8_u16m1(src, index) __riscv_th_vget_v_u16m8_u16m1(src, index)
#define __riscv_vget_v_u16m4_u16m2(src, index) __riscv_th_vget_v_u16m4_u16m2(src, index)
#define __riscv_vget_v_u16m8_u16m2(src, index) __riscv_th_vget_v_u16m8_u16m2(src, index)
#define __riscv_vget_v_u16m8_u16m4(src, index) __riscv_th_vget_v_u16m8_u16m4(src, index)
#define __riscv_vget_v_u32m2_u32m1(src, index) __riscv_th_vget_v_u32m2_u32m1(src, index)
#define __riscv_vget_v_u32m4_u32m1(src, index) __riscv_th_vget_v_u32m4_u32m1(src, index)
#define __riscv_vget_v_u32m8_u32m1(src, index) __riscv_th_vget_v_u32m8_u32m1(src, index)
#define __riscv_vget_v_u32m4_u32m2(src, index) __riscv_th_vget_v_u32m4_u32m2(src, index)
#define __riscv_vget_v_u32m8_u32m2(src, index) __riscv_th_vget_v_u32m8_u32m2(src, index)
#define __riscv_vget_v_u32m8_u32m4(src, index) __riscv_th_vget_v_u32m8_u32m4(src, index)
#define __riscv_vget_v_u64m2_u64m1(src, index) __riscv_th_vget_v_u64m2_u64m1(src, index)
#define __riscv_vget_v_u64m4_u64m1(src, index) __riscv_th_vget_v_u64m4_u64m1(src, index)
#define __riscv_vget_v_u64m8_u64m1(src, index) __riscv_th_vget_v_u64m8_u64m1(src, index)
#define __riscv_vget_v_u64m4_u64m2(src, index) __riscv_th_vget_v_u64m4_u64m2(src, index)
#define __riscv_vget_v_u64m8_u64m2(src, index) __riscv_th_vget_v_u64m8_u64m2(src, index)
#define __riscv_vget_v_u64m8_u64m4(src, index) __riscv_th_vget_v_u64m8_u64m4(src, index)

#define __riscv_vset_v_f16m1_f16m2(dest, index, val) __riscv_th_vset_v_f16m1_f16m2(dest, index, val)
#define __riscv_vset_v_f16m1_f16m4(dest, index, val) __riscv_th_vset_v_f16m1_f16m4(dest, index, val)
#define __riscv_vset_v_f16m2_f16m4(dest, index, val) __riscv_th_vset_v_f16m2_f16m4(dest, index, val)
#define __riscv_vset_v_f16m1_f16m8(dest, index, val) __riscv_th_vset_v_f16m1_f16m8(dest, index, val)
#define __riscv_vset_v_f16m2_f16m8(dest, index, val) __riscv_th_vset_v_f16m2_f16m8(dest, index, val)
#define __riscv_vset_v_f16m4_f16m8(dest, index, val) __riscv_th_vset_v_f16m4_f16m8(dest, index, val)
#define __riscv_vset_v_f32m1_f32m2(dest, index, val) __riscv_th_vset_v_f32m1_f32m2(dest, index, val)
#define __riscv_vset_v_f32m1_f32m4(dest, index, val) __riscv_th_vset_v_f32m1_f32m4(dest, index, val)
#define __riscv_vset_v_f32m2_f32m4(dest, index, val) __riscv_th_vset_v_f32m2_f32m4(dest, index, val)
#define __riscv_vset_v_f32m1_f32m8(dest, index, val) __riscv_th_vset_v_f32m1_f32m8(dest, index, val)
#define __riscv_vset_v_f32m2_f32m8(dest, index, val) __riscv_th_vset_v_f32m2_f32m8(dest, index, val)
#define __riscv_vset_v_f32m4_f32m8(dest, index, val) __riscv_th_vset_v_f32m4_f32m8(dest, index, val)
#define __riscv_vset_v_f64m1_f64m2(dest, index, val) __riscv_th_vset_v_f64m1_f64m2(dest, index, val)
#define __riscv_vset_v_f64m1_f64m4(dest, index, val) __riscv_th_vset_v_f64m1_f64m4(dest, index, val)
#define __riscv_vset_v_f64m2_f64m4(dest, index, val) __riscv_th_vset_v_f64m2_f64m4(dest, index, val)
#define __riscv_vset_v_f64m1_f64m8(dest, index, val) __riscv_th_vset_v_f64m1_f64m8(dest, index, val)
#define __riscv_vset_v_f64m2_f64m8(dest, index, val) __riscv_th_vset_v_f64m2_f64m8(dest, index, val)
#define __riscv_vset_v_f64m4_f64m8(dest, index, val) __riscv_th_vset_v_f64m4_f64m8(dest, index, val)
#define __riscv_vset_v_i8m1_i8m2(dest, index, val) __riscv_th_vset_v_i8m1_i8m2(dest, index, val)
#define __riscv_vset_v_i8m1_i8m4(dest, index, val) __riscv_th_vset_v_i8m1_i8m4(dest, index, val)
#define __riscv_vset_v_i8m2_i8m4(dest, index, val) __riscv_th_vset_v_i8m2_i8m4(dest, index, val)
#define __riscv_vset_v_i8m1_i8m8(dest, index, val) __riscv_th_vset_v_i8m1_i8m8(dest, index, val)
#define __riscv_vset_v_i8m2_i8m8(dest, index, val) __riscv_th_vset_v_i8m2_i8m8(dest, index, val)
#define __riscv_vset_v_i8m4_i8m8(dest, index, val) __riscv_th_vset_v_i8m4_i8m8(dest, index, val)
#define __riscv_vset_v_i16m1_i16m2(dest, index, val) __riscv_th_vset_v_i16m1_i16m2(dest, index, val)
#define __riscv_vset_v_i16m1_i16m4(dest, index, val) __riscv_th_vset_v_i16m1_i16m4(dest, index, val)
#define __riscv_vset_v_i16m2_i16m4(dest, index, val) __riscv_th_vset_v_i16m2_i16m4(dest, index, val)
#define __riscv_vset_v_i16m1_i16m8(dest, index, val) __riscv_th_vset_v_i16m1_i16m8(dest, index, val)
#define __riscv_vset_v_i16m2_i16m8(dest, index, val) __riscv_th_vset_v_i16m2_i16m8(dest, index, val)
#define __riscv_vset_v_i16m4_i16m8(dest, index, val) __riscv_th_vset_v_i16m4_i16m8(dest, index, val)
#define __riscv_vset_v_i32m1_i32m2(dest, index, val) __riscv_th_vset_v_i32m1_i32m2(dest, index, val)
#define __riscv_vset_v_i32m1_i32m4(dest, index, val) __riscv_th_vset_v_i32m1_i32m4(dest, index, val)
#define __riscv_vset_v_i32m2_i32m4(dest, index, val) __riscv_th_vset_v_i32m2_i32m4(dest, index, val)
#define __riscv_vset_v_i32m1_i32m8(dest, index, val) __riscv_th_vset_v_i32m1_i32m8(dest, index, val)
#define __riscv_vset_v_i32m2_i32m8(dest, index, val) __riscv_th_vset_v_i32m2_i32m8(dest, index, val)
#define __riscv_vset_v_i32m4_i32m8(dest, index, val) __riscv_th_vset_v_i32m4_i32m8(dest, index, val)
#define __riscv_vset_v_i64m1_i64m2(dest, index, val) __riscv_th_vset_v_i64m1_i64m2(dest, index, val)
#define __riscv_vset_v_i64m1_i64m4(dest, index, val) __riscv_th_vset_v_i64m1_i64m4(dest, index, val)
#define __riscv_vset_v_i64m2_i64m4(dest, index, val) __riscv_th_vset_v_i64m2_i64m4(dest, index, val)
#define __riscv_vset_v_i64m1_i64m8(dest, index, val) __riscv_th_vset_v_i64m1_i64m8(dest, index, val)
#define __riscv_vset_v_i64m2_i64m8(dest, index, val) __riscv_th_vset_v_i64m2_i64m8(dest, index, val)
#define __riscv_vset_v_i64m4_i64m8(dest, index, val) __riscv_th_vset_v_i64m4_i64m8(dest, index, val)
#define __riscv_vset_v_u8m1_u8m2(dest, index, val) __riscv_th_vset_v_u8m1_u8m2(dest, index, val)
#define __riscv_vset_v_u8m1_u8m4(dest, index, val) __riscv_th_vset_v_u8m1_u8m4(dest, index, val)
#define __riscv_vset_v_u8m2_u8m4(dest, index, val) __riscv_th_vset_v_u8m2_u8m4(dest, index, val)
#define __riscv_vset_v_u8m1_u8m8(dest, index, val) __riscv_th_vset_v_u8m1_u8m8(dest, index, val)
#define __riscv_vset_v_u8m2_u8m8(dest, index, val) __riscv_th_vset_v_u8m2_u8m8(dest, index, val)
#define __riscv_vset_v_u8m4_u8m8(dest, index, val) __riscv_th_vset_v_u8m4_u8m8(dest, index, val)
#define __riscv_vset_v_u16m1_u16m2(dest, index, val) __riscv_th_vset_v_u16m1_u16m2(dest, index, val)
#define __riscv_vset_v_u16m1_u16m4(dest, index, val) __riscv_th_vset_v_u16m1_u16m4(dest, index, val)
#define __riscv_vset_v_u16m2_u16m4(dest, index, val) __riscv_th_vset_v_u16m2_u16m4(dest, index, val)
#define __riscv_vset_v_u16m1_u16m8(dest, index, val) __riscv_th_vset_v_u16m1_u16m8(dest, index, val)
#define __riscv_vset_v_u16m2_u16m8(dest, index, val) __riscv_th_vset_v_u16m2_u16m8(dest, index, val)
#define __riscv_vset_v_u16m4_u16m8(dest, index, val) __riscv_th_vset_v_u16m4_u16m8(dest, index, val)
#define __riscv_vset_v_u32m1_u32m2(dest, index, val) __riscv_th_vset_v_u32m1_u32m2(dest, index, val)
#define __riscv_vset_v_u32m1_u32m4(dest, index, val) __riscv_th_vset_v_u32m1_u32m4(dest, index, val)
#define __riscv_vset_v_u32m2_u32m4(dest, index, val) __riscv_th_vset_v_u32m2_u32m4(dest, index, val)
#define __riscv_vset_v_u32m1_u32m8(dest, index, val) __riscv_th_vset_v_u32m1_u32m8(dest, index, val)
#define __riscv_vset_v_u32m2_u32m8(dest, index, val) __riscv_th_vset_v_u32m2_u32m8(dest, index, val)
#define __riscv_vset_v_u32m4_u32m8(dest, index, val) __riscv_th_vset_v_u32m4_u32m8(dest, index, val)
#define __riscv_vset_v_u64m1_u64m2(dest, index, val) __riscv_th_vset_v_u64m1_u64m2(dest, index, val)
#define __riscv_vset_v_u64m1_u64m4(dest, index, val) __riscv_th_vset_v_u64m1_u64m4(dest, index, val)
#define __riscv_vset_v_u64m2_u64m4(dest, index, val) __riscv_th_vset_v_u64m2_u64m4(dest, index, val)
#define __riscv_vset_v_u64m1_u64m8(dest, index, val) __riscv_th_vset_v_u64m1_u64m8(dest, index, val)
#define __riscv_vset_v_u64m2_u64m8(dest, index, val) __riscv_th_vset_v_u64m2_u64m8(dest, index, val)
#define __riscv_vset_v_u64m4_u64m8(dest, index, val) __riscv_th_vset_v_u64m4_u64m8(dest, index, val)

#define __riscv_vset_v_f16m1_f16m1x2(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x2(dest, index, val)
#define __riscv_vset_v_f16m1_f16m1x3(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x3(dest, index, val)
#define __riscv_vset_v_f16m1_f16m1x4(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x4(dest, index, val)
#define __riscv_vset_v_f16m1_f16m1x5(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x5(dest, index, val)
#define __riscv_vset_v_f16m1_f16m1x6(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x6(dest, index, val)
#define __riscv_vset_v_f16m1_f16m1x7(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x7(dest, index, val)
#define __riscv_vset_v_f16m1_f16m1x8(dest, index, val) __riscv_th_vset_v_f16m1_f16m1x8(dest, index, val)
#define __riscv_vset_v_f16m2_f16m2x2(dest, index, val) __riscv_th_vset_v_f16m2_f16m2x2(dest, index, val)
#define __riscv_vset_v_f16m2_f16m2x3(dest, index, val) __riscv_th_vset_v_f16m2_f16m2x3(dest, index, val)
#define __riscv_vset_v_f16m2_f16m2x4(dest, index, val) __riscv_th_vset_v_f16m2_f16m2x4(dest, index, val)
#define __riscv_vset_v_f16m4_f16m4x2(dest, index, val) __riscv_th_vset_v_f16m4_f16m4x2(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x2(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x2(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x3(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x3(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x4(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x4(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x5(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x5(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x6(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x6(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x7(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x7(dest, index, val)
#define __riscv_vset_v_f32m1_f32m1x8(dest, index, val) __riscv_th_vset_v_f32m1_f32m1x8(dest, index, val)
#define __riscv_vset_v_f32m2_f32m2x2(dest, index, val) __riscv_th_vset_v_f32m2_f32m2x2(dest, index, val)
#define __riscv_vset_v_f32m2_f32m2x3(dest, index, val) __riscv_th_vset_v_f32m2_f32m2x3(dest, index, val)
#define __riscv_vset_v_f32m2_f32m2x4(dest, index, val) __riscv_th_vset_v_f32m2_f32m2x4(dest, index, val)
#define __riscv_vset_v_f32m4_f32m4x2(dest, index, val) __riscv_th_vset_v_f32m4_f32m4x2(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x2(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x2(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x3(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x3(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x4(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x4(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x5(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x5(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x6(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x6(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x7(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x7(dest, index, val)
#define __riscv_vset_v_f64m1_f64m1x8(dest, index, val) __riscv_th_vset_v_f64m1_f64m1x8(dest, index, val)
#define __riscv_vset_v_f64m2_f64m2x2(dest, index, val) __riscv_th_vset_v_f64m2_f64m2x2(dest, index, val)
#define __riscv_vset_v_f64m2_f64m2x3(dest, index, val) __riscv_th_vset_v_f64m2_f64m2x3(dest, index, val)
#define __riscv_vset_v_f64m2_f64m2x4(dest, index, val) __riscv_th_vset_v_f64m2_f64m2x4(dest, index, val)
#define __riscv_vset_v_f64m4_f64m4x2(dest, index, val) __riscv_th_vset_v_f64m4_f64m4x2(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x2(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x2(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x3(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x3(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x4(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x4(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x5(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x5(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x6(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x6(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x7(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x7(dest, index, val)
#define __riscv_vset_v_i8m1_i8m1x8(dest, index, val) __riscv_th_vset_v_i8m1_i8m1x8(dest, index, val)
#define __riscv_vset_v_i8m2_i8m2x2(dest, index, val) __riscv_th_vset_v_i8m2_i8m2x2(dest, index, val)
#define __riscv_vset_v_i8m2_i8m2x3(dest, index, val) __riscv_th_vset_v_i8m2_i8m2x3(dest, index, val)
#define __riscv_vset_v_i8m2_i8m2x4(dest, index, val) __riscv_th_vset_v_i8m2_i8m2x4(dest, index, val)
#define __riscv_vset_v_i8m4_i8m4x2(dest, index, val) __riscv_th_vset_v_i8m4_i8m4x2(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x2(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x2(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x3(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x3(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x4(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x4(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x5(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x5(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x6(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x6(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x7(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x7(dest, index, val)
#define __riscv_vset_v_i16m1_i16m1x8(dest, index, val) __riscv_th_vset_v_i16m1_i16m1x8(dest, index, val)
#define __riscv_vset_v_i16m2_i16m2x2(dest, index, val) __riscv_th_vset_v_i16m2_i16m2x2(dest, index, val)
#define __riscv_vset_v_i16m2_i16m2x3(dest, index, val) __riscv_th_vset_v_i16m2_i16m2x3(dest, index, val)
#define __riscv_vset_v_i16m2_i16m2x4(dest, index, val) __riscv_th_vset_v_i16m2_i16m2x4(dest, index, val)
#define __riscv_vset_v_i16m4_i16m4x2(dest, index, val) __riscv_th_vset_v_i16m4_i16m4x2(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x2(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x2(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x3(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x3(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x4(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x4(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x5(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x5(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x6(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x6(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x7(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x7(dest, index, val)
#define __riscv_vset_v_i32m1_i32m1x8(dest, index, val) __riscv_th_vset_v_i32m1_i32m1x8(dest, index, val)
#define __riscv_vset_v_i32m2_i32m2x2(dest, index, val) __riscv_th_vset_v_i32m2_i32m2x2(dest, index, val)
#define __riscv_vset_v_i32m2_i32m2x3(dest, index, val) __riscv_th_vset_v_i32m2_i32m2x3(dest, index, val)
#define __riscv_vset_v_i32m2_i32m2x4(dest, index, val) __riscv_th_vset_v_i32m2_i32m2x4(dest, index, val)
#define __riscv_vset_v_i32m4_i32m4x2(dest, index, val) __riscv_th_vset_v_i32m4_i32m4x2(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x2(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x2(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x3(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x3(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x4(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x4(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x5(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x5(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x6(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x6(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x7(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x7(dest, index, val)
#define __riscv_vset_v_i64m1_i64m1x8(dest, index, val) __riscv_th_vset_v_i64m1_i64m1x8(dest, index, val)
#define __riscv_vset_v_i64m2_i64m2x2(dest, index, val) __riscv_th_vset_v_i64m2_i64m2x2(dest, index, val)
#define __riscv_vset_v_i64m2_i64m2x3(dest, index, val) __riscv_th_vset_v_i64m2_i64m2x3(dest, index, val)
#define __riscv_vset_v_i64m2_i64m2x4(dest, index, val) __riscv_th_vset_v_i64m2_i64m2x4(dest, index, val)
#define __riscv_vset_v_i64m4_i64m4x2(dest, index, val) __riscv_th_vset_v_i64m4_i64m4x2(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x2(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x2(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x3(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x3(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x4(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x4(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x5(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x5(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x6(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x6(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x7(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x7(dest, index, val)
#define __riscv_vset_v_u8m1_u8m1x8(dest, index, val) __riscv_th_vset_v_u8m1_u8m1x8(dest, index, val)
#define __riscv_vset_v_u8m2_u8m2x2(dest, index, val) __riscv_th_vset_v_u8m2_u8m2x2(dest, index, val)
#define __riscv_vset_v_u8m2_u8m2x3(dest, index, val) __riscv_th_vset_v_u8m2_u8m2x3(dest, index, val)
#define __riscv_vset_v_u8m2_u8m2x4(dest, index, val) __riscv_th_vset_v_u8m2_u8m2x4(dest, index, val)
#define __riscv_vset_v_u8m4_u8m4x2(dest, index, val) __riscv_th_vset_v_u8m4_u8m4x2(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x2(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x2(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x3(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x3(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x4(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x4(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x5(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x5(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x6(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x6(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x7(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x7(dest, index, val)
#define __riscv_vset_v_u16m1_u16m1x8(dest, index, val) __riscv_th_vset_v_u16m1_u16m1x8(dest, index, val)
#define __riscv_vset_v_u16m2_u16m2x2(dest, index, val) __riscv_th_vset_v_u16m2_u16m2x2(dest, index, val)
#define __riscv_vset_v_u16m2_u16m2x3(dest, index, val) __riscv_th_vset_v_u16m2_u16m2x3(dest, index, val)
#define __riscv_vset_v_u16m2_u16m2x4(dest, index, val) __riscv_th_vset_v_u16m2_u16m2x4(dest, index, val)
#define __riscv_vset_v_u16m4_u16m4x2(dest, index, val) __riscv_th_vset_v_u16m4_u16m4x2(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x2(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x2(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x3(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x3(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x4(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x4(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x5(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x5(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x6(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x6(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x7(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x7(dest, index, val)
#define __riscv_vset_v_u32m1_u32m1x8(dest, index, val) __riscv_th_vset_v_u32m1_u32m1x8(dest, index, val)
#define __riscv_vset_v_u32m2_u32m2x2(dest, index, val) __riscv_th_vset_v_u32m2_u32m2x2(dest, index, val)
#define __riscv_vset_v_u32m2_u32m2x3(dest, index, val) __riscv_th_vset_v_u32m2_u32m2x3(dest, index, val)
#define __riscv_vset_v_u32m2_u32m2x4(dest, index, val) __riscv_th_vset_v_u32m2_u32m2x4(dest, index, val)
#define __riscv_vset_v_u32m4_u32m4x2(dest, index, val) __riscv_th_vset_v_u32m4_u32m4x2(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x2(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x2(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x3(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x3(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x4(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x4(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x5(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x5(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x6(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x6(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x7(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x7(dest, index, val)
#define __riscv_vset_v_u64m1_u64m1x8(dest, index, val) __riscv_th_vset_v_u64m1_u64m1x8(dest, index, val)
#define __riscv_vset_v_u64m2_u64m2x2(dest, index, val) __riscv_th_vset_v_u64m2_u64m2x2(dest, index, val)
#define __riscv_vset_v_u64m2_u64m2x3(dest, index, val) __riscv_th_vset_v_u64m2_u64m2x3(dest, index, val)
#define __riscv_vset_v_u64m2_u64m2x4(dest, index, val) __riscv_th_vset_v_u64m2_u64m2x4(dest, index, val)
#define __riscv_vset_v_u64m4_u64m4x2(dest, index, val) __riscv_th_vset_v_u64m4_u64m4x2(dest, index, val)

#define __riscv_vget_v_f16m1x2_f16m1(src, index) __riscv_th_vget_v_f16m1x2_f16m1(src, index)
#define __riscv_vget_v_f16m1x3_f16m1(src, index) __riscv_th_vget_v_f16m1x3_f16m1(src, index)
#define __riscv_vget_v_f16m1x4_f16m1(src, index) __riscv_th_vget_v_f16m1x4_f16m1(src, index)
#define __riscv_vget_v_f16m1x5_f16m1(src, index) __riscv_th_vget_v_f16m1x5_f16m1(src, index)
#define __riscv_vget_v_f16m1x6_f16m1(src, index) __riscv_th_vget_v_f16m1x6_f16m1(src, index)
#define __riscv_vget_v_f16m1x7_f16m1(src, index) __riscv_th_vget_v_f16m1x7_f16m1(src, index)
#define __riscv_vget_v_f16m1x8_f16m1(src, index) __riscv_th_vget_v_f16m1x8_f16m1(src, index)
#define __riscv_vget_v_f16m2x2_f16m2(src, index) __riscv_th_vget_v_f16m2x2_f16m2(src, index)
#define __riscv_vget_v_f16m2x3_f16m2(src, index) __riscv_th_vget_v_f16m2x3_f16m2(src, index)
#define __riscv_vget_v_f16m2x4_f16m2(src, index) __riscv_th_vget_v_f16m2x4_f16m2(src, index)
#define __riscv_vget_v_f16m4x2_f16m4(src, index) __riscv_th_vget_v_f16m4x2_f16m4(src, index)
#define __riscv_vget_v_f32m1x2_f32m1(src, index) __riscv_th_vget_v_f32m1x2_f32m1(src, index)
#define __riscv_vget_v_f32m1x3_f32m1(src, index) __riscv_th_vget_v_f32m1x3_f32m1(src, index)
#define __riscv_vget_v_f32m1x4_f32m1(src, index) __riscv_th_vget_v_f32m1x4_f32m1(src, index)
#define __riscv_vget_v_f32m1x5_f32m1(src, index) __riscv_th_vget_v_f32m1x5_f32m1(src, index)
#define __riscv_vget_v_f32m1x6_f32m1(src, index) __riscv_th_vget_v_f32m1x6_f32m1(src, index)
#define __riscv_vget_v_f32m1x7_f32m1(src, index) __riscv_th_vget_v_f32m1x7_f32m1(src, index)
#define __riscv_vget_v_f32m1x8_f32m1(src, index) __riscv_th_vget_v_f32m1x8_f32m1(src, index)
#define __riscv_vget_v_f32m2x2_f32m2(src, index) __riscv_th_vget_v_f32m2x2_f32m2(src, index)
#define __riscv_vget_v_f32m2x3_f32m2(src, index) __riscv_th_vget_v_f32m2x3_f32m2(src, index)
#define __riscv_vget_v_f32m2x4_f32m2(src, index) __riscv_th_vget_v_f32m2x4_f32m2(src, index)
#define __riscv_vget_v_f32m4x2_f32m4(src, index) __riscv_th_vget_v_f32m4x2_f32m4(src, index)
#define __riscv_vget_v_f64m1x2_f64m1(src, index) __riscv_th_vget_v_f64m1x2_f64m1(src, index)
#define __riscv_vget_v_f64m1x3_f64m1(src, index) __riscv_th_vget_v_f64m1x3_f64m1(src, index)
#define __riscv_vget_v_f64m1x4_f64m1(src, index) __riscv_th_vget_v_f64m1x4_f64m1(src, index)
#define __riscv_vget_v_f64m1x5_f64m1(src, index) __riscv_th_vget_v_f64m1x5_f64m1(src, index)
#define __riscv_vget_v_f64m1x6_f64m1(src, index) __riscv_th_vget_v_f64m1x6_f64m1(src, index)
#define __riscv_vget_v_f64m1x7_f64m1(src, index) __riscv_th_vget_v_f64m1x7_f64m1(src, index)
#define __riscv_vget_v_f64m1x8_f64m1(src, index) __riscv_th_vget_v_f64m1x8_f64m1(src, index)
#define __riscv_vget_v_f64m2x2_f64m2(src, index) __riscv_th_vget_v_f64m2x2_f64m2(src, index)
#define __riscv_vget_v_f64m2x3_f64m2(src, index) __riscv_th_vget_v_f64m2x3_f64m2(src, index)
#define __riscv_vget_v_f64m2x4_f64m2(src, index) __riscv_th_vget_v_f64m2x4_f64m2(src, index)
#define __riscv_vget_v_f64m4x2_f64m4(src, index) __riscv_th_vget_v_f64m4x2_f64m4(src, index)
#define __riscv_vget_v_i8m1x2_i8m1(src, index) __riscv_th_vget_v_i8m1x2_i8m1(src, index)
#define __riscv_vget_v_i8m1x3_i8m1(src, index) __riscv_th_vget_v_i8m1x3_i8m1(src, index)
#define __riscv_vget_v_i8m1x4_i8m1(src, index) __riscv_th_vget_v_i8m1x4_i8m1(src, index)
#define __riscv_vget_v_i8m1x5_i8m1(src, index) __riscv_th_vget_v_i8m1x5_i8m1(src, index)
#define __riscv_vget_v_i8m1x6_i8m1(src, index) __riscv_th_vget_v_i8m1x6_i8m1(src, index)
#define __riscv_vget_v_i8m1x7_i8m1(src, index) __riscv_th_vget_v_i8m1x7_i8m1(src, index)
#define __riscv_vget_v_i8m1x8_i8m1(src, index) __riscv_th_vget_v_i8m1x8_i8m1(src, index)
#define __riscv_vget_v_i8m2x2_i8m2(src, index) __riscv_th_vget_v_i8m2x2_i8m2(src, index)
#define __riscv_vget_v_i8m2x3_i8m2(src, index) __riscv_th_vget_v_i8m2x3_i8m2(src, index)
#define __riscv_vget_v_i8m2x4_i8m2(src, index) __riscv_th_vget_v_i8m2x4_i8m2(src, index)
#define __riscv_vget_v_i8m4x2_i8m4(src, index) __riscv_th_vget_v_i8m4x2_i8m4(src, index)
#define __riscv_vget_v_i16m1x2_i16m1(src, index) __riscv_th_vget_v_i16m1x2_i16m1(src, index)
#define __riscv_vget_v_i16m1x3_i16m1(src, index) __riscv_th_vget_v_i16m1x3_i16m1(src, index)
#define __riscv_vget_v_i16m1x4_i16m1(src, index) __riscv_th_vget_v_i16m1x4_i16m1(src, index)
#define __riscv_vget_v_i16m1x5_i16m1(src, index) __riscv_th_vget_v_i16m1x5_i16m1(src, index)
#define __riscv_vget_v_i16m1x6_i16m1(src, index) __riscv_th_vget_v_i16m1x6_i16m1(src, index)
#define __riscv_vget_v_i16m1x7_i16m1(src, index) __riscv_th_vget_v_i16m1x7_i16m1(src, index)
#define __riscv_vget_v_i16m1x8_i16m1(src, index) __riscv_th_vget_v_i16m1x8_i16m1(src, index)
#define __riscv_vget_v_i16m2x2_i16m2(src, index) __riscv_th_vget_v_i16m2x2_i16m2(src, index)
#define __riscv_vget_v_i16m2x3_i16m2(src, index) __riscv_th_vget_v_i16m2x3_i16m2(src, index)
#define __riscv_vget_v_i16m2x4_i16m2(src, index) __riscv_th_vget_v_i16m2x4_i16m2(src, index)
#define __riscv_vget_v_i16m4x2_i16m4(src, index) __riscv_th_vget_v_i16m4x2_i16m4(src, index)
#define __riscv_vget_v_i32m1x2_i32m1(src, index) __riscv_th_vget_v_i32m1x2_i32m1(src, index)
#define __riscv_vget_v_i32m1x3_i32m1(src, index) __riscv_th_vget_v_i32m1x3_i32m1(src, index)
#define __riscv_vget_v_i32m1x4_i32m1(src, index) __riscv_th_vget_v_i32m1x4_i32m1(src, index)
#define __riscv_vget_v_i32m1x5_i32m1(src, index) __riscv_th_vget_v_i32m1x5_i32m1(src, index)
#define __riscv_vget_v_i32m1x6_i32m1(src, index) __riscv_th_vget_v_i32m1x6_i32m1(src, index)
#define __riscv_vget_v_i32m1x7_i32m1(src, index) __riscv_th_vget_v_i32m1x7_i32m1(src, index)
#define __riscv_vget_v_i32m1x8_i32m1(src, index) __riscv_th_vget_v_i32m1x8_i32m1(src, index)
#define __riscv_vget_v_i32m2x2_i32m2(src, index) __riscv_th_vget_v_i32m2x2_i32m2(src, index)
#define __riscv_vget_v_i32m2x3_i32m2(src, index) __riscv_th_vget_v_i32m2x3_i32m2(src, index)
#define __riscv_vget_v_i32m2x4_i32m2(src, index) __riscv_th_vget_v_i32m2x4_i32m2(src, index)
#define __riscv_vget_v_i32m4x2_i32m4(src, index) __riscv_th_vget_v_i32m4x2_i32m4(src, index)
#define __riscv_vget_v_i64m1x2_i64m1(src, index) __riscv_th_vget_v_i64m1x2_i64m1(src, index)
#define __riscv_vget_v_i64m1x3_i64m1(src, index) __riscv_th_vget_v_i64m1x3_i64m1(src, index)
#define __riscv_vget_v_i64m1x4_i64m1(src, index) __riscv_th_vget_v_i64m1x4_i64m1(src, index)
#define __riscv_vget_v_i64m1x5_i64m1(src, index) __riscv_th_vget_v_i64m1x5_i64m1(src, index)
#define __riscv_vget_v_i64m1x6_i64m1(src, index) __riscv_th_vget_v_i64m1x6_i64m1(src, index)
#define __riscv_vget_v_i64m1x7_i64m1(src, index) __riscv_th_vget_v_i64m1x7_i64m1(src, index)
#define __riscv_vget_v_i64m1x8_i64m1(src, index) __riscv_th_vget_v_i64m1x8_i64m1(src, index)
#define __riscv_vget_v_i64m2x2_i64m2(src, index) __riscv_th_vget_v_i64m2x2_i64m2(src, index)
#define __riscv_vget_v_i64m2x3_i64m2(src, index) __riscv_th_vget_v_i64m2x3_i64m2(src, index)
#define __riscv_vget_v_i64m2x4_i64m2(src, index) __riscv_th_vget_v_i64m2x4_i64m2(src, index)
#define __riscv_vget_v_i64m4x2_i64m4(src, index) __riscv_th_vget_v_i64m4x2_i64m4(src, index)
#define __riscv_vget_v_u8m1x2_u8m1(src, index) __riscv_th_vget_v_u8m1x2_u8m1(src, index)
#define __riscv_vget_v_u8m1x3_u8m1(src, index) __riscv_th_vget_v_u8m1x3_u8m1(src, index)
#define __riscv_vget_v_u8m1x4_u8m1(src, index) __riscv_th_vget_v_u8m1x4_u8m1(src, index)
#define __riscv_vget_v_u8m1x5_u8m1(src, index) __riscv_th_vget_v_u8m1x5_u8m1(src, index)
#define __riscv_vget_v_u8m1x6_u8m1(src, index) __riscv_th_vget_v_u8m1x6_u8m1(src, index)
#define __riscv_vget_v_u8m1x7_u8m1(src, index) __riscv_th_vget_v_u8m1x7_u8m1(src, index)
#define __riscv_vget_v_u8m1x8_u8m1(src, index) __riscv_th_vget_v_u8m1x8_u8m1(src, index)
#define __riscv_vget_v_u8m2x2_u8m2(src, index) __riscv_th_vget_v_u8m2x2_u8m2(src, index)
#define __riscv_vget_v_u8m2x3_u8m2(src, index) __riscv_th_vget_v_u8m2x3_u8m2(src, index)
#define __riscv_vget_v_u8m2x4_u8m2(src, index) __riscv_th_vget_v_u8m2x4_u8m2(src, index)
#define __riscv_vget_v_u8m4x2_u8m4(src, index) __riscv_th_vget_v_u8m4x2_u8m4(src, index)
#define __riscv_vget_v_u16m1x2_u16m1(src, index) __riscv_th_vget_v_u16m1x2_u16m1(src, index)
#define __riscv_vget_v_u16m1x3_u16m1(src, index) __riscv_th_vget_v_u16m1x3_u16m1(src, index)
#define __riscv_vget_v_u16m1x4_u16m1(src, index) __riscv_th_vget_v_u16m1x4_u16m1(src, index)
#define __riscv_vget_v_u16m1x5_u16m1(src, index) __riscv_th_vget_v_u16m1x5_u16m1(src, index)
#define __riscv_vget_v_u16m1x6_u16m1(src, index) __riscv_th_vget_v_u16m1x6_u16m1(src, index)
#define __riscv_vget_v_u16m1x7_u16m1(src, index) __riscv_th_vget_v_u16m1x7_u16m1(src, index)
#define __riscv_vget_v_u16m1x8_u16m1(src, index) __riscv_th_vget_v_u16m1x8_u16m1(src, index)
#define __riscv_vget_v_u16m2x2_u16m2(src, index) __riscv_th_vget_v_u16m2x2_u16m2(src, index)
#define __riscv_vget_v_u16m2x3_u16m2(src, index) __riscv_th_vget_v_u16m2x3_u16m2(src, index)
#define __riscv_vget_v_u16m2x4_u16m2(src, index) __riscv_th_vget_v_u16m2x4_u16m2(src, index)
#define __riscv_vget_v_u16m4x2_u16m4(src, index) __riscv_th_vget_v_u16m4x2_u16m4(src, index)
#define __riscv_vget_v_u32m1x2_u32m1(src, index) __riscv_th_vget_v_u32m1x2_u32m1(src, index)
#define __riscv_vget_v_u32m1x3_u32m1(src, index) __riscv_th_vget_v_u32m1x3_u32m1(src, index)
#define __riscv_vget_v_u32m1x4_u32m1(src, index) __riscv_th_vget_v_u32m1x4_u32m1(src, index)
#define __riscv_vget_v_u32m1x5_u32m1(src, index) __riscv_th_vget_v_u32m1x5_u32m1(src, index)
#define __riscv_vget_v_u32m1x6_u32m1(src, index) __riscv_th_vget_v_u32m1x6_u32m1(src, index)
#define __riscv_vget_v_u32m1x7_u32m1(src, index) __riscv_th_vget_v_u32m1x7_u32m1(src, index)
#define __riscv_vget_v_u32m1x8_u32m1(src, index) __riscv_th_vget_v_u32m1x8_u32m1(src, index)
#define __riscv_vget_v_u32m2x2_u32m2(src, index) __riscv_th_vget_v_u32m2x2_u32m2(src, index)
#define __riscv_vget_v_u32m2x3_u32m2(src, index) __riscv_th_vget_v_u32m2x3_u32m2(src, index)
#define __riscv_vget_v_u32m2x4_u32m2(src, index) __riscv_th_vget_v_u32m2x4_u32m2(src, index)
#define __riscv_vget_v_u32m4x2_u32m4(src, index) __riscv_th_vget_v_u32m4x2_u32m4(src, index)
#define __riscv_vget_v_u64m1x2_u64m1(src, index) __riscv_th_vget_v_u64m1x2_u64m1(src, index)
#define __riscv_vget_v_u64m1x3_u64m1(src, index) __riscv_th_vget_v_u64m1x3_u64m1(src, index)
#define __riscv_vget_v_u64m1x4_u64m1(src, index) __riscv_th_vget_v_u64m1x4_u64m1(src, index)
#define __riscv_vget_v_u64m1x5_u64m1(src, index) __riscv_th_vget_v_u64m1x5_u64m1(src, index)
#define __riscv_vget_v_u64m1x6_u64m1(src, index) __riscv_th_vget_v_u64m1x6_u64m1(src, index)
#define __riscv_vget_v_u64m1x7_u64m1(src, index) __riscv_th_vget_v_u64m1x7_u64m1(src, index)
#define __riscv_vget_v_u64m1x8_u64m1(src, index) __riscv_th_vget_v_u64m1x8_u64m1(src, index)
#define __riscv_vget_v_u64m2x2_u64m2(src, index) __riscv_th_vget_v_u64m2x2_u64m2(src, index)
#define __riscv_vget_v_u64m2x3_u64m2(src, index) __riscv_th_vget_v_u64m2x3_u64m2(src, index)
#define __riscv_vget_v_u64m2x4_u64m2(src, index) __riscv_th_vget_v_u64m2x4_u64m2(src, index)
#define __riscv_vget_v_u64m4x2_u64m4(src, index) __riscv_th_vget_v_u64m4x2_u64m4(src, index)


#define __riscv_vcreate_v_f16mf4x2(v0, v1) __riscv_th_vcreate_v_f16mf4x2(v0, v1)
#define __riscv_vcreate_v_f16mf4x3(v0, v1, v2) __riscv_th_vcreate_v_f16mf4x3(v0, v1, v2)
#define __riscv_vcreate_v_f16mf4x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f16mf4x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f16mf4x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_f16mf4x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_f16mf4x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_f16mf4x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_f16mf4x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_f16mf4x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_f16mf4x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_f16mf4x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_f16mf2x2(v0, v1) __riscv_th_vcreate_v_f16mf2x2(v0, v1)
#define __riscv_vcreate_v_f16mf2x3(v0, v1, v2) __riscv_th_vcreate_v_f16mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_f16mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f16mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f16mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_f16mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_f16mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_f16mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_f16mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_f16mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_f16mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_f16mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_f16m1x2(v0, v1) __riscv_th_vcreate_v_f16m1x2(v0, v1)
#define __riscv_vcreate_v_f16m1x3(v0, v1, v2) __riscv_th_vcreate_v_f16m1x3(v0, v1, v2)
#define __riscv_vcreate_v_f16m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f16m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f16m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_f16m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_f16m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_f16m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_f16m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_f16m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_f16m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_f16m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_f16m2x2(v0, v1) __riscv_th_vcreate_v_f16m2x2(v0, v1)
#define __riscv_vcreate_v_f16m2x3(v0, v1, v2) __riscv_th_vcreate_v_f16m2x3(v0, v1, v2)
#define __riscv_vcreate_v_f16m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f16m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f16m4x2(v0, v1) __riscv_th_vcreate_v_f16m4x2(v0, v1)
#define __riscv_vcreate_v_f32mf2x2(v0, v1) __riscv_th_vcreate_v_f32mf2x2(v0, v1)
#define __riscv_vcreate_v_f32mf2x3(v0, v1, v2) __riscv_th_vcreate_v_f32mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_f32mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f32mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f32mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_f32mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_f32mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_f32mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_f32mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_f32mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_f32mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_f32mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_f32m1x2(v0, v1) __riscv_th_vcreate_v_f32m1x2(v0, v1)
#define __riscv_vcreate_v_f32m1x3(v0, v1, v2) __riscv_th_vcreate_v_f32m1x3(v0, v1, v2)
#define __riscv_vcreate_v_f32m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f32m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f32m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_f32m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_f32m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_f32m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_f32m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_f32m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_f32m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_f32m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_f32m2x2(v0, v1) __riscv_th_vcreate_v_f32m2x2(v0, v1)
#define __riscv_vcreate_v_f32m2x3(v0, v1, v2) __riscv_th_vcreate_v_f32m2x3(v0, v1, v2)
#define __riscv_vcreate_v_f32m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f32m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f32m4x2(v0, v1) __riscv_th_vcreate_v_f32m4x2(v0, v1)
#define __riscv_vcreate_v_f64m1x2(v0, v1) __riscv_th_vcreate_v_f64m1x2(v0, v1)
#define __riscv_vcreate_v_f64m1x3(v0, v1, v2) __riscv_th_vcreate_v_f64m1x3(v0, v1, v2)
#define __riscv_vcreate_v_f64m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f64m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f64m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_f64m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_f64m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_f64m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_f64m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_f64m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_f64m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_f64m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_f64m2x2(v0, v1) __riscv_th_vcreate_v_f64m2x2(v0, v1)
#define __riscv_vcreate_v_f64m2x3(v0, v1, v2) __riscv_th_vcreate_v_f64m2x3(v0, v1, v2)
#define __riscv_vcreate_v_f64m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_f64m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_f64m4x2(v0, v1) __riscv_th_vcreate_v_f64m4x2(v0, v1)
#define __riscv_vcreate_v_i8mf8x2(v0, v1) __riscv_th_vcreate_v_i8mf8x2(v0, v1)
#define __riscv_vcreate_v_i8mf8x3(v0, v1, v2) __riscv_th_vcreate_v_i8mf8x3(v0, v1, v2)
#define __riscv_vcreate_v_i8mf8x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i8mf8x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i8mf8x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i8mf8x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i8mf8x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i8mf8x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i8mf8x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i8mf8x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i8mf8x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i8mf8x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i8mf4x2(v0, v1) __riscv_th_vcreate_v_i8mf4x2(v0, v1)
#define __riscv_vcreate_v_i8mf4x3(v0, v1, v2) __riscv_th_vcreate_v_i8mf4x3(v0, v1, v2)
#define __riscv_vcreate_v_i8mf4x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i8mf4x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i8mf4x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i8mf4x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i8mf4x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i8mf4x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i8mf4x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i8mf4x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i8mf4x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i8mf4x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i8mf2x2(v0, v1) __riscv_th_vcreate_v_i8mf2x2(v0, v1)
#define __riscv_vcreate_v_i8mf2x3(v0, v1, v2) __riscv_th_vcreate_v_i8mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_i8mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i8mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i8mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i8mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i8mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i8mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i8mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i8mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i8mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i8mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i8m1x2(v0, v1) __riscv_th_vcreate_v_i8m1x2(v0, v1)
#define __riscv_vcreate_v_i8m1x3(v0, v1, v2) __riscv_th_vcreate_v_i8m1x3(v0, v1, v2)
#define __riscv_vcreate_v_i8m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i8m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i8m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i8m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i8m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i8m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i8m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i8m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i8m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i8m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i8m2x2(v0, v1) __riscv_th_vcreate_v_i8m2x2(v0, v1)
#define __riscv_vcreate_v_i8m2x3(v0, v1, v2) __riscv_th_vcreate_v_i8m2x3(v0, v1, v2)
#define __riscv_vcreate_v_i8m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i8m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i8m4x2(v0, v1) __riscv_th_vcreate_v_i8m4x2(v0, v1)
#define __riscv_vcreate_v_i16mf4x2(v0, v1) __riscv_th_vcreate_v_i16mf4x2(v0, v1)
#define __riscv_vcreate_v_i16mf4x3(v0, v1, v2) __riscv_th_vcreate_v_i16mf4x3(v0, v1, v2)
#define __riscv_vcreate_v_i16mf4x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i16mf4x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i16mf4x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i16mf4x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i16mf4x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i16mf4x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i16mf4x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i16mf4x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i16mf4x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i16mf4x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i16mf2x2(v0, v1) __riscv_th_vcreate_v_i16mf2x2(v0, v1)
#define __riscv_vcreate_v_i16mf2x3(v0, v1, v2) __riscv_th_vcreate_v_i16mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_i16mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i16mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i16mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i16mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i16mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i16mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i16mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i16mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i16mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i16mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i16m1x2(v0, v1) __riscv_th_vcreate_v_i16m1x2(v0, v1)
#define __riscv_vcreate_v_i16m1x3(v0, v1, v2) __riscv_th_vcreate_v_i16m1x3(v0, v1, v2)
#define __riscv_vcreate_v_i16m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i16m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i16m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i16m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i16m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i16m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i16m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i16m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i16m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i16m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i16m2x2(v0, v1) __riscv_th_vcreate_v_i16m2x2(v0, v1)
#define __riscv_vcreate_v_i16m2x3(v0, v1, v2) __riscv_th_vcreate_v_i16m2x3(v0, v1, v2)
#define __riscv_vcreate_v_i16m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i16m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i16m4x2(v0, v1) __riscv_th_vcreate_v_i16m4x2(v0, v1)
#define __riscv_vcreate_v_i32mf2x2(v0, v1) __riscv_th_vcreate_v_i32mf2x2(v0, v1)
#define __riscv_vcreate_v_i32mf2x3(v0, v1, v2) __riscv_th_vcreate_v_i32mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_i32mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i32mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i32mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i32mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i32mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i32mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i32mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i32mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i32mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i32mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i32m1x2(v0, v1) __riscv_th_vcreate_v_i32m1x2(v0, v1)
#define __riscv_vcreate_v_i32m1x3(v0, v1, v2) __riscv_th_vcreate_v_i32m1x3(v0, v1, v2)
#define __riscv_vcreate_v_i32m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i32m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i32m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i32m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i32m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i32m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i32m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i32m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i32m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i32m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i32m2x2(v0, v1) __riscv_th_vcreate_v_i32m2x2(v0, v1)
#define __riscv_vcreate_v_i32m2x3(v0, v1, v2) __riscv_th_vcreate_v_i32m2x3(v0, v1, v2)
#define __riscv_vcreate_v_i32m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i32m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i32m4x2(v0, v1) __riscv_th_vcreate_v_i32m4x2(v0, v1)
#define __riscv_vcreate_v_i64m1x2(v0, v1) __riscv_th_vcreate_v_i64m1x2(v0, v1)
#define __riscv_vcreate_v_i64m1x3(v0, v1, v2) __riscv_th_vcreate_v_i64m1x3(v0, v1, v2)
#define __riscv_vcreate_v_i64m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i64m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i64m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_i64m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_i64m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_i64m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_i64m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_i64m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_i64m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_i64m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_i64m2x2(v0, v1) __riscv_th_vcreate_v_i64m2x2(v0, v1)
#define __riscv_vcreate_v_i64m2x3(v0, v1, v2) __riscv_th_vcreate_v_i64m2x3(v0, v1, v2)
#define __riscv_vcreate_v_i64m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_i64m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_i64m4x2(v0, v1) __riscv_th_vcreate_v_i64m4x2(v0, v1)
#define __riscv_vcreate_v_u8mf8x2(v0, v1) __riscv_th_vcreate_v_u8mf8x2(v0, v1)
#define __riscv_vcreate_v_u8mf8x3(v0, v1, v2) __riscv_th_vcreate_v_u8mf8x3(v0, v1, v2)
#define __riscv_vcreate_v_u8mf8x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u8mf8x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u8mf8x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u8mf8x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u8mf8x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u8mf8x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u8mf8x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u8mf8x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u8mf8x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u8mf8x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u8mf4x2(v0, v1) __riscv_th_vcreate_v_u8mf4x2(v0, v1)
#define __riscv_vcreate_v_u8mf4x3(v0, v1, v2) __riscv_th_vcreate_v_u8mf4x3(v0, v1, v2)
#define __riscv_vcreate_v_u8mf4x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u8mf4x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u8mf4x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u8mf4x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u8mf4x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u8mf4x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u8mf4x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u8mf4x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u8mf4x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u8mf4x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u8mf2x2(v0, v1) __riscv_th_vcreate_v_u8mf2x2(v0, v1)
#define __riscv_vcreate_v_u8mf2x3(v0, v1, v2) __riscv_th_vcreate_v_u8mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_u8mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u8mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u8mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u8mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u8mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u8mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u8mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u8mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u8mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u8mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u8m1x2(v0, v1) __riscv_th_vcreate_v_u8m1x2(v0, v1)
#define __riscv_vcreate_v_u8m1x3(v0, v1, v2) __riscv_th_vcreate_v_u8m1x3(v0, v1, v2)
#define __riscv_vcreate_v_u8m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u8m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u8m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u8m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u8m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u8m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u8m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u8m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u8m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u8m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u8m2x2(v0, v1) __riscv_th_vcreate_v_u8m2x2(v0, v1)
#define __riscv_vcreate_v_u8m2x3(v0, v1, v2) __riscv_th_vcreate_v_u8m2x3(v0, v1, v2)
#define __riscv_vcreate_v_u8m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u8m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u8m4x2(v0, v1) __riscv_th_vcreate_v_u8m4x2(v0, v1)
#define __riscv_vcreate_v_u16mf4x2(v0, v1) __riscv_th_vcreate_v_u16mf4x2(v0, v1)
#define __riscv_vcreate_v_u16mf4x3(v0, v1, v2) __riscv_th_vcreate_v_u16mf4x3(v0, v1, v2)
#define __riscv_vcreate_v_u16mf4x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u16mf4x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u16mf4x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u16mf4x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u16mf4x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u16mf4x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u16mf4x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u16mf4x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u16mf4x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u16mf4x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u16mf2x2(v0, v1) __riscv_th_vcreate_v_u16mf2x2(v0, v1)
#define __riscv_vcreate_v_u16mf2x3(v0, v1, v2) __riscv_th_vcreate_v_u16mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_u16mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u16mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u16mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u16mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u16mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u16mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u16mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u16mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u16mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u16mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u16m1x2(v0, v1) __riscv_th_vcreate_v_u16m1x2(v0, v1)
#define __riscv_vcreate_v_u16m1x3(v0, v1, v2) __riscv_th_vcreate_v_u16m1x3(v0, v1, v2)
#define __riscv_vcreate_v_u16m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u16m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u16m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u16m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u16m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u16m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u16m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u16m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u16m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u16m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u16m2x2(v0, v1) __riscv_th_vcreate_v_u16m2x2(v0, v1)
#define __riscv_vcreate_v_u16m2x3(v0, v1, v2) __riscv_th_vcreate_v_u16m2x3(v0, v1, v2)
#define __riscv_vcreate_v_u16m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u16m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u16m4x2(v0, v1) __riscv_th_vcreate_v_u16m4x2(v0, v1)
#define __riscv_vcreate_v_u32mf2x2(v0, v1) __riscv_th_vcreate_v_u32mf2x2(v0, v1)
#define __riscv_vcreate_v_u32mf2x3(v0, v1, v2) __riscv_th_vcreate_v_u32mf2x3(v0, v1, v2)
#define __riscv_vcreate_v_u32mf2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u32mf2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u32mf2x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u32mf2x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u32mf2x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u32mf2x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u32mf2x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u32mf2x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u32mf2x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u32mf2x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u32m1x2(v0, v1) __riscv_th_vcreate_v_u32m1x2(v0, v1)
#define __riscv_vcreate_v_u32m1x3(v0, v1, v2) __riscv_th_vcreate_v_u32m1x3(v0, v1, v2)
#define __riscv_vcreate_v_u32m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u32m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u32m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u32m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u32m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u32m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u32m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u32m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u32m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u32m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u32m2x2(v0, v1) __riscv_th_vcreate_v_u32m2x2(v0, v1)
#define __riscv_vcreate_v_u32m2x3(v0, v1, v2) __riscv_th_vcreate_v_u32m2x3(v0, v1, v2)
#define __riscv_vcreate_v_u32m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u32m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u32m4x2(v0, v1) __riscv_th_vcreate_v_u32m4x2(v0, v1)
#define __riscv_vcreate_v_u64m1x2(v0, v1) __riscv_th_vcreate_v_u64m1x2(v0, v1)
#define __riscv_vcreate_v_u64m1x3(v0, v1, v2) __riscv_th_vcreate_v_u64m1x3(v0, v1, v2)
#define __riscv_vcreate_v_u64m1x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u64m1x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u64m1x5(v0, v1, v2, v3, v4) __riscv_th_vcreate_v_u64m1x5(v0, v1, v2, v3, v4)
#define __riscv_vcreate_v_u64m1x6(v0, v1, v2, v3, v4, v5) __riscv_th_vcreate_v_u64m1x6(v0, v1, v2, v3, v4, v5)
#define __riscv_vcreate_v_u64m1x7(v0, v1, v2, v3, v4, v5, v6) __riscv_th_vcreate_v_u64m1x7(v0, v1, v2, v3, v4, v5, v6)
#define __riscv_vcreate_v_u64m1x8(v0, v1, v2, v3, v4, v5, v6, v7) __riscv_th_vcreate_v_u64m1x8(v0, v1, v2, v3, v4, v5, v6, v7)
#define __riscv_vcreate_v_u64m2x2(v0, v1) __riscv_th_vcreate_v_u64m2x2(v0, v1)
#define __riscv_vcreate_v_u64m2x3(v0, v1, v2) __riscv_th_vcreate_v_u64m2x3(v0, v1, v2)
#define __riscv_vcreate_v_u64m2x4(v0, v1, v2, v3) __riscv_th_vcreate_v_u64m2x4(v0, v1, v2, v3)
#define __riscv_vcreate_v_u64m4x2(v0, v1) __riscv_th_vcreate_v_u64m4x2(v0, v1)

}] in
def th_vector_misc_wrapper_macros: RVVHeader;
