//==--- riscv_vector_xtheadv.td - RISC-V V-ext Builtin function list ------===//
//
//  Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
//  See https://llvm.org/LICENSE.txt for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the builtins for RISC-V V-extension. See:
//
//     https://github.com/riscv-non-isa/rvv-intrinsic-doc/tree/v0.7.1
//
//===----------------------------------------------------------------------===//

//===----------------------------------------------------------------------===//
// 7. Vector Loads and Stores
//===----------------------------------------------------------------------===//

let HeaderCode =
[{
// Vector Unit-stride loads
#define __riscv_vlb_v_i8m1(src_ptr, vl) __riscv_th_vlb_v_i8m1(src_ptr, vl)
#define __riscv_vlb_v_i8m2(src_ptr, vl) __riscv_th_vlb_v_i8m2(src_ptr, vl)
#define __riscv_vlb_v_i8m4(src_ptr, vl) __riscv_th_vlb_v_i8m4(src_ptr, vl)
#define __riscv_vlb_v_i8m8(src_ptr, vl) __riscv_th_vlb_v_i8m8(src_ptr, vl)
#define __riscv_vlb_v_i16m1(src_ptr, vl) __riscv_th_vlb_v_i16m1(src_ptr, vl)
#define __riscv_vlb_v_i16m2(src_ptr, vl) __riscv_th_vlb_v_i16m2(src_ptr, vl)
#define __riscv_vlb_v_i16m4(src_ptr, vl) __riscv_th_vlb_v_i16m4(src_ptr, vl)
#define __riscv_vlb_v_i16m8(src_ptr, vl) __riscv_th_vlb_v_i16m8(src_ptr, vl)
#define __riscv_vlb_v_i32m1(src_ptr, vl) __riscv_th_vlb_v_i32m1(src_ptr, vl)
#define __riscv_vlb_v_i32m2(src_ptr, vl) __riscv_th_vlb_v_i32m2(src_ptr, vl)
#define __riscv_vlb_v_i32m4(src_ptr, vl) __riscv_th_vlb_v_i32m4(src_ptr, vl)
#define __riscv_vlb_v_i32m8(src_ptr, vl) __riscv_th_vlb_v_i32m8(src_ptr, vl)
#define __riscv_vlb_v_i64m1(src_ptr, vl) __riscv_th_vlb_v_i64m1(src_ptr, vl)
#define __riscv_vlb_v_i64m2(src_ptr, vl) __riscv_th_vlb_v_i64m2(src_ptr, vl)
#define __riscv_vlb_v_i64m4(src_ptr, vl) __riscv_th_vlb_v_i64m4(src_ptr, vl)
#define __riscv_vlb_v_i64m8(src_ptr, vl) __riscv_th_vlb_v_i64m8(src_ptr, vl)
#define __riscv_vlh_v_i8m1(src_ptr, vl) __riscv_th_vlh_v_i8m1(src_ptr, vl)
#define __riscv_vlh_v_i8m2(src_ptr, vl) __riscv_th_vlh_v_i8m2(src_ptr, vl)
#define __riscv_vlh_v_i8m4(src_ptr, vl) __riscv_th_vlh_v_i8m4(src_ptr, vl)
#define __riscv_vlh_v_i8m8(src_ptr, vl) __riscv_th_vlh_v_i8m8(src_ptr, vl)
#define __riscv_vlh_v_i16m1(src_ptr, vl) __riscv_th_vlh_v_i16m1(src_ptr, vl)
#define __riscv_vlh_v_i16m2(src_ptr, vl) __riscv_th_vlh_v_i16m2(src_ptr, vl)
#define __riscv_vlh_v_i16m4(src_ptr, vl) __riscv_th_vlh_v_i16m4(src_ptr, vl)
#define __riscv_vlh_v_i16m8(src_ptr, vl) __riscv_th_vlh_v_i16m8(src_ptr, vl)
#define __riscv_vlh_v_i32m1(src_ptr, vl) __riscv_th_vlh_v_i32m1(src_ptr, vl)
#define __riscv_vlh_v_i32m2(src_ptr, vl) __riscv_th_vlh_v_i32m2(src_ptr, vl)
#define __riscv_vlh_v_i32m4(src_ptr, vl) __riscv_th_vlh_v_i32m4(src_ptr, vl)
#define __riscv_vlh_v_i32m8(src_ptr, vl) __riscv_th_vlh_v_i32m8(src_ptr, vl)
#define __riscv_vlh_v_i64m1(src_ptr, vl) __riscv_th_vlh_v_i64m1(src_ptr, vl)
#define __riscv_vlh_v_i64m2(src_ptr, vl) __riscv_th_vlh_v_i64m2(src_ptr, vl)
#define __riscv_vlh_v_i64m4(src_ptr, vl) __riscv_th_vlh_v_i64m4(src_ptr, vl)
#define __riscv_vlh_v_i64m8(src_ptr, vl) __riscv_th_vlh_v_i64m8(src_ptr, vl)
#define __riscv_vlw_v_i8m1(src_ptr, vl) __riscv_th_vlw_v_i8m1(src_ptr, vl)
#define __riscv_vlw_v_i8m2(src_ptr, vl) __riscv_th_vlw_v_i8m2(src_ptr, vl)
#define __riscv_vlw_v_i8m4(src_ptr, vl) __riscv_th_vlw_v_i8m4(src_ptr, vl)
#define __riscv_vlw_v_i8m8(src_ptr, vl) __riscv_th_vlw_v_i8m8(src_ptr, vl)
#define __riscv_vlw_v_i16m1(src_ptr, vl) __riscv_th_vlw_v_i16m1(src_ptr, vl)
#define __riscv_vlw_v_i16m2(src_ptr, vl) __riscv_th_vlw_v_i16m2(src_ptr, vl)
#define __riscv_vlw_v_i16m4(src_ptr, vl) __riscv_th_vlw_v_i16m4(src_ptr, vl)
#define __riscv_vlw_v_i16m8(src_ptr, vl) __riscv_th_vlw_v_i16m8(src_ptr, vl)
#define __riscv_vlw_v_i32m1(src_ptr, vl) __riscv_th_vlw_v_i32m1(src_ptr, vl)
#define __riscv_vlw_v_i32m2(src_ptr, vl) __riscv_th_vlw_v_i32m2(src_ptr, vl)
#define __riscv_vlw_v_i32m4(src_ptr, vl) __riscv_th_vlw_v_i32m4(src_ptr, vl)
#define __riscv_vlw_v_i32m8(src_ptr, vl) __riscv_th_vlw_v_i32m8(src_ptr, vl)
#define __riscv_vlw_v_i64m1(src_ptr, vl) __riscv_th_vlw_v_i64m1(src_ptr, vl)
#define __riscv_vlw_v_i64m2(src_ptr, vl) __riscv_th_vlw_v_i64m2(src_ptr, vl)
#define __riscv_vlw_v_i64m4(src_ptr, vl) __riscv_th_vlw_v_i64m4(src_ptr, vl)
#define __riscv_vlw_v_i64m8(src_ptr, vl) __riscv_th_vlw_v_i64m8(src_ptr, vl)
#define __riscv_vlbu_v_u8m1(src_ptr, vl) __riscv_th_vlbu_v_u8m1(src_ptr, vl)
#define __riscv_vlbu_v_u8m2(src_ptr, vl) __riscv_th_vlbu_v_u8m2(src_ptr, vl)
#define __riscv_vlbu_v_u8m4(src_ptr, vl) __riscv_th_vlbu_v_u8m4(src_ptr, vl)
#define __riscv_vlbu_v_u8m8(src_ptr, vl) __riscv_th_vlbu_v_u8m8(src_ptr, vl)
#define __riscv_vlbu_v_u16m1(src_ptr, vl) __riscv_th_vlbu_v_u16m1(src_ptr, vl)
#define __riscv_vlbu_v_u16m2(src_ptr, vl) __riscv_th_vlbu_v_u16m2(src_ptr, vl)
#define __riscv_vlbu_v_u16m4(src_ptr, vl) __riscv_th_vlbu_v_u16m4(src_ptr, vl)
#define __riscv_vlbu_v_u16m8(src_ptr, vl) __riscv_th_vlbu_v_u16m8(src_ptr, vl)
#define __riscv_vlbu_v_u32m1(src_ptr, vl) __riscv_th_vlbu_v_u32m1(src_ptr, vl)
#define __riscv_vlbu_v_u32m2(src_ptr, vl) __riscv_th_vlbu_v_u32m2(src_ptr, vl)
#define __riscv_vlbu_v_u32m4(src_ptr, vl) __riscv_th_vlbu_v_u32m4(src_ptr, vl)
#define __riscv_vlbu_v_u32m8(src_ptr, vl) __riscv_th_vlbu_v_u32m8(src_ptr, vl)
#define __riscv_vlbu_v_u64m1(src_ptr, vl) __riscv_th_vlbu_v_u64m1(src_ptr, vl)
#define __riscv_vlbu_v_u64m2(src_ptr, vl) __riscv_th_vlbu_v_u64m2(src_ptr, vl)
#define __riscv_vlbu_v_u64m4(src_ptr, vl) __riscv_th_vlbu_v_u64m4(src_ptr, vl)
#define __riscv_vlbu_v_u64m8(src_ptr, vl) __riscv_th_vlbu_v_u64m8(src_ptr, vl)
#define __riscv_vlhu_v_u8m1(src_ptr, vl) __riscv_th_vlhu_v_u8m1(src_ptr, vl)
#define __riscv_vlhu_v_u8m2(src_ptr, vl) __riscv_th_vlhu_v_u8m2(src_ptr, vl)
#define __riscv_vlhu_v_u8m4(src_ptr, vl) __riscv_th_vlhu_v_u8m4(src_ptr, vl)
#define __riscv_vlhu_v_u8m8(src_ptr, vl) __riscv_th_vlhu_v_u8m8(src_ptr, vl)
#define __riscv_vlhu_v_u16m1(src_ptr, vl) __riscv_th_vlhu_v_u16m1(src_ptr, vl)
#define __riscv_vlhu_v_u16m2(src_ptr, vl) __riscv_th_vlhu_v_u16m2(src_ptr, vl)
#define __riscv_vlhu_v_u16m4(src_ptr, vl) __riscv_th_vlhu_v_u16m4(src_ptr, vl)
#define __riscv_vlhu_v_u16m8(src_ptr, vl) __riscv_th_vlhu_v_u16m8(src_ptr, vl)
#define __riscv_vlhu_v_u32m1(src_ptr, vl) __riscv_th_vlhu_v_u32m1(src_ptr, vl)
#define __riscv_vlhu_v_u32m2(src_ptr, vl) __riscv_th_vlhu_v_u32m2(src_ptr, vl)
#define __riscv_vlhu_v_u32m4(src_ptr, vl) __riscv_th_vlhu_v_u32m4(src_ptr, vl)
#define __riscv_vlhu_v_u32m8(src_ptr, vl) __riscv_th_vlhu_v_u32m8(src_ptr, vl)
#define __riscv_vlhu_v_u64m1(src_ptr, vl) __riscv_th_vlhu_v_u64m1(src_ptr, vl)
#define __riscv_vlhu_v_u64m2(src_ptr, vl) __riscv_th_vlhu_v_u64m2(src_ptr, vl)
#define __riscv_vlhu_v_u64m4(src_ptr, vl) __riscv_th_vlhu_v_u64m4(src_ptr, vl)
#define __riscv_vlhu_v_u64m8(src_ptr, vl) __riscv_th_vlhu_v_u64m8(src_ptr, vl)
#define __riscv_vlwu_v_u8m1(src_ptr, vl) __riscv_th_vlwu_v_u8m1(src_ptr, vl)
#define __riscv_vlwu_v_u8m2(src_ptr, vl) __riscv_th_vlwu_v_u8m2(src_ptr, vl)
#define __riscv_vlwu_v_u8m4(src_ptr, vl) __riscv_th_vlwu_v_u8m4(src_ptr, vl)
#define __riscv_vlwu_v_u8m8(src_ptr, vl) __riscv_th_vlwu_v_u8m8(src_ptr, vl)
#define __riscv_vlwu_v_u16m1(src_ptr, vl) __riscv_th_vlwu_v_u16m1(src_ptr, vl)
#define __riscv_vlwu_v_u16m2(src_ptr, vl) __riscv_th_vlwu_v_u16m2(src_ptr, vl)
#define __riscv_vlwu_v_u16m4(src_ptr, vl) __riscv_th_vlwu_v_u16m4(src_ptr, vl)
#define __riscv_vlwu_v_u16m8(src_ptr, vl) __riscv_th_vlwu_v_u16m8(src_ptr, vl)
#define __riscv_vlwu_v_u32m1(src_ptr, vl) __riscv_th_vlwu_v_u32m1(src_ptr, vl)
#define __riscv_vlwu_v_u32m2(src_ptr, vl) __riscv_th_vlwu_v_u32m2(src_ptr, vl)
#define __riscv_vlwu_v_u32m4(src_ptr, vl) __riscv_th_vlwu_v_u32m4(src_ptr, vl)
#define __riscv_vlwu_v_u32m8(src_ptr, vl) __riscv_th_vlwu_v_u32m8(src_ptr, vl)
#define __riscv_vlwu_v_u64m1(src_ptr, vl) __riscv_th_vlwu_v_u64m1(src_ptr, vl)
#define __riscv_vlwu_v_u64m2(src_ptr, vl) __riscv_th_vlwu_v_u64m2(src_ptr, vl)
#define __riscv_vlwu_v_u64m4(src_ptr, vl) __riscv_th_vlwu_v_u64m4(src_ptr, vl)
#define __riscv_vlwu_v_u64m8(src_ptr, vl) __riscv_th_vlwu_v_u64m8(src_ptr, vl)
#define __riscv_vle8_v_i8m1(src_ptr, vl) __riscv_th_vle8_v_i8m1(src_ptr, vl)
#define __riscv_vle8_v_i8m2(src_ptr, vl) __riscv_th_vle8_v_i8m2(src_ptr, vl)
#define __riscv_vle8_v_i8m4(src_ptr, vl) __riscv_th_vle8_v_i8m4(src_ptr, vl)
#define __riscv_vle8_v_i8m8(src_ptr, vl) __riscv_th_vle8_v_i8m8(src_ptr, vl)
#define __riscv_vle16_v_i16m1(src_ptr, vl) __riscv_th_vle16_v_i16m1(src_ptr, vl)
#define __riscv_vle16_v_i16m2(src_ptr, vl) __riscv_th_vle16_v_i16m2(src_ptr, vl)
#define __riscv_vle16_v_i16m4(src_ptr, vl) __riscv_th_vle16_v_i16m4(src_ptr, vl)
#define __riscv_vle16_v_i16m8(src_ptr, vl) __riscv_th_vle16_v_i16m8(src_ptr, vl)
#define __riscv_vle32_v_i32m1(src_ptr, vl) __riscv_th_vle32_v_i32m1(src_ptr, vl)
#define __riscv_vle32_v_i32m2(src_ptr, vl) __riscv_th_vle32_v_i32m2(src_ptr, vl)
#define __riscv_vle32_v_i32m4(src_ptr, vl) __riscv_th_vle32_v_i32m4(src_ptr, vl)
#define __riscv_vle32_v_i32m8(src_ptr, vl) __riscv_th_vle32_v_i32m8(src_ptr, vl)
#define __riscv_vle64_v_i64m1(src_ptr, vl) __riscv_th_vle64_v_i64m1(src_ptr, vl)
#define __riscv_vle64_v_i64m2(src_ptr, vl) __riscv_th_vle64_v_i64m2(src_ptr, vl)
#define __riscv_vle64_v_i64m4(src_ptr, vl) __riscv_th_vle64_v_i64m4(src_ptr, vl)
#define __riscv_vle64_v_i64m8(src_ptr, vl) __riscv_th_vle64_v_i64m8(src_ptr, vl)
#define __riscv_vle8_v_u8m1(src_ptr, vl) __riscv_th_vle8_v_u8m1(src_ptr, vl)
#define __riscv_vle8_v_u8m2(src_ptr, vl) __riscv_th_vle8_v_u8m2(src_ptr, vl)
#define __riscv_vle8_v_u8m4(src_ptr, vl) __riscv_th_vle8_v_u8m4(src_ptr, vl)
#define __riscv_vle8_v_u8m8(src_ptr, vl) __riscv_th_vle8_v_u8m8(src_ptr, vl)
#define __riscv_vle16_v_u16m1(src_ptr, vl) __riscv_th_vle16_v_u16m1(src_ptr, vl)
#define __riscv_vle16_v_u16m2(src_ptr, vl) __riscv_th_vle16_v_u16m2(src_ptr, vl)
#define __riscv_vle16_v_u16m4(src_ptr, vl) __riscv_th_vle16_v_u16m4(src_ptr, vl)
#define __riscv_vle16_v_u16m8(src_ptr, vl) __riscv_th_vle16_v_u16m8(src_ptr, vl)
#define __riscv_vle32_v_u32m1(src_ptr, vl) __riscv_th_vle32_v_u32m1(src_ptr, vl)
#define __riscv_vle32_v_u32m2(src_ptr, vl) __riscv_th_vle32_v_u32m2(src_ptr, vl)
#define __riscv_vle32_v_u32m4(src_ptr, vl) __riscv_th_vle32_v_u32m4(src_ptr, vl)
#define __riscv_vle32_v_u32m8(src_ptr, vl) __riscv_th_vle32_v_u32m8(src_ptr, vl)
#define __riscv_vle64_v_u64m1(src_ptr, vl) __riscv_th_vle64_v_u64m1(src_ptr, vl)
#define __riscv_vle64_v_u64m2(src_ptr, vl) __riscv_th_vle64_v_u64m2(src_ptr, vl)
#define __riscv_vle64_v_u64m4(src_ptr, vl) __riscv_th_vle64_v_u64m4(src_ptr, vl)
#define __riscv_vle64_v_u64m8(src_ptr, vl) __riscv_th_vle64_v_u64m8(src_ptr, vl)
#define __riscv_vle16_v_f16m1(src_ptr, vl) __riscv_th_vle16_v_f16m1(src_ptr, vl)
#define __riscv_vle16_v_f16m2(src_ptr, vl) __riscv_th_vle16_v_f16m2(src_ptr, vl)
#define __riscv_vle16_v_f16m4(src_ptr, vl) __riscv_th_vle16_v_f16m4(src_ptr, vl)
#define __riscv_vle16_v_f16m8(src_ptr, vl) __riscv_th_vle16_v_f16m8(src_ptr, vl)
#define __riscv_vle32_v_f32m1(src_ptr, vl) __riscv_th_vle32_v_f32m1(src_ptr, vl)
#define __riscv_vle32_v_f32m2(src_ptr, vl) __riscv_th_vle32_v_f32m2(src_ptr, vl)
#define __riscv_vle32_v_f32m4(src_ptr, vl) __riscv_th_vle32_v_f32m4(src_ptr, vl)
#define __riscv_vle32_v_f32m8(src_ptr, vl) __riscv_th_vle32_v_f32m8(src_ptr, vl)
#define __riscv_vle64_v_f64m1(src_ptr, vl) __riscv_th_vle64_v_f64m1(src_ptr, vl)
#define __riscv_vle64_v_f64m2(src_ptr, vl) __riscv_th_vle64_v_f64m2(src_ptr, vl)
#define __riscv_vle64_v_f64m4(src_ptr, vl) __riscv_th_vle64_v_f64m4(src_ptr, vl)
#define __riscv_vle64_v_f64m8(src_ptr, vl) __riscv_th_vle64_v_f64m8(src_ptr, vl)

// Vector Unit-stride stores
#define __riscv_vsb_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vsb_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vsb_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vsh_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vsh_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vsw_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vsw_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_i8m1(dst_ptr, vector_value, vl) __riscv_th_vse8_v_i8m1(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_i8m2(dst_ptr, vector_value, vl) __riscv_th_vse8_v_i8m2(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_i8m4(dst_ptr, vector_value, vl) __riscv_th_vse8_v_i8m4(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_i8m8(dst_ptr, vector_value, vl) __riscv_th_vse8_v_i8m8(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_i16m1(dst_ptr, vector_value, vl) __riscv_th_vse16_v_i16m1(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_i16m2(dst_ptr, vector_value, vl) __riscv_th_vse16_v_i16m2(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_i16m4(dst_ptr, vector_value, vl) __riscv_th_vse16_v_i16m4(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_i16m8(dst_ptr, vector_value, vl) __riscv_th_vse16_v_i16m8(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_i32m1(dst_ptr, vector_value, vl) __riscv_th_vse32_v_i32m1(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_i32m2(dst_ptr, vector_value, vl) __riscv_th_vse32_v_i32m2(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_i32m4(dst_ptr, vector_value, vl) __riscv_th_vse32_v_i32m4(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_i32m8(dst_ptr, vector_value, vl) __riscv_th_vse32_v_i32m8(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_i64m1(dst_ptr, vector_value, vl) __riscv_th_vse64_v_i64m1(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_i64m2(dst_ptr, vector_value, vl) __riscv_th_vse64_v_i64m2(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_i64m4(dst_ptr, vector_value, vl) __riscv_th_vse64_v_i64m4(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_i64m8(dst_ptr, vector_value, vl) __riscv_th_vse64_v_i64m8(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_u8m1(dst_ptr, vector_value, vl) __riscv_th_vse8_v_u8m1(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_u8m2(dst_ptr, vector_value, vl) __riscv_th_vse8_v_u8m2(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_u8m4(dst_ptr, vector_value, vl) __riscv_th_vse8_v_u8m4(dst_ptr, vector_value, vl)
#define __riscv_vse8_v_u8m8(dst_ptr, vector_value, vl) __riscv_th_vse8_v_u8m8(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_u16m1(dst_ptr, vector_value, vl) __riscv_th_vse16_v_u16m1(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_u16m2(dst_ptr, vector_value, vl) __riscv_th_vse16_v_u16m2(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_u16m4(dst_ptr, vector_value, vl) __riscv_th_vse16_v_u16m4(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_u16m8(dst_ptr, vector_value, vl) __riscv_th_vse16_v_u16m8(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_u32m1(dst_ptr, vector_value, vl) __riscv_th_vse32_v_u32m1(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_u32m2(dst_ptr, vector_value, vl) __riscv_th_vse32_v_u32m2(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_u32m4(dst_ptr, vector_value, vl) __riscv_th_vse32_v_u32m4(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_u32m8(dst_ptr, vector_value, vl) __riscv_th_vse32_v_u32m8(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_u64m1(dst_ptr, vector_value, vl) __riscv_th_vse64_v_u64m1(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_u64m2(dst_ptr, vector_value, vl) __riscv_th_vse64_v_u64m2(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_u64m4(dst_ptr, vector_value, vl) __riscv_th_vse64_v_u64m4(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_u64m8(dst_ptr, vector_value, vl) __riscv_th_vse64_v_u64m8(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_f16m1(dst_ptr, vector_value, vl) __riscv_th_vse16_v_f16m1(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_f16m2(dst_ptr, vector_value, vl) __riscv_th_vse16_v_f16m2(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_f16m4(dst_ptr, vector_value, vl) __riscv_th_vse16_v_f16m4(dst_ptr, vector_value, vl)
#define __riscv_vse16_v_f16m8(dst_ptr, vector_value, vl) __riscv_th_vse16_v_f16m8(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_f32m1(dst_ptr, vector_value, vl) __riscv_th_vse32_v_f32m1(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_f32m2(dst_ptr, vector_value, vl) __riscv_th_vse32_v_f32m2(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_f32m4(dst_ptr, vector_value, vl) __riscv_th_vse32_v_f32m4(dst_ptr, vector_value, vl)
#define __riscv_vse32_v_f32m8(dst_ptr, vector_value, vl) __riscv_th_vse32_v_f32m8(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_f64m1(dst_ptr, vector_value, vl) __riscv_th_vse64_v_f64m1(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_f64m2(dst_ptr, vector_value, vl) __riscv_th_vse64_v_f64m2(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_f64m4(dst_ptr, vector_value, vl) __riscv_th_vse64_v_f64m4(dst_ptr, vector_value, vl)
#define __riscv_vse64_v_f64m8(dst_ptr, vector_value, vl) __riscv_th_vse64_v_f64m8(dst_ptr, vector_value, vl)

}] in
def th_unit_stride_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Strided loads
#define __riscv_vlsb_v_i8m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i8m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i8m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i8m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i16m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i32m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m1(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m2(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m4(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlsb_v_i64m8(src_ptr, stride, vl) __riscv_th_vlsb_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i8m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i16m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i32m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m1(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m2(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m4(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlsh_v_i64m8(src_ptr, stride, vl) __riscv_th_vlsh_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i8m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i16m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i32m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m1(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m2(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m4(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlsw_v_i64m8(src_ptr, stride, vl) __riscv_th_vlsw_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u8m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u16m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u32m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m1(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m2(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m4(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlsbu_v_u64m8(src_ptr, stride, vl) __riscv_th_vlsbu_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u8m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u16m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u32m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m1(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m2(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m4(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlshu_v_u64m8(src_ptr, stride, vl) __riscv_th_vlshu_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u8m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u16m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u32m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m1(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m2(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m4(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlswu_v_u64m8(src_ptr, stride, vl) __riscv_th_vlswu_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m1(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m1(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m2(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m2(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m4(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m4(src_ptr, stride, vl)
#define __riscv_vlse8_v_i8m8(src_ptr, stride, vl) __riscv_th_vlse8_v_i8m8(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m1(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m1(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m2(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m2(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m4(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m4(src_ptr, stride, vl)
#define __riscv_vlse16_v_i16m8(src_ptr, stride, vl) __riscv_th_vlse16_v_i16m8(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m1(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m1(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m2(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m2(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m4(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m4(src_ptr, stride, vl)
#define __riscv_vlse32_v_i32m8(src_ptr, stride, vl) __riscv_th_vlse32_v_i32m8(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m1(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m1(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m2(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m2(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m4(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m4(src_ptr, stride, vl)
#define __riscv_vlse64_v_i64m8(src_ptr, stride, vl) __riscv_th_vlse64_v_i64m8(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m1(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m1(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m2(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m2(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m4(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m4(src_ptr, stride, vl)
#define __riscv_vlse8_v_u8m8(src_ptr, stride, vl) __riscv_th_vlse8_v_u8m8(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m1(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m1(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m2(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m2(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m4(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m4(src_ptr, stride, vl)
#define __riscv_vlse16_v_u16m8(src_ptr, stride, vl) __riscv_th_vlse16_v_u16m8(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m1(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m1(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m2(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m2(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m4(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m4(src_ptr, stride, vl)
#define __riscv_vlse32_v_u32m8(src_ptr, stride, vl) __riscv_th_vlse32_v_u32m8(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m1(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m1(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m2(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m2(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m4(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m4(src_ptr, stride, vl)
#define __riscv_vlse64_v_u64m8(src_ptr, stride, vl) __riscv_th_vlse64_v_u64m8(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m1(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m1(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m2(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m2(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m4(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m4(src_ptr, stride, vl)
#define __riscv_vlse16_v_f16m8(src_ptr, stride, vl) __riscv_th_vlse16_v_f16m8(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m1(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m1(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m2(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m2(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m4(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m4(src_ptr, stride, vl)
#define __riscv_vlse32_v_f32m8(src_ptr, stride, vl) __riscv_th_vlse32_v_f32m8(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m1(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m1(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m2(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m2(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m4(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m4(src_ptr, stride, vl)
#define __riscv_vlse64_v_f64m8(src_ptr, stride, vl) __riscv_th_vlse64_v_f64m8(src_ptr, stride, vl)

// Vector Strided stores
#define __riscv_vssb_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssb_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssb_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssh_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssh_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vssw_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vssw_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_i8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_i8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_i8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_i8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_i8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_i8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_i8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_i8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_i16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_i16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_i16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_i16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_i16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_i16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_i16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_i16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_i32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_i32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_i32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_i32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_i32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_i32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_i32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_i32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_i64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_i64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_i64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_i64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_i64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_i64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_i64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_i64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_u8m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_u8m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_u8m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_u8m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_u8m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_u8m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse8_v_u8m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse8_v_u8m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_u16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_u16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_u16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_u16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_u16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_u16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_u16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_u16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_u32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_u32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_u32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_u32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_u32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_u32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_u32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_u32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_u64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_u64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_u64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_u64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_u64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_u64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_u64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_u64m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_f16m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_f16m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_f16m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_f16m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_f16m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_f16m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse16_v_f16m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse16_v_f16m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_f32m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_f32m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_f32m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_f32m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_f32m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_f32m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse32_v_f32m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse32_v_f32m8(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_f64m1(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_f64m1(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_f64m2(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_f64m2(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_f64m4(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_f64m4(dst_ptr, stride, vector_value, vl)
#define __riscv_vsse64_v_f64m8(dst_ptr, stride, vector_value, vl) __riscv_th_vsse64_v_f64m8(dst_ptr, stride, vector_value, vl)

}] in
def th_strided_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Unit-stride Fault-Only-First loads
#define __riscv_vle8ff_v_i8m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_i8m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_i8m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_i8m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_i8m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_i16m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_i16m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_i32m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_i32m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_i64m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_i64m8(src_ptr, new_vl_ptr, vl)

#define __riscv_vle8ff_v_u8m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_u8m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_u8m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle8ff_v_u8m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle8ff_v_u8m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_u16m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_u16m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_u32m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_u32m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_u64m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_u64m8(src_ptr, new_vl_ptr, vl)

#define __riscv_vle16ff_v_f16m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_f16m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_f16m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle16ff_v_f16m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle16ff_v_f16m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle32ff_v_f32m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle32ff_v_f32m8(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m1(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m1(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m2(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m2(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m4(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m4(src_ptr, new_vl_ptr, vl)
#define __riscv_vle64ff_v_f64m8(src_ptr, new_vl_ptr, vl) __riscv_th_vle64ff_v_f64m8(src_ptr, new_vl_ptr, vl)
}] in
def th_unit_stride_ff_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Indexed Load/Store Operations
#define __riscv_vlxb_v_i8m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i8m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i8m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i8m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i8m8(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i16m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i16m8(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i32m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i32m8(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m1(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m1(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m2(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m2(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m4(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m4(src_ptr, indexed, vl)
#define __riscv_vlxb_v_i64m8(src_ptr, indexed, vl) __riscv_th_vlxb_v_i64m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i8m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i8m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i16m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i16m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i32m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i32m8(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m1(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m1(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m2(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m2(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m4(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m4(src_ptr, indexed, vl)
#define __riscv_vlxh_v_i64m8(src_ptr, indexed, vl) __riscv_th_vlxh_v_i64m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i8m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i8m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i16m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i16m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i32m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i32m8(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m1(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m1(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m2(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m2(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m4(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m4(src_ptr, indexed, vl)
#define __riscv_vlxw_v_i64m8(src_ptr, indexed, vl) __riscv_th_vlxw_v_i64m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u8m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u8m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u16m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u16m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u32m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u32m8(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m1(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m1(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m2(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m2(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m4(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m4(src_ptr, indexed, vl)
#define __riscv_vlxbu_v_u64m8(src_ptr, indexed, vl) __riscv_th_vlxbu_v_u64m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u8m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u8m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u16m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u16m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u32m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u32m8(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m1(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m1(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m2(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m2(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m4(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m4(src_ptr, indexed, vl)
#define __riscv_vlxhu_v_u64m8(src_ptr, indexed, vl) __riscv_th_vlxhu_v_u64m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u8m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u8m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u16m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u16m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u32m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u32m8(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m1(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m1(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m2(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m2(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m4(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m4(src_ptr, indexed, vl)
#define __riscv_vlxwu_v_u64m8(src_ptr, indexed, vl) __riscv_th_vlxwu_v_u64m8(src_ptr, indexed, vl)
#define __riscv_vsxb_v_i8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_i64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_i64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_i64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_i64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_i64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_i64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxb_v_u64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxb_v_u64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxh_v_u64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxh_v_u64m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u8m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u8m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u16m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u16m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u32m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u32m8(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m1(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m1(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m2(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m2(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m4(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m4(dst_ptr, indexed, value, vl)
#define __riscv_vsxw_v_u64m8(dst_ptr, indexed, value, vl) __riscv_th_vsxw_v_u64m8(dst_ptr, indexed, value, vl)
}] in
def th_indexed_wrapper_macros: RVVHeader;


let HeaderCode =
[{
// Vector Single-Width Integer Add and Subtract
#define __riscv_vadd_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_i64m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vadd_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vadd_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vadd_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_i64m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vadd_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vadd_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vsub_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_i64m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vsub_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vsub_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vsub_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_i64m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vsub_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vsub_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vrsub_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_i64m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vrsub_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vrsub_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vneg_v_i8m1(op1_v, vl) __riscv_th_vneg_v_i8m1(op1_v, vl)
#define __riscv_vneg_v_i8m2(op1_v, vl) __riscv_th_vneg_v_i8m2(op1_v, vl)
#define __riscv_vneg_v_i8m4(op1_v, vl) __riscv_th_vneg_v_i8m4(op1_v, vl)
#define __riscv_vneg_v_i8m8(op1_v, vl) __riscv_th_vneg_v_i8m8(op1_v, vl)
#define __riscv_vneg_v_i16m1(op1_v, vl) __riscv_th_vneg_v_i16m1(op1_v, vl)
#define __riscv_vneg_v_i16m2(op1_v, vl) __riscv_th_vneg_v_i16m2(op1_v, vl)
#define __riscv_vneg_v_i16m4(op1_v, vl) __riscv_th_vneg_v_i16m4(op1_v, vl)
#define __riscv_vneg_v_i16m8(op1_v, vl) __riscv_th_vneg_v_i16m8(op1_v, vl)
#define __riscv_vneg_v_i32m1(op1_v, vl) __riscv_th_vneg_v_i32m1(op1_v, vl)
#define __riscv_vneg_v_i32m2(op1_v, vl) __riscv_th_vneg_v_i32m2(op1_v, vl)
#define __riscv_vneg_v_i32m4(op1_v, vl) __riscv_th_vneg_v_i32m4(op1_v, vl)
#define __riscv_vneg_v_i32m8(op1_v, vl) __riscv_th_vneg_v_i32m8(op1_v, vl)
#define __riscv_vneg_v_i64m1(op1_v, vl) __riscv_th_vneg_v_i64m1(op1_v, vl)
#define __riscv_vneg_v_i64m2(op1_v, vl) __riscv_th_vneg_v_i64m2(op1_v, vl)
#define __riscv_vneg_v_i64m4(op1_v, vl) __riscv_th_vneg_v_i64m4(op1_v, vl)
#define __riscv_vneg_v_i64m8(op1_v, vl) __riscv_th_vneg_v_i64m8(op1_v, vl)
#define __riscv_vneg_v_u8m1(op1_v, vl) __riscv_th_vneg_v_u8m1(op1_v, vl)
#define __riscv_vneg_v_u8m2(op1_v, vl) __riscv_th_vneg_v_u8m2(op1_v, vl)
#define __riscv_vneg_v_u8m4(op1_v, vl) __riscv_th_vneg_v_u8m4(op1_v, vl)
#define __riscv_vneg_v_u8m8(op1_v, vl) __riscv_th_vneg_v_u8m8(op1_v, vl)
#define __riscv_vneg_v_u16m1(op1_v, vl) __riscv_th_vneg_v_u16m1(op1_v, vl)
#define __riscv_vneg_v_u16m2(op1_v, vl) __riscv_th_vneg_v_u16m2(op1_v, vl)
#define __riscv_vneg_v_u16m4(op1_v, vl) __riscv_th_vneg_v_u16m4(op1_v, vl)
#define __riscv_vneg_v_u16m8(op1_v, vl) __riscv_th_vneg_v_u16m8(op1_v, vl)
#define __riscv_vneg_v_u32m1(op1_v, vl) __riscv_th_vneg_v_u32m1(op1_v, vl)
#define __riscv_vneg_v_u32m2(op1_v, vl) __riscv_th_vneg_v_u32m2(op1_v, vl)
#define __riscv_vneg_v_u32m4(op1_v, vl) __riscv_th_vneg_v_u32m4(op1_v, vl)
#define __riscv_vneg_v_u32m8(op1_v, vl) __riscv_th_vneg_v_u32m8(op1_v, vl)
#define __riscv_vneg_v_u64m1(op1_v, vl) __riscv_th_vneg_v_u64m1(op1_v, vl)
#define __riscv_vneg_v_u64m2(op1_v, vl) __riscv_th_vneg_v_u64m2(op1_v, vl)
#define __riscv_vneg_v_u64m4(op1_v, vl) __riscv_th_vneg_v_u64m4(op1_v, vl)
#define __riscv_vneg_v_u64m8(op1_v, vl) __riscv_th_vneg_v_u64m8(op1_v, vl)

}] in
def th_single_width_integer_add_wrapper_macros: RVVHeader;


let HeaderCode =
[{
// Vector Widening Integer Add and Subtract
#define __riscv_vwadd_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vwadd_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vwadd_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vwaddu_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vwaddu_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vwaddu_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vwadd_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vwadd_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vwadd_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vwaddu_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vwaddu_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vwaddu_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vwadd_wv_i16m2(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i16m2(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i16m4(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i16m4(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i16m8(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i16m8(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m1(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m1(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m2(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m2(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m4(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m4(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i32m8(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i32m8(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m1(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m1(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m2(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m2(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m4(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m4(op1_wv, op2_v, vl)
#define __riscv_vwadd_wv_i64m8(op1_wv, op2_v, vl) __riscv_th_vwadd_wv_i64m8(op1_wv, op2_v, vl)

#define __riscv_vwaddu_wv_u16m2(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u16m2(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u16m4(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u16m4(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u16m8(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u16m8(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m1(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m1(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m2(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m2(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m4(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m4(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u32m8(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u32m8(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m1(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m1(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m2(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m2(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m4(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m4(op1_wv, op2_v, vl)
#define __riscv_vwaddu_wv_u64m8(op1_wv, op2_v, vl) __riscv_th_vwaddu_wv_u64m8(op1_wv, op2_v, vl)

#define __riscv_vwadd_wx_i16m1(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m1(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i16m2(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m2(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i16m4(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m4(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i16m8(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i16m8(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m1(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m1(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m2(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m2(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m4(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m4(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i32m8(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i32m8(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m1(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m1(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m2(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m2(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m4(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m4(op1_wx, op2_x, vl)
#define __riscv_vwadd_wx_i64m8(op1_wx, op2_x, vl) __riscv_th_vwadd_wx_i64m8(op1_wx, op2_x, vl)

#define __riscv_vwaddu_wx_u16m1(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m1(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u16m2(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m2(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u16m4(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m4(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u16m8(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u16m8(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m1(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m1(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m2(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m2(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m4(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m4(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u32m8(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u32m8(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m1(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m1(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m2(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m2(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m4(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m4(op1_wx, op2_x, vl)
#define __riscv_vwaddu_wx_u64m8(op1_wx, op2_x, vl) __riscv_th_vwaddu_wx_u64m8(op1_wx, op2_x, vl)

#define __riscv_vwsub_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vwsub_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vwsub_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vwsubu_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vwsubu_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vwsubu_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vwsub_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vwsub_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vwsub_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vwsubu_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vwsubu_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vwsubu_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vwsub_wv_i16m2(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i16m2(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i16m4(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i16m4(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i16m8(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i16m8(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m1(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m1(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m2(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m2(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m4(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m4(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i32m8(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i32m8(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m1(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m1(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m2(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m2(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m4(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m4(op1_wv, op2_v, vl)
#define __riscv_vwsub_wv_i64m8(op1_wv, op2_v, vl) __riscv_th_vwsub_wv_i64m8(op1_wv, op2_v, vl)

#define __riscv_vwsubu_wv_u16m2(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u16m2(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u16m4(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u16m4(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u16m8(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u16m8(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m1(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m1(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m2(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m2(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m4(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m4(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u32m8(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u32m8(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m1(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m1(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m2(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m2(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m4(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m4(op1_wv, op2_v, vl)
#define __riscv_vwsubu_wv_u64m8(op1_wv, op2_v, vl) __riscv_th_vwsubu_wv_u64m8(op1_wv, op2_v, vl)

#define __riscv_vwsub_wx_i16m1(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m1(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i16m2(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m2(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i16m4(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m4(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i16m8(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i16m8(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m1(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m1(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m2(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m2(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m4(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m4(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i32m8(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i32m8(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m1(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m1(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m2(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m2(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m4(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m4(op1_wx, op2_x, vl)
#define __riscv_vwsub_wx_i64m8(op1_wx, op2_x, vl) __riscv_th_vwsub_wx_i64m8(op1_wx, op2_x, vl)

#define __riscv_vwsubu_wx_u16m1(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m1(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u16m2(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m2(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u16m4(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m4(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u16m8(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u16m8(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m1(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m1(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m2(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m2(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m4(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m4(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u32m8(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u32m8(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m1(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m1(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m2(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m2(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m4(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m4(op1_wx, op2_x, vl)
#define __riscv_vwsubu_wx_u64m8(op1_wx, op2_x, vl) __riscv_th_vwsubu_wx_u64m8(op1_wx, op2_x, vl)

}] in
def th_widening_integer_add_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Bitwise Logical Operations
#define __riscv_vand_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vand_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vand_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vand_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vand_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vand_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vand_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vand_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vand_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vand_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vor_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vor_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vor_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vor_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vor_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vor_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vor_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vor_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vor_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vor_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vxor_vv_i8m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i8m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i8m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i8m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i8m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i16m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i16m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i32m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i32m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_i64m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_i64m8(op1_v, op2_v, vl)

#define __riscv_vxor_vv_u8m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u8m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u8m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u8m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u8m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u16m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u16m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u32m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u32m8(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m1(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m1(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m2(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m2(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m4(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m4(op1_v, op2_v, vl)
#define __riscv_vxor_vv_u64m8(op1_v, op2_v, vl) __riscv_th_vxor_vv_u64m8(op1_v, op2_v, vl)

#define __riscv_vxor_vx_i8m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i8m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i8m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i8m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i8m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i16m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i16m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i32m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i32m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_i64m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_i64m8(op1_v, op2_x, vl)

#define __riscv_vxor_vx_u8m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u8m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u8m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u8m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u8m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u16m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u16m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u32m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u32m8(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m1(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m1(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m2(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m2(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m4(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m4(op1_v, op2_x, vl)
#define __riscv_vxor_vx_u64m8(op1_v, op2_x, vl) __riscv_th_vxor_vx_u64m8(op1_v, op2_x, vl)

#define __riscv_vnot_v_i8m1(op1_v, vl) __riscv_th_vnot_v_i8m1(op1_v, vl)
#define __riscv_vnot_v_i8m2(op1_v, vl) __riscv_th_vnot_v_i8m2(op1_v, vl)
#define __riscv_vnot_v_i8m4(op1_v, vl) __riscv_th_vnot_v_i8m4(op1_v, vl)
#define __riscv_vnot_v_i8m8(op1_v, vl) __riscv_th_vnot_v_i8m8(op1_v, vl)
#define __riscv_vnot_v_i16m1(op1_v, vl) __riscv_th_vnot_v_i16m1(op1_v, vl)
#define __riscv_vnot_v_i16m2(op1_v, vl) __riscv_th_vnot_v_i16m2(op1_v, vl)
#define __riscv_vnot_v_i16m4(op1_v, vl) __riscv_th_vnot_v_i16m4(op1_v, vl)
#define __riscv_vnot_v_i16m8(op1_v, vl) __riscv_th_vnot_v_i16m8(op1_v, vl)
#define __riscv_vnot_v_i32m1(op1_v, vl) __riscv_th_vnot_v_i32m1(op1_v, vl)
#define __riscv_vnot_v_i32m2(op1_v, vl) __riscv_th_vnot_v_i32m2(op1_v, vl)
#define __riscv_vnot_v_i32m4(op1_v, vl) __riscv_th_vnot_v_i32m4(op1_v, vl)
#define __riscv_vnot_v_i32m8(op1_v, vl) __riscv_th_vnot_v_i32m8(op1_v, vl)
#define __riscv_vnot_v_i64m1(op1_v, vl) __riscv_th_vnot_v_i64m1(op1_v, vl)
#define __riscv_vnot_v_i64m2(op1_v, vl) __riscv_th_vnot_v_i64m2(op1_v, vl)
#define __riscv_vnot_v_i64m4(op1_v, vl) __riscv_th_vnot_v_i64m4(op1_v, vl)
#define __riscv_vnot_v_i64m8(op1_v, vl) __riscv_th_vnot_v_i64m8(op1_v, vl)

#define __riscv_vnot_v_u8m1(op1_v, vl) __riscv_th_vnot_v_u8m1(op1_v, vl)
#define __riscv_vnot_v_u8m2(op1_v, vl) __riscv_th_vnot_v_u8m2(op1_v, vl)
#define __riscv_vnot_v_u8m4(op1_v, vl) __riscv_th_vnot_v_u8m4(op1_v, vl)
#define __riscv_vnot_v_u8m8(op1_v, vl) __riscv_th_vnot_v_u8m8(op1_v, vl)
#define __riscv_vnot_v_u16m1(op1_v, vl) __riscv_th_vnot_v_u16m1(op1_v, vl)
#define __riscv_vnot_v_u16m2(op1_v, vl) __riscv_th_vnot_v_u16m2(op1_v, vl)
#define __riscv_vnot_v_u16m4(op1_v, vl) __riscv_th_vnot_v_u16m4(op1_v, vl)
#define __riscv_vnot_v_u16m8(op1_v, vl) __riscv_th_vnot_v_u16m8(op1_v, vl)
#define __riscv_vnot_v_u32m1(op1_v, vl) __riscv_th_vnot_v_u32m1(op1_v, vl)
#define __riscv_vnot_v_u32m2(op1_v, vl) __riscv_th_vnot_v_u32m2(op1_v, vl)
#define __riscv_vnot_v_u32m4(op1_v, vl) __riscv_th_vnot_v_u32m4(op1_v, vl)
#define __riscv_vnot_v_u32m8(op1_v, vl) __riscv_th_vnot_v_u32m8(op1_v, vl)
#define __riscv_vnot_v_u64m1(op1_v, vl) __riscv_th_vnot_v_u64m1(op1_v, vl)
#define __riscv_vnot_v_u64m2(op1_v, vl) __riscv_th_vnot_v_u64m2(op1_v, vl)
#define __riscv_vnot_v_u64m4(op1_v, vl) __riscv_th_vnot_v_u64m4(op1_v, vl)
#define __riscv_vnot_v_u64m8(op1_v, vl) __riscv_th_vnot_v_u64m8(op1_v, vl)

}] in
def th_bitwise_logical_wrapper_macros: RVVHeader;

let HeaderCode =
[{
// Vector Single Width Integer Bit Shift Operations
#define __riscv_vsll_vv_i8m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i8m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i8m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i8m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i8m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i16m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i16m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i32m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i32m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_i64m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_i64m8(op1_v, shift_v, vl)

#define __riscv_vsll_vv_u8m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u8m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u8m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u8m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u8m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u16m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u16m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u32m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u32m8(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m1(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m1(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m2(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m2(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m4(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m4(op1_v, shift_v, vl)
#define __riscv_vsll_vv_u64m8(op1_v, shift_v, vl) __riscv_th_vsll_vv_u64m8(op1_v, shift_v, vl)

#define __riscv_vsll_vx_i8m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i8m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i8m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i8m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i8m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i16m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i16m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i32m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i32m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_i64m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_i64m8(op1_v, shift_x, vl)

#define __riscv_vsll_vx_u8m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u8m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u8m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u8m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u8m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u16m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u16m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u32m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u32m8(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m1(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m1(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m2(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m2(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m4(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m4(op1_v, shift_x, vl)
#define __riscv_vsll_vx_u64m8(op1_v, shift_x, vl) __riscv_th_vsll_vx_u64m8(op1_v, shift_x, vl)

#define __riscv_vsrl_vv_u8m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u8m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u8m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u8m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u8m8(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u16m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u16m8(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u32m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u32m8(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m1(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m1(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m2(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m2(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m4(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m4(op1_v, shift_v, vl)
#define __riscv_vsrl_vv_u64m8(op1_v, shift_v, vl) __riscv_th_vsrl_vv_u64m8(op1_v, shift_v, vl)

#define __riscv_vsrl_vx_u8m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u8m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u8m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u8m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u8m8(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u16m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u16m8(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u32m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u32m8(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m1(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m1(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m2(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m2(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m4(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m4(op1_v, shift_x, vl)
#define __riscv_vsrl_vx_u64m8(op1_v, shift_x, vl) __riscv_th_vsrl_vx_u64m8(op1_v, shift_x, vl)

#define __riscv_vsra_vv_i8m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i8m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i8m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i8m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i8m8(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i16m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i16m8(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i32m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i32m8(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m1(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m1(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m2(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m2(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m4(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m4(op1_v, shift_v, vl)
#define __riscv_vsra_vv_i64m8(op1_v, shift_v, vl) __riscv_th_vsra_vv_i64m8(op1_v, shift_v, vl)

#define __riscv_vsra_vx_i8m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i8m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i8m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i8m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i8m8(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i16m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i16m8(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i32m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i32m8(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m1(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m1(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m2(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m2(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m4(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m4(op1_v, shift_x, vl)
#define __riscv_vsra_vx_i64m8(op1_v, shift_x, vl) __riscv_th_vsra_vx_i64m8(op1_v, shift_x, vl)

}] in
def th_single_width_integer_bit_shift_wrapper_macros: RVVHeader;

// 12.6. Vector Narrowing Integer Right Shift Operations

let HeaderCode =
[{
// Vector Narrowing Integer Right Shift Operations
#define __riscv_vnsrl_wv_u8m1(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u8m1(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u8m2(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u8m2(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u8m4(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u8m4(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u16m1(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u16m1(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u16m2(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u16m2(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u16m4(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u16m4(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u32m1(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u32m1(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u32m2(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u32m2(op1_v, shift_v, vl)
#define __riscv_vnsrl_wv_u32m4(op1_v, shift_v, vl) __riscv_th_vnsrl_wv_u32m4(op1_v, shift_v, vl)

#define __riscv_vnsrl_wx_u8m1(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u8m1(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u8m2(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u8m2(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u8m4(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u8m4(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u16m1(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u16m1(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u16m2(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u16m2(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u16m4(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u16m4(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u32m1(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u32m1(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u32m2(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u32m2(op1_v, shift_x, vl)
#define __riscv_vnsrl_wx_u32m4(op1_v, shift_x, vl) __riscv_th_vnsrl_wx_u32m4(op1_v, shift_x, vl)

#define __riscv_vnsra_wv_i8m1(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i8m1(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i8m2(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i8m2(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i8m4(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i8m4(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i16m1(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i16m1(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i16m2(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i16m2(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i16m4(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i16m4(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i32m1(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i32m1(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i32m2(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i32m2(op1_v, shift_v, vl)
#define __riscv_vnsra_wv_i32m4(op1_v, shift_v, vl) __riscv_th_vnsra_wv_i32m4(op1_v, shift_v, vl)

#define __riscv_vnsra_wx_i8m1(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i8m1(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i8m2(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i8m2(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i8m4(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i8m4(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i16m1(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i16m1(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i16m2(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i16m2(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i16m4(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i16m4(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i32m1(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i32m1(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i32m2(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i32m2(op1_v, shift_x, vl)
#define __riscv_vnsra_wx_i32m4(op1_v, shift_x, vl) __riscv_th_vnsra_wx_i32m4(op1_v, shift_x, vl)

}] in
def th_narrowing_integer_right_shift_wrapper_macros: RVVHeader;

// 12.7. Vector Integer Comparison Operations

// 12.8. Vector Integer Min/Max Operations

// 12.9. Vector Single-Width Integer Multiply Operations

// 12.10. Vector Integer Divide Operations

// 12.11. Vector Widening Integer Multiply Operations

// 12.12. Vector Single-Width Integer Multiply-Add Operations

// 12.13. Vector Widening Integer Multiply-Add Operations

// 12.14. Vector Integer Merge Operations

// 12.15. Vector Integer Move Operations

// 13.1. Vector Single-Width Saturating Add and Subtract

let HeaderCode =
[{
// Vector Single-Width Saturating Add and Subtract
#define __riscv_vsadd_vv_i8m1(op1, op2, vl) __riscv_th_vsadd_vv_i8m1(op1, op2, vl)
#define __riscv_vsadd_vx_i8m1(op1, op2, vl) __riscv_th_vsadd_vx_i8m1(op1, op2, vl)
#define __riscv_vsadd_vv_i8m2(op1, op2, vl) __riscv_th_vsadd_vv_i8m2(op1, op2, vl)
#define __riscv_vsadd_vx_i8m2(op1, op2, vl) __riscv_th_vsadd_vx_i8m2(op1, op2, vl)
#define __riscv_vsadd_vv_i8m4(op1, op2, vl) __riscv_th_vsadd_vv_i8m4(op1, op2, vl)
#define __riscv_vsadd_vx_i8m4(op1, op2, vl) __riscv_th_vsadd_vx_i8m4(op1, op2, vl)
#define __riscv_vsadd_vv_i8m8(op1, op2, vl) __riscv_th_vsadd_vv_i8m8(op1, op2, vl)
#define __riscv_vsadd_vx_i8m8(op1, op2, vl) __riscv_th_vsadd_vx_i8m8(op1, op2, vl)
#define __riscv_vsadd_vv_i16m1(op1, op2, vl) __riscv_th_vsadd_vv_i16m1(op1, op2, vl)
#define __riscv_vsadd_vx_i16m1(op1, op2, vl) __riscv_th_vsadd_vx_i16m1(op1, op2, vl)
#define __riscv_vsadd_vv_i16m2(op1, op2, vl) __riscv_th_vsadd_vv_i16m2(op1, op2, vl)
#define __riscv_vsadd_vx_i16m2(op1, op2, vl) __riscv_th_vsadd_vx_i16m2(op1, op2, vl)
#define __riscv_vsadd_vv_i16m4(op1, op2, vl) __riscv_th_vsadd_vv_i16m4(op1, op2, vl)
#define __riscv_vsadd_vx_i16m4(op1, op2, vl) __riscv_th_vsadd_vx_i16m4(op1, op2, vl)
#define __riscv_vsadd_vv_i16m8(op1, op2, vl) __riscv_th_vsadd_vv_i16m8(op1, op2, vl)
#define __riscv_vsadd_vx_i16m8(op1, op2, vl) __riscv_th_vsadd_vx_i16m8(op1, op2, vl)
#define __riscv_vsadd_vv_i32m1(op1, op2, vl) __riscv_th_vsadd_vv_i32m1(op1, op2, vl)
#define __riscv_vsadd_vx_i32m1(op1, op2, vl) __riscv_th_vsadd_vx_i32m1(op1, op2, vl)
#define __riscv_vsadd_vv_i32m2(op1, op2, vl) __riscv_th_vsadd_vv_i32m2(op1, op2, vl)
#define __riscv_vsadd_vx_i32m2(op1, op2, vl) __riscv_th_vsadd_vx_i32m2(op1, op2, vl)
#define __riscv_vsadd_vv_i32m4(op1, op2, vl) __riscv_th_vsadd_vv_i32m4(op1, op2, vl)
#define __riscv_vsadd_vx_i32m4(op1, op2, vl) __riscv_th_vsadd_vx_i32m4(op1, op2, vl)
#define __riscv_vsadd_vv_i32m8(op1, op2, vl) __riscv_th_vsadd_vv_i32m8(op1, op2, vl)
#define __riscv_vsadd_vx_i32m8(op1, op2, vl) __riscv_th_vsadd_vx_i32m8(op1, op2, vl)
#define __riscv_vsadd_vv_i64m1(op1, op2, vl) __riscv_th_vsadd_vv_i64m1(op1, op2, vl)
#define __riscv_vsadd_vx_i64m1(op1, op2, vl) __riscv_th_vsadd_vx_i64m1(op1, op2, vl)
#define __riscv_vsadd_vv_i64m2(op1, op2, vl) __riscv_th_vsadd_vv_i64m2(op1, op2, vl)
#define __riscv_vsadd_vx_i64m2(op1, op2, vl) __riscv_th_vsadd_vx_i64m2(op1, op2, vl)
#define __riscv_vsadd_vv_i64m4(op1, op2, vl) __riscv_th_vsadd_vv_i64m4(op1, op2, vl)
#define __riscv_vsadd_vx_i64m4(op1, op2, vl) __riscv_th_vsadd_vx_i64m4(op1, op2, vl)
#define __riscv_vsadd_vv_i64m8(op1, op2, vl) __riscv_th_vsadd_vv_i64m8(op1, op2, vl)
#define __riscv_vsadd_vx_i64m8(op1, op2, vl) __riscv_th_vsadd_vx_i64m8(op1, op2, vl)

#define __riscv_vsadd_vv_i8m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i8m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i8m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i8m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i8m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i8m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i8m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i8m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i8m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i8m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i8m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i8m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i8m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i8m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i8m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i8m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i16m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i16m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i16m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i16m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i16m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i16m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i16m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i16m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i16m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i16m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i16m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i16m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i16m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i16m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i16m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i16m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i32m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i32m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i32m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i32m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i32m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i32m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i32m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i32m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i32m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i32m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i32m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i32m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i32m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i32m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i32m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i32m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i64m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i64m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i64m1_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i64m1_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i64m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i64m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i64m2_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i64m2_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i64m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i64m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i64m4_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i64m4_m(mask, op1, op2, vl)
#define __riscv_vsadd_vv_i64m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vv_i64m8_m(mask, op1, op2, vl)
#define __riscv_vsadd_vx_i64m8_m(mask, op1, op2, vl) __riscv_th_vsadd_vx_i64m8_m(mask, op1, op2, vl)

#define __riscv_vssub_vv_i8m1(op1, op2, vl) __riscv_th_vssub_vv_i8m1(op1, op2, vl)
#define __riscv_vssub_vx_i8m1(op1, op2, vl) __riscv_th_vssub_vx_i8m1(op1, op2, vl)
#define __riscv_vssub_vv_i8m2(op1, op2, vl) __riscv_th_vssub_vv_i8m2(op1, op2, vl)
#define __riscv_vssub_vx_i8m2(op1, op2, vl) __riscv_th_vssub_vx_i8m2(op1, op2, vl)
#define __riscv_vssub_vv_i8m4(op1, op2, vl) __riscv_th_vssub_vv_i8m4(op1, op2, vl)
#define __riscv_vssub_vx_i8m4(op1, op2, vl) __riscv_th_vssub_vx_i8m4(op1, op2, vl)
#define __riscv_vssub_vv_i8m8(op1, op2, vl) __riscv_th_vssub_vv_i8m8(op1, op2, vl)
#define __riscv_vssub_vx_i8m8(op1, op2, vl) __riscv_th_vssub_vx_i8m8(op1, op2, vl)
#define __riscv_vssub_vv_i16m1(op1, op2, vl) __riscv_th_vssub_vv_i16m1(op1, op2, vl)
#define __riscv_vssub_vx_i16m1(op1, op2, vl) __riscv_th_vssub_vx_i16m1(op1, op2, vl)
#define __riscv_vssub_vv_i16m2(op1, op2, vl) __riscv_th_vssub_vv_i16m2(op1, op2, vl)
#define __riscv_vssub_vx_i16m2(op1, op2, vl) __riscv_th_vssub_vx_i16m2(op1, op2, vl)
#define __riscv_vssub_vv_i16m4(op1, op2, vl) __riscv_th_vssub_vv_i16m4(op1, op2, vl)
#define __riscv_vssub_vx_i16m4(op1, op2, vl) __riscv_th_vssub_vx_i16m4(op1, op2, vl)
#define __riscv_vssub_vv_i16m8(op1, op2, vl) __riscv_th_vssub_vv_i16m8(op1, op2, vl)
#define __riscv_vssub_vx_i16m8(op1, op2, vl) __riscv_th_vssub_vx_i16m8(op1, op2, vl)
#define __riscv_vssub_vv_i32m1(op1, op2, vl) __riscv_th_vssub_vv_i32m1(op1, op2, vl)
#define __riscv_vssub_vx_i32m1(op1, op2, vl) __riscv_th_vssub_vx_i32m1(op1, op2, vl)
#define __riscv_vssub_vv_i32m2(op1, op2, vl) __riscv_th_vssub_vv_i32m2(op1, op2, vl)
#define __riscv_vssub_vx_i32m2(op1, op2, vl) __riscv_th_vssub_vx_i32m2(op1, op2, vl)
#define __riscv_vssub_vv_i32m4(op1, op2, vl) __riscv_th_vssub_vv_i32m4(op1, op2, vl)
#define __riscv_vssub_vx_i32m4(op1, op2, vl) __riscv_th_vssub_vx_i32m4(op1, op2, vl)
#define __riscv_vssub_vv_i32m8(op1, op2, vl) __riscv_th_vssub_vv_i32m8(op1, op2, vl)
#define __riscv_vssub_vx_i32m8(op1, op2, vl) __riscv_th_vssub_vx_i32m8(op1, op2, vl)
#define __riscv_vssub_vv_i64m1(op1, op2, vl) __riscv_th_vssub_vv_i64m1(op1, op2, vl)
#define __riscv_vssub_vx_i64m1(op1, op2, vl) __riscv_th_vssub_vx_i64m1(op1, op2, vl)
#define __riscv_vssub_vv_i64m2(op1, op2, vl) __riscv_th_vssub_vv_i64m2(op1, op2, vl)
#define __riscv_vssub_vx_i64m2(op1, op2, vl) __riscv_th_vssub_vx_i64m2(op1, op2, vl)
#define __riscv_vssub_vv_i64m4(op1, op2, vl) __riscv_th_vssub_vv_i64m4(op1, op2, vl)
#define __riscv_vssub_vx_i64m4(op1, op2, vl) __riscv_th_vssub_vx_i64m4(op1, op2, vl)
#define __riscv_vssub_vv_i64m8(op1, op2, vl) __riscv_th_vssub_vv_i64m8(op1, op2, vl)
#define __riscv_vssub_vx_i64m8(op1, op2, vl) __riscv_th_vssub_vx_i64m8(op1, op2, vl)

#define __riscv_vssub_vv_i8m1_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i8m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i8m1_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i8m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i8m2_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i8m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i8m2_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i8m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i8m4_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i8m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i8m4_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i8m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i8m8_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i8m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i8m8_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i8m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i16m1_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i16m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i16m1_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i16m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i16m2_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i16m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i16m2_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i16m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i16m4_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i16m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i16m4_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i16m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i16m8_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i16m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i16m8_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i16m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i32m1_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i32m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i32m1_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i32m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i32m2_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i32m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i32m2_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i32m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i32m4_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i32m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i32m4_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i32m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i32m8_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i32m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i32m8_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i32m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i64m1_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i64m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i64m1_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i64m1_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i64m2_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i64m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i64m2_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i64m2_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i64m4_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i64m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i64m4_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i64m4_m(mask, op1, op2, vl)
#define __riscv_vssub_vv_i64m8_m(mask, op1, op2, vl) __riscv_th_vssub_vv_i64m8_m(mask, op1, op2, vl)
#define __riscv_vssub_vx_i64m8_m(mask, op1, op2, vl) __riscv_th_vssub_vx_i64m8_m(mask, op1, op2, vl)

#define __riscv_vsaddu_vv_u8m1(op1, op2, vl) __riscv_th_vsaddu_vv_u8m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m1(op1, op2, vl) __riscv_th_vsaddu_vx_u8m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u8m2(op1, op2, vl) __riscv_th_vsaddu_vv_u8m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m2(op1, op2, vl) __riscv_th_vsaddu_vx_u8m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u8m4(op1, op2, vl) __riscv_th_vsaddu_vv_u8m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m4(op1, op2, vl) __riscv_th_vsaddu_vx_u8m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u8m8(op1, op2, vl) __riscv_th_vsaddu_vv_u8m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u8m8(op1, op2, vl) __riscv_th_vsaddu_vx_u8m8(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m1(op1, op2, vl) __riscv_th_vsaddu_vv_u16m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m1(op1, op2, vl) __riscv_th_vsaddu_vx_u16m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m2(op1, op2, vl) __riscv_th_vsaddu_vv_u16m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m2(op1, op2, vl) __riscv_th_vsaddu_vx_u16m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m4(op1, op2, vl) __riscv_th_vsaddu_vv_u16m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m4(op1, op2, vl) __riscv_th_vsaddu_vx_u16m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u16m8(op1, op2, vl) __riscv_th_vsaddu_vv_u16m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u16m8(op1, op2, vl) __riscv_th_vsaddu_vx_u16m8(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m1(op1, op2, vl) __riscv_th_vsaddu_vv_u32m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m1(op1, op2, vl) __riscv_th_vsaddu_vx_u32m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m2(op1, op2, vl) __riscv_th_vsaddu_vv_u32m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m2(op1, op2, vl) __riscv_th_vsaddu_vx_u32m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m4(op1, op2, vl) __riscv_th_vsaddu_vv_u32m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m4(op1, op2, vl) __riscv_th_vsaddu_vx_u32m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u32m8(op1, op2, vl) __riscv_th_vsaddu_vv_u32m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u32m8(op1, op2, vl) __riscv_th_vsaddu_vx_u32m8(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m1(op1, op2, vl) __riscv_th_vsaddu_vv_u64m1(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m1(op1, op2, vl) __riscv_th_vsaddu_vx_u64m1(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m2(op1, op2, vl) __riscv_th_vsaddu_vv_u64m2(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m2(op1, op2, vl) __riscv_th_vsaddu_vx_u64m2(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m4(op1, op2, vl) __riscv_th_vsaddu_vv_u64m4(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m4(op1, op2, vl) __riscv_th_vsaddu_vx_u64m4(op1, op2, vl)
#define __riscv_vsaddu_vv_u64m8(op1, op2, vl) __riscv_th_vsaddu_vv_u64m8(op1, op2, vl)
#define __riscv_vsaddu_vx_u64m8(op1, op2, vl) __riscv_th_vsaddu_vx_u64m8(op1, op2, vl)

#define __riscv_vsaddu_vv_u8m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u8m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u8m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u8m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u8m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u8m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u8m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u8m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u8m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u8m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u8m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u8m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u8m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u16m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u16m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u16m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u16m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u16m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u16m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u16m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u16m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u16m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u16m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u32m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u32m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u32m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u32m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u32m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u32m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u32m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u32m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u32m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u32m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u64m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m1_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u64m1_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u64m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m2_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u64m2_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u64m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m4_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u64m4_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vv_u64m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vv_u64m8_m(mask, op1, op2, vl)
#define __riscv_vsaddu_vx_u64m8_m(mask, op1, op2, vl) __riscv_th_vsaddu_vx_u64m8_m(mask, op1, op2, vl)

#define __riscv_vssubu_vv_u8m1(op1, op2, vl) __riscv_th_vssubu_vv_u8m1(op1, op2, vl)
#define __riscv_vssubu_vx_u8m1(op1, op2, vl) __riscv_th_vssubu_vx_u8m1(op1, op2, vl)
#define __riscv_vssubu_vv_u8m2(op1, op2, vl) __riscv_th_vssubu_vv_u8m2(op1, op2, vl)
#define __riscv_vssubu_vx_u8m2(op1, op2, vl) __riscv_th_vssubu_vx_u8m2(op1, op2, vl)
#define __riscv_vssubu_vv_u8m4(op1, op2, vl) __riscv_th_vssubu_vv_u8m4(op1, op2, vl)
#define __riscv_vssubu_vx_u8m4(op1, op2, vl) __riscv_th_vssubu_vx_u8m4(op1, op2, vl)
#define __riscv_vssubu_vv_u8m8(op1, op2, vl) __riscv_th_vssubu_vv_u8m8(op1, op2, vl)
#define __riscv_vssubu_vx_u8m8(op1, op2, vl) __riscv_th_vssubu_vx_u8m8(op1, op2, vl)
#define __riscv_vssubu_vv_u16m1(op1, op2, vl) __riscv_th_vssubu_vv_u16m1(op1, op2, vl)
#define __riscv_vssubu_vx_u16m1(op1, op2, vl) __riscv_th_vssubu_vx_u16m1(op1, op2, vl)
#define __riscv_vssubu_vv_u16m2(op1, op2, vl) __riscv_th_vssubu_vv_u16m2(op1, op2, vl)
#define __riscv_vssubu_vx_u16m2(op1, op2, vl) __riscv_th_vssubu_vx_u16m2(op1, op2, vl)
#define __riscv_vssubu_vv_u16m4(op1, op2, vl) __riscv_th_vssubu_vv_u16m4(op1, op2, vl)
#define __riscv_vssubu_vx_u16m4(op1, op2, vl) __riscv_th_vssubu_vx_u16m4(op1, op2, vl)
#define __riscv_vssubu_vv_u16m8(op1, op2, vl) __riscv_th_vssubu_vv_u16m8(op1, op2, vl)
#define __riscv_vssubu_vx_u16m8(op1, op2, vl) __riscv_th_vssubu_vx_u16m8(op1, op2, vl)
#define __riscv_vssubu_vv_u32m1(op1, op2, vl) __riscv_th_vssubu_vv_u32m1(op1, op2, vl)
#define __riscv_vssubu_vx_u32m1(op1, op2, vl) __riscv_th_vssubu_vx_u32m1(op1, op2, vl)
#define __riscv_vssubu_vv_u32m2(op1, op2, vl) __riscv_th_vssubu_vv_u32m2(op1, op2, vl)
#define __riscv_vssubu_vx_u32m2(op1, op2, vl) __riscv_th_vssubu_vx_u32m2(op1, op2, vl)
#define __riscv_vssubu_vv_u32m4(op1, op2, vl) __riscv_th_vssubu_vv_u32m4(op1, op2, vl)
#define __riscv_vssubu_vx_u32m4(op1, op2, vl) __riscv_th_vssubu_vx_u32m4(op1, op2, vl)
#define __riscv_vssubu_vv_u32m8(op1, op2, vl) __riscv_th_vssubu_vv_u32m8(op1, op2, vl)
#define __riscv_vssubu_vx_u32m8(op1, op2, vl) __riscv_th_vssubu_vx_u32m8(op1, op2, vl)
#define __riscv_vssubu_vv_u64m1(op1, op2, vl) __riscv_th_vssubu_vv_u64m1(op1, op2, vl)
#define __riscv_vssubu_vx_u64m1(op1, op2, vl) __riscv_th_vssubu_vx_u64m1(op1, op2, vl)
#define __riscv_vssubu_vv_u64m2(op1, op2, vl) __riscv_th_vssubu_vv_u64m2(op1, op2, vl)
#define __riscv_vssubu_vx_u64m2(op1, op2, vl) __riscv_th_vssubu_vx_u64m2(op1, op2, vl)
#define __riscv_vssubu_vv_u64m4(op1, op2, vl) __riscv_th_vssubu_vv_u64m4(op1, op2, vl)
#define __riscv_vssubu_vx_u64m4(op1, op2, vl) __riscv_th_vssubu_vx_u64m4(op1, op2, vl)
#define __riscv_vssubu_vv_u64m8(op1, op2, vl) __riscv_th_vssubu_vv_u64m8(op1, op2, vl)
#define __riscv_vssubu_vx_u64m8(op1, op2, vl) __riscv_th_vssubu_vx_u64m8(op1, op2, vl)

#define __riscv_vssubu_vv_u8m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u8m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u8m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u8m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u8m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u8m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u8m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u8m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u8m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u8m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u8m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u8m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u8m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u8m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u8m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u8m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u16m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u16m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u16m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u16m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u16m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u16m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u16m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u16m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u16m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u16m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u16m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u16m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u16m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u16m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u16m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u16m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u32m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u32m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u32m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u32m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u32m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u32m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u32m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u32m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u32m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u32m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u32m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u32m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u32m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u32m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u32m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u32m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u64m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u64m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u64m1_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u64m1_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u64m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u64m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u64m2_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u64m2_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u64m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u64m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u64m4_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u64m4_m(mask, op1, op2, vl)
#define __riscv_vssubu_vv_u64m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vv_u64m8_m(mask, op1, op2, vl)
#define __riscv_vssubu_vx_u64m8_m(mask, op1, op2, vl) __riscv_th_vssubu_vx_u64m8_m(mask, op1, op2, vl)

}] in
def th_single_width_saturating_add_wrapper_macros: RVVHeader;

// 13.2. Vector Single-Width Averaging Add and Subtract

let HeaderCode =
[{
// Vector Single-Width Averaging Add and Subtract
#define __riscv_vaadd_vv_i8m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m8(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m8(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m8(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m1(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m1(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m1(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m1(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m2(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m2(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m2(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m2(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m4(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m4(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m4(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m4(op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m8(op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m8(op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m8(op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m8(op1, op2, rm, vl)

#define __riscv_vaadd_vv_i8m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i8m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i8m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i8m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i8m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i16m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i16m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i16m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i16m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i32m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i32m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i32m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i32m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m1_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m1_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m2_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m2_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m4_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m4_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vv_i64m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vv_i64m8_m(mask, op1, op2, rm, vl)
#define __riscv_vaadd_vx_i64m8_m(mask, op1, op2, rm, vl) __riscv_th_vaadd_vx_i64m8_m(mask, op1, op2, rm, vl)

#define __riscv_vasub_vv_i8m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i8m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i8m8(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i16m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i16m8(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i32m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i32m8(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m1(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m1(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m1(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m1(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m2(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m2(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m2(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m2(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m4(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m4(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m4(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m4(op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m8(op1, op2, rm, vl) __riscv_th_vasub_vv_i64m8(op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m8(op1, op2, rm, vl) __riscv_th_vasub_vx_i64m8(op1, op2, rm, vl)

#define __riscv_vasub_vv_i8m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i8m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i8m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i8m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i8m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i16m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i16m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i16m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i16m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i32m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i32m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i32m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i32m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m1_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m1_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m2_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m2_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m4_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m4_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vv_i64m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vv_i64m8_m(mask, op1, op2, rm, vl)
#define __riscv_vasub_vx_i64m8_m(mask, op1, op2, rm, vl) __riscv_th_vasub_vx_i64m8_m(mask, op1, op2, rm, vl)

}] in
def th_single_width_averaging_add_and_subtract_wrapper_macros: RVVHeader;

// 13.3.  Vector Single-Width Fractional Multiply with Rounding and Saturation

let HeaderCode =
[{

#define __riscv_vsmul_vv_i8m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m8(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m8(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m8(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m1(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m1(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m1(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m1(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m2(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m2(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m2(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m2(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m4(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m4(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m4(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m4(op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m8(op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m8(op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m8(op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m8(op1, op2, rm, vl)

#define __riscv_vsmul_vv_i8m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i8m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i8m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i8m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i8m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i16m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i16m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i16m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i16m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i32m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i32m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i32m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i32m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m1_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m1_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m2_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m2_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m4_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m4_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vv_i64m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vv_i64m8_m(mask, op1, op2, rm, vl)
#define __riscv_vsmul_vx_i64m8_m(mask, op1, op2, rm, vl) __riscv_th_vsmul_vx_i64m8_m(mask, op1, op2, rm, vl)

}] in
def th_single_width_fractional_multiply_with_rounding_and_saturation_wrapper_macros: RVVHeader;
